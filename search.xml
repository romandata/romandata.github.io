<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Kafka客户端命令操作</title>
    <url>/2020/10/08/Kafka%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<h4 id="Topic-管理"><a href="#Topic-管理" class="headerlink" title="Topic 管理"></a>Topic 管理</h4><p><code>kafka-topics.sh</code>  的参数</p>
<table>
<thead>
<tr>
<th align="left">参数名称</th>
<th>解释及其作用</th>
</tr>
</thead>
<tbody><tr>
<td align="left">–alter</td>
<td>用于修改主题，包括分区数及Topic的配置</td>
</tr>
<tr>
<td align="left">–config</td>
<td>创建或修改Topic时，用于设置 Topic 级别的配置</td>
</tr>
<tr>
<td align="left">–create</td>
<td>创建 Topic</td>
</tr>
<tr>
<td align="left">–delete</td>
<td>删除 Topic</td>
</tr>
<tr>
<td align="left">–delete-config</td>
<td>删除 Topic 级别被覆盖的配置</td>
</tr>
<tr>
<td align="left">–describe</td>
<td>查看 Topic 的详细信息</td>
</tr>
<tr>
<td align="left">–disable-rack-aware</td>
<td>创建 Topic 时不考虑机架信息</td>
</tr>
<tr>
<td align="left">–help</td>
<td>打印帮助信息文档</td>
</tr>
<tr>
<td align="left">–if-exists</td>
<td>修改或删除 Topic时，只有 Topic 存在时才会执行操作</td>
</tr>
<tr>
<td align="left">–if-not-exists</td>
<td>创建 Topic 时，只有 Topic 不存在才会执行操作</td>
</tr>
<tr>
<td align="left">–list</td>
<td>列出所有可用的 Topic</td>
</tr>
<tr>
<td align="left">–partitions</td>
<td>创建 Topic 或增加分区时 指定分区数</td>
</tr>
<tr>
<td align="left">–replica-assignment</td>
<td>手工制定分区副本的分配方案</td>
</tr>
<tr>
<td align="left">–replication-factor</td>
<td>创建 Topic 时指定副本数</td>
</tr>
<tr>
<td align="left">–topic</td>
<td>指定 Topic 名称</td>
</tr>
<tr>
<td align="left">–topics-with-overrides</td>
<td>使用describe查看 Topic 时，只展示包含覆盖配置的 Topic</td>
</tr>
<tr>
<td align="left">–unavailable-partition</td>
<td>使用describe查看 Topic 时，只展示包含无leader的分区</td>
</tr>
<tr>
<td align="left">–under-replicated-partitions</td>
<td>使用describe查看 Topic 时，只展示包含失效副本的分区</td>
</tr>
<tr>
<td align="left">–zookeeper</td>
<td>指定连接的zookeeper信息（必填）（zk1:2181/kafka）</td>
</tr>
</tbody></table>
<h6 id="创建-Topic"><a href="#创建-Topic" class="headerlink" title="创建 Topic"></a>创建 Topic</h6><p>首先确认一个参数 <code>auto.create.topics.enable=true</code> ，此参数用来自动创建 Topic，也就说当发送消费等操作用到了未创建的 <code>Topic</code> 的时候，此参数会帮忙自动创建一个默认配置的 Topic。<strong>强烈建议在 Kafka 配置关闭这个参数。</strong></p>
<p> 接下来看看怎么在客户端手动创建一个 <code>Topic</code>，使用  </p>
<p>-topics.sh<code>，指定</code>Zookeeper、partition、replica`  完成创建即可。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# pwd</span><br><span class="line">/data/kafka/kafka_2.11-0.10.2.1/bin</span><br><span class="line">[root@tnode1 bin]# ./kafka-topics.sh --create --zookeeper tnode3:2181 --replication-factor 2 --partitions 3 --topic LxmTest</span><br><span class="line">Created topic "LxmTest".</span><br></pre></td></tr></table></figure>

<h6 id="查看-Topic"><a href="#查看-Topic" class="headerlink" title="查看 Topic"></a>查看 Topic</h6><p>查看所有 <code>Topic</code></p>
<p>查看所有 <code>Topic</code> 信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-topics.sh --zookeeper tnode3:2181  --describe</span><br><span class="line">Topic:LxmTest	PartitionCount:3	ReplicationFactor:2	Configs:</span><br><span class="line">	Topic: LxmTest	Partition: 0	Leader: 1	Replicas: 1,0	Isr: 1,0</span><br><span class="line">	Topic: LxmTest	Partition: 1	Leader: 2	Replicas: 2,1	Isr: 2,1</span><br><span class="line">	Topic: LxmTest	Partition: 2	Leader: 0	Replicas: 0,2	Isr: 0,2</span><br><span class="line">... </span><br><span class="line">(其他 topic 信息省略)</span><br></pre></td></tr></table></figure>

<p>查看单个 <code>Topic</code> 信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-topics.sh --zookeeper tnode3:2181  --describe --topic LxmTest</span><br><span class="line">Topic:LxmTest	PartitionCount:3	ReplicationFactor:2	Configs:</span><br><span class="line">	Topic: LxmTest	Partition: 0	Leader: 1	Replicas: 1,0	Isr: 1,0</span><br><span class="line">	Topic: LxmTest	Partition: 1	Leader: 2	Replicas: 2,1	Isr: 2,1</span><br><span class="line">	Topic: LxmTest	Partition: 2	Leader: 0	Replicas: 0,2	Isr: 0,2</span><br></pre></td></tr></table></figure>

<h6 id="删除-Topic"><a href="#删除-Topic" class="headerlink" title="删除 Topic"></a>删除 Topic</h6><p>删除 Topic 分为手动和自动两种方式</p>
<ul>
<li>手动删除 Topic ：需要先删除 Zookeeper 上的相关元数据，然后删除各个 Broker 节点上日志目录的下的此 Topic 的分区日志。</li>
<li>自动删除 Topic ：首先 <code>delete.topic.enable=true</code> 参数必须配置，然后通过 客户端命令删除 Topic，将会自动将元数据和各分区的数据日志删除。如果未配置，将只会删除元数据，不会自动删除日志。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-topics.sh --zookeeper tnode3:2181  --delete --topic LxmTest</span><br><span class="line">Topic LxmTest is marked for deletion.</span><br><span class="line">Note: This will have no impact if delete.topic.enable is not set to true.</span><br></pre></td></tr></table></figure>

<h6 id="增加分区"><a href="#增加分区" class="headerlink" title="增加分区"></a>增加分区</h6><p>一般来说，我们在需要提升较大吞吐的情况下，可以选择增加 partition 。扩展分区命令如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-topics.sh --zookeeper tnode3:2181 --topic LxmTest  --alter --partitions 3</span><br><span class="line">WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affected</span><br><span class="line">Error while executing topic command : The number of partitions for a topic can only be increased</span><br><span class="line">[2019-08-23 08:42:20,176] ERROR kafka.admin.AdminOperationException: The number of partitions for a topic can only be increased</span><br><span class="line">	at kafka.admin.AdminUtils$.addPartitions(AdminUtils.scala:271)</span><br><span class="line">	at kafka.admin.TopicCommand$$anonfun$alterTopic$1.apply(TopicCommand.scala:145)</span><br><span class="line">	at kafka.admin.TopicCommand$$anonfun$alterTopic$1.apply(TopicCommand.scala:122)</span><br><span class="line">	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)</span><br><span class="line">	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)</span><br><span class="line">	at kafka.admin.TopicCommand$.alterTopic(TopicCommand.scala:122)</span><br><span class="line">	at kafka.admin.TopicCommand$.main(TopicCommand.scala:62)</span><br><span class="line">	at kafka.admin.TopicCommand.main(TopicCommand.scala)</span><br><span class="line"> (kafka.admin.TopicCommand$)</span><br><span class="line">[root@tnode1 bin]# ./kafka-topics.sh --zookeeper tnode3:2181 --topic LxmTest  --alter --partitions 4</span><br><span class="line">WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affected</span><br><span class="line">Adding partitions succeeded!</span><br></pre></td></tr></table></figure>

<p>目前分区只能增加，不能减少或者不变的进行修改。</p>
<h4 id="配置管理"><a href="#配置管理" class="headerlink" title="配置管理"></a>配置管理</h4><p>配置管理分为 Topic 级别和 Client 级别两种</p>
<h6 id="Topics-级别"><a href="#Topics-级别" class="headerlink" title="Topics  级别"></a>Topics  级别</h6><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --describe</span><br><span class="line">Configs for topic 'LxmTest' are</span><br></pre></td></tr></table></figure>

<p>这里的结果表示没有做单独配置，均使用的集群默认配置。</p>
<p>增加一个配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --alter --add-config flush.messages=2</span><br><span class="line">Completed Updating config for entity: topic 'LxmTest'.</span><br><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --describe</span><br><span class="line">Configs for topic 'LxmTest' are flush.messages=2</span><br></pre></td></tr></table></figure>

<p>再增加一个配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --alter --add-config retention.ms=259200000</span><br><span class="line">Completed Updating config for entity: topic 'LxmTest'.</span><br><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --describe</span><br><span class="line">Configs for topic 'LxmTest' are retention.ms=259200000,flush.messages=2</span><br></pre></td></tr></table></figure>

<p>一次性添加多个配置，配置用逗号分隔即可</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --alter --add-config retention.ms=259200000,flush.messages=6</span><br><span class="line">Completed Updating config for entity: topic 'LxmTest'.</span><br><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --describe</span><br><span class="line">Configs for topic 'LxmTest' are retention.ms=259200000,flush.messages=6</span><br></pre></td></tr></table></figure>

<p>修改其中一个配置，其实增加没有区别</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --alter --add-config flush.messages=5</span><br><span class="line">Completed Updating config for entity: topic 'LxmTest'.</span><br><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --describe</span><br><span class="line">Configs for topic 'LxmTest' are retention.ms=259200000,flush.messages=5</span><br></pre></td></tr></table></figure>

<p>删除一个配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --alter --delete-config retention.ms</span><br><span class="line">Completed Updating config for entity: topic 'LxmTest'.</span><br><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --describe</span><br><span class="line">Configs for topic 'LxmTest' are flush.messages=5</span><br></pre></td></tr></table></figure>

<h6 id="Clients-级别"><a href="#Clients-级别" class="headerlink" title="Clients 级别"></a>Clients 级别</h6><h4 id="分区管理"><a href="#分区管理" class="headerlink" title="分区管理"></a>分区管理</h4><h6 id="分区平衡"><a href="#分区平衡" class="headerlink" title="分区平衡"></a>分区平衡</h6><p>分区平衡分为自动和手动两种：</p>
<ul>
<li>自动：在 server.properties 中配置参数 <code>auto.leader.rebalance.enable = true</code> ，那么集群将会定时进行分区平衡，将 prefer-replica 副本即AR列表第一个副本，置为 leader 副本。 (<code>leader.imbalance.per.broker.percentage = 10</code> 此参数来决定自动平衡的阈值即失衡率超过10%会开始平衡)</li>
<li>手动：执行平衡命令：<code>./kafka-preferred-replica-election.sh --zookeeper tnode3:2181</code></li>
</ul>
<h6 id="分区迁移"><a href="#分区迁移" class="headerlink" title="分区迁移"></a>分区迁移</h6><p>如果我们需要下线一个 broker 节点，那么就需要将该节点上的相关 Topic 的 partition 迁移到其他 broker 节点上去，因为 Kafka 不会自动进行分区迁移的操作，如果不进行手动迁移，就会发生副本实效和数据丢失的情况。同样，如果是我们新上线一个节点，只有新增 Topic 的分区才会分配到新节点，此时也需要进行分区迁移操作。</p>
<p>这个 Topic 目前三个分区分别在 0、1、2 三个 broker 上。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-topics.sh  --zookeeper tnode3:2181 --describe --topic LxmTest</span><br><span class="line">Topic:LxmTest	PartitionCount:3	ReplicationFactor:2	Configs:retention.ms=259200000,flush.messages=6</span><br><span class="line">	Topic: LxmTest	Partition: 0	Leader: 0	Replicas: 0,2	Isr: 0,2</span><br><span class="line">	Topic: LxmTest	Partition: 1	Leader: 1	Replicas: 1,0	Isr: 1,0</span><br><span class="line">	Topic: LxmTest	Partition: 2	Leader: 2	Replicas: 2,1	Isr: 2,1</span><br></pre></td></tr></table></figure>

<p>现在，我们需要下线 broker2，也就是我们需要将 partition 2 迁移到 0,1 broker 上，步骤如下：</p>
<ol>
<li><p>先创建一个分区迁移的配置文件，这里叫  <code>topic-move.json</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    "topics":</span><br><span class="line">    	[	</span><br><span class="line">    		&#123;</span><br><span class="line">            	"topic":"LxmTest"</span><br><span class="line">        	&#125;</span><br><span class="line">    	],</span><br><span class="line">    "version":1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>然后执行命令生成迁移计划</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-reassign-partitions.sh --zookeeper tnode3:2181 --topics-to-move-json-file topic-move.json --broker-list "0,1" --generate</span><br><span class="line">Current partition replica assignment</span><br><span class="line">&#123;"version":1,"partitions":[&#123;"topic":"LxmTest","partition":2,"replicas":[2,1]&#125;,                            &#123;"topic":"LxmTest","partition":0,"replicas":[0,2]&#125;,                            &#123;"topic":"LxmTest","partition":1,"replicas":[1,0]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Proposed partition reassignment configuration</span><br><span class="line">&#123;"version":1,"partitions":[&#123;"topic":"LxmTest","partition":2,"replicas":[1,0]&#125;,                            &#123;"topic":"LxmTest","partition":0,"replicas":[1,0]&#125;,                            &#123;"topic":"LxmTest","partition":1,"replicas":[0,1]&#125;]&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>然后将上面的 <code>Proposed partition reassignment configuration</code> 下面的内容复制到 <code>topic-repartition.json</code> 文件中</p>
</li>
<li><p>执行迁移计划</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# vi topic-repartition.json</span><br><span class="line">[root@tnode1 bin]# ./kafka-reassign-partitions.sh --zookeeper tnode3:2181 --reassignment-json-file topic-repartition.json --execute</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;"version":1,"partitions":[&#123;"topic":"LxmTest","partition":2,"replicas":[2,1]&#125;,                            &#123;"topic":"LxmTest","partition":0,"replicas":[0,2]&#125;,                            &#123;"topic":"LxmTest","partition":1,"replicas":[1,0]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started reassignment of partitions.</span><br></pre></td></tr></table></figure>
</li>
<li><p>验证迁移是否成功</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-reassign-partitions.sh --zookeeper tnode3:2181 --reassignment-json-file topic-repartition.json --verify</span><br><span class="line">Status of partition reassignment: </span><br><span class="line">Reassignment of partition [LxmTest,2] completed successfully</span><br><span class="line">Reassignment of partition [LxmTest,0] completed successfully</span><br><span class="line">Reassignment of partition [LxmTest,1] completed successfully</span><br></pre></td></tr></table></figure>
</li>
<li><p>再次查看 topic 信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-topics.sh  --zookeeper tnode3:2181 --describe --topic LxmTest</span><br><span class="line">Topic:LxmTest	PartitionCount:3	ReplicationFactor:2	Configs:retention.ms=259200000,flush.messages=6</span><br><span class="line">	Topic: LxmTest	Partition: 0	Leader: 0	Replicas: 1,0	Isr: 0,1</span><br><span class="line">	Topic: LxmTest	Partition: 1	Leader: 1	Replicas: 0,1	Isr: 1,0</span><br><span class="line">	Topic: LxmTest	Partition: 2	Leader: 1	Replicas: 1,0	Isr: 1,0</span><br></pre></td></tr></table></figure>

<p>发现已经没有 broker 2 节点的分区了 </p>
</li>
</ol>
<h6 id="集群扩容"><a href="#集群扩容" class="headerlink" title="集群扩容"></a>集群扩容</h6><p>集群扩容很好理解，就是Kafka磁盘容量不足了，需要增加 broker 节点，然后迁移分区。和上面的 broker下线操作类似，只不是生成迁移计划的时候，增加 broker 而已。具体代码就不贴了。</p>
<p>总得来说，其实集群迁移，无论是 broker 下线 还是 集群扩容 或者是 增加副本，我们都只要手动梳理迁移计划，尽量将各个分区平衡分配，同时将 AR 列表第一个副本 (即默认的 leader 副本) 平均分配，然后进行迁移即可。</p>
<p><strong>！## ！大量 topic 进行迁移时，最好分批进行，避免影响正常业务。另外，可以限制迁移的流量。</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-reassign-partitions.sh --zookeeper tnode3:2181 --reassignment-json-file topic-repartition.json --execute --throttle 1024</span><br></pre></td></tr></table></figure>

<p><strong>！## ！如果做了流量限制，需要使用 –verify 来检查是否完成迁移，在完成时，–verify 参数会解除限流</strong></p>
<h2 id="kafka设置某个topic的数据过期时间"><a href="#kafka设置某个topic的数据过期时间" class="headerlink" title="kafka设置某个topic的数据过期时间"></a>kafka设置某个topic的数据过期时间</h2><h6 id="全局设置"><a href="#全局设置" class="headerlink" title="全局设置"></a>全局设置</h6><p>修改 server.properties</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">log.retention.hours&#x3D;72</span><br><span class="line">log.cleanup.policy&#x3D;delete</span><br></pre></td></tr></table></figure>

<h6 id="单独对某一个topic设置过期时间"><a href="#单独对某一个topic设置过期时间" class="headerlink" title="单独对某一个topic设置过期时间"></a>单独对某一个topic设置过期时间</h6><p>但如果只有某一个topic数据量过大。想单独对这个topic的过期时间设置短点：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;kafka-configs.sh --zookeeper localhost:2181 --alter --entity-name mytopic --entity-type topics --add-config retention.ms&#x3D;86400000</span><br></pre></td></tr></table></figure>

<p>retention.ms = 86400000 为一天，单位是毫秒。</p>
<p>查看设置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;kafka-configs.sh --zookeeper localhost:2181 --describe --entity-name mytopic --entity-type topics</span><br><span class="line"></span><br><span class="line">Configs for mytopic are retention.ms&#x3D;86400000</span><br></pre></td></tr></table></figure>

<h6 id="立即删除某个topic下的数据"><a href="#立即删除某个topic下的数据" class="headerlink" title="立即删除某个topic下的数据"></a>立即删除某个topic下的数据</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;kafka-topics.sh --zookeeper localhost:2181 --alter --topic mytopic --config cleanup.policy&#x3D;delete</span><br></pre></td></tr></table></figure>









]]></content>
      <categories>
        <category>kafka</category>
        <category>基础操作</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka问题记录</title>
    <url>/2020/10/09/Kafka%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<p>日志：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Exception in thread &quot;Thread-0&quot; org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.</span><br><span class="line">	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.sendOffsetCommitRequest(ConsumerCoordinator.java:702)</span><br><span class="line">	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsSync(ConsumerCoordinator.java:581)</span><br><span class="line">	at org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:1090)</span><br><span class="line">	at com.eebbk.da.kafka2HDFS.handle.KafkaToParquetAutoOffset.run(KafkaToParquetAutoOffset.java:287)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure>

<p>查看日志，因为处理poll到数据的时间超过了 <code>max.poll.interval.ms</code>  导致 <code>consumer</code> 发生了 <code>rebalance</code> ，从而无法手动提交 <code>offset</code>。</p>
<p>溯源：影响处理poll数据的参数是 <code>max.poll.records</code>，因为此参数设置过大，导致超  <code>max.poll.interval.ms</code>  时间。（<code>max.poll.records</code>默认500条，<code>max.poll.interval.ms</code> 默认300000）</p>
<p>解决办法：降低吞吐量，即将 <code>max.poll.records</code> 调小，目前环境我设置的为5000，调整为2000</p>
<p>​                 调整超时时间，将 <code>max.poll.interval.ms</code>  调整为 600000</p>
<p>​                 在手动提交失败是，跳过本次提交，继续处理，期待下一次提交。</p>
<h3 id="KAFKA-broker一直起不来，有时候有报错信息，有时候直接崩溃。"><a href="#KAFKA-broker一直起不来，有时候有报错信息，有时候直接崩溃。" class="headerlink" title="KAFKA broker一直起不来，有时候有报错信息，有时候直接崩溃。"></a>KAFKA broker一直起不来，有时候有报错信息，有时候直接崩溃。</h3><p>启动日志：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">java.io.IOException: Map failed</span><br><span class="line">        at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:888)</span><br><span class="line">        at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:111)</span><br><span class="line">        at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:101)</span><br><span class="line">        at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:213)</span><br><span class="line">        at kafka.log.AbstractIndex.resize(AbstractIndex.scala:101)</span><br><span class="line">        at kafka.log.LogSegment.truncateTo(LogSegment.scala:292)</span><br><span class="line">        at kafka.log.Log.truncateTo(Log.scala:893)</span><br><span class="line">        at kafka.log.LogManager$$anonfun$truncateTo$2.apply(LogManager.scala:301)</span><br><span class="line">        at kafka.log.LogManager$$anonfun$truncateTo$2.apply(LogManager.scala:293)</span><br><span class="line">        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)</span><br><span class="line">        at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)</span><br><span class="line">        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)</span><br><span class="line">        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)</span><br><span class="line">        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)</span><br><span class="line">        at kafka.log.LogManager.truncateTo(LogManager.scala:293)</span><br><span class="line">        at kafka.server.ReplicaManager.makeFollowers(ReplicaManager.scala:854)</span><br><span class="line">        at kafka.server.ReplicaManager.becomeLeaderOrFollower(ReplicaManager.scala:700)</span><br><span class="line">        at kafka.server.KafkaApis.handleLeaderAndIsrRequest(KafkaApis.scala:148)</span><br><span class="line">        at kafka.server.KafkaApis.handle(KafkaApis.scala:84)</span><br><span class="line">        at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:62)</span><br><span class="line">        at java.lang.Thread.run(Thread.java:745)</span><br><span class="line">Caused by: java.lang.OutOfMemoryError: Map failed</span><br><span class="line">        at sun.nio.ch.FileChannelImpl.map0(Native Method)</span><br><span class="line">        at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:885)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">其他直接JVM崩溃</span><br></pre></td></tr></table></figure>

<p>JVM崩溃日志：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#</span><br><span class="line"># There is insufficient memory for the Java Runtime Environment to continue.</span><br><span class="line"># Native memory allocation (malloc) failed to allocate 312 bytes for AllocateHeap</span><br><span class="line"># Possible reasons:</span><br><span class="line">#   The system is out of physical RAM or swap space</span><br><span class="line">#   In 32 bit mode, the process size limit was hit</span><br><span class="line"># Possible solutions:</span><br><span class="line">#   Reduce memory load on the system</span><br><span class="line">#   Increase physical memory or swap space</span><br><span class="line">#   Check if swap backing store is full</span><br><span class="line">#   Use 64 bit Java on a 64 bit OS</span><br><span class="line">#   Decrease Java heap size (-Xmx&#x2F;-Xms)</span><br><span class="line">#   Decrease number of Java threads</span><br><span class="line">#   Decrease Java thread stack sizes (-Xss)</span><br><span class="line">#   Set larger code cache with -XX:ReservedCodeCacheSize&#x3D;</span><br><span class="line"># This output file may be truncated or incomplete.</span><br><span class="line">#</span><br><span class="line">#  Out of Memory Error (allocation.inline.hpp:61), pid&#x3D;56865, tid&#x3D;0x00007fb2aa587700</span><br><span class="line">#</span><br><span class="line"># JRE version: Java(TM) SE Runtime Environment (8.0_131-b11) (build 1.8.0_131-b11)</span><br><span class="line"># Java VM: Java HotSpot(TM) 64-Bit Server VM (25.131-b11 mixed mode linux-amd64 compressed oops)</span><br><span class="line"># Core dump written. Default location: &#x2F;data1&#x2F;kafka&#x2F;kafka_2.11-0.10.2.1&#x2F;core or core.56865</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line">---------------  T H R E A D  ---------------</span><br><span class="line"></span><br><span class="line">Current thread (0x00007fcaa1e3f000):  JavaThread &quot;ExpirationReaper-5&quot; [_thread_in_vm, id&#x3D;57248, stack(0x00007fb2aa487000,0x00007fb2aa588000)]</span><br><span class="line"></span><br><span class="line">Stack: [0x00007fb2aa487000,0x00007fb2aa588000],  sp&#x3D;0x00007fb2aa586900,  free space&#x3D;1022k</span><br><span class="line">Native frames: (J&#x3D;compiled Java code, j&#x3D;interpreted, Vv&#x3D;VM code, C&#x3D;native code)</span><br><span class="line">V  [libjvm.so+0xac826a]</span><br><span class="line">V  [libjvm.so+0x4fd4cb]</span><br><span class="line">V  [libjvm.so+0x2e23c8]</span><br><span class="line">V  [libjvm.so+0x2e2444]</span><br><span class="line">V  [libjvm.so+0x70f40a]</span><br><span class="line">V  [libjvm.so+0xa76910]</span><br><span class="line">V  [libjvm.so+0x927568]</span><br><span class="line">C  [libpthread.so.0+0x7aa1]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">---------------  P R O C E S S  ---------------</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">power management:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Memory: 4k page, physical 65860516k(289912k free), swap 20479996k(19336528k free)</span><br><span class="line"></span><br><span class="line">vm_info: Java HotSpot(TM) 64-Bit Server VM (25.131-b11) for linux-amd64 JRE (1.8.0_131-b11), built on Mar 15 2017 01:23:40 by &quot;java_re&quot; with gcc 4.3.0 20080428 (Red Hat 4.3.0-8)</span><br><span class="line"></span><br><span class="line">time: Wed Jun 10 08:48:32 2020</span><br><span class="line">elapsed time: 322 seconds (0d 0h 5m 22s)</span><br></pre></td></tr></table></figure>

<p>搜索  <a href="https://blog.csdn.net/b644ROfP20z37485O35M/article/details/81571710" target="_blank" rel="noopener">https://blog.csdn.net/b644ROfP20z37485O35M/article/details/81571710</a></p>
<h5 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h5><p>从上面分析解决问题的方法有两个</p>
<ul>
<li>增大系统限制<code>/proc/sys/vm/max_map_count</code>  <code>vm.max_map_count=200000直接写到/etc/sysctl.conf中,然后执行sysctl -p</code></li>
<li>kafka的索引文件是否不需要一直有？是否可以限制一下</li>
</ul>
<h5 id="问题总结"><a href="#问题总结" class="headerlink" title="问题总结"></a>问题总结</h5><p>上面的过程是我思考的一个过程，可能过程有些绕，不过我这里可以来个简单的概述，描述下整个问题发生的过程：</p>
<p>kafka做了很多索引文件的内存映射，每个索引文件占用的内存还很大，这些索引文件并且一直占着没有释放，于是随着索引文件数的增多，而慢慢达到了物理内存的一个上限，比如映射到某个索引文件的时候，物理内存还剩1G，但是我们要映射一个超过1G的文件，因此会映射失败，映射失败接着就做了一次System GC，而在System GC过程中因为PermSize和MaxPermSize不一样，从而导致了在Full GC完之后Perm进行扩容，在扩容的时候因为又调用了一次mmap，而在mmap的时候check是否达到了vma的最大上限，也就是<code>/proc/sys/vm/max_map_count</code>里的65530，如果超过了，就直接crash了。</p>
<p>这只是我从此次crash文件里能想像到的一个现场，当然其实可能会有更多的场景，只要是能触发mmap动作的地方都有可能是导致crash的案发现场，比如后面又给了我一个crash文件，是在创建线程栈的时候因为mmap而导致的crash，完全和OOM没有关系，所以根本原因还是因为kafka做了太多的索引文件映射，导致mmap的vma非常多，超过了系统的限制，从而导致了crash。</p>
]]></content>
      <categories>
        <category>kafka</category>
        <category>基础操作</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka集群扩容操作记录</title>
    <url>/2020/10/09/Kafka%E9%9B%86%E7%BE%A4%E6%89%A9%E5%AE%B9%E6%93%8D%E4%BD%9C%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<h1 id="kafka集群扩容记录"><a href="#kafka集群扩容记录" class="headerlink" title="kafka集群扩容记录"></a>kafka集群扩容记录</h1><p>因为我们需要扩容 3 个broker，即broker从 0,1,2,3,4,5 扩容到 0,1,2,3,4,5,6,7,8 ，所以先进行第一部分工作</p>
<h2 id="集群扩容"><a href="#集群扩容" class="headerlink" title="集群扩容"></a>集群扩容</h2><ul>
<li><p>新机器的 6,7,8 的准备：JAVA、文件句柄数、防火墙、主机名以及hosts映射</p>
</li>
<li><p>从 0,1,2,3,4,5 选一台机器，拷贝其上的 kafka 安装目录到 6,7,8 三台机器，删除其中的logs目录，修改 server.properties 文件中的 broker.id 项。</p>
</li>
<li><p>新机器启动kafka：</p>
<p><code>sh kafka-server-start.sh -daemon ../config/server.properties</code></p>
</li>
<li><p>检查新broker是否加入成功，检查zookeeper上的/broker/ids下的node是否包含新增broker.id</p>
</li>
</ul>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><h5 id="查看现有Topic的partition、副本以及leader和follower分布。"><a href="#查看现有Topic的partition、副本以及leader和follower分布。" class="headerlink" title="查看现有Topic的partition、副本以及leader和follower分布。"></a>查看现有Topic的partition、副本以及leader和follower分布。</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@gs-kafka1 kafka_2.11-0.10.2.1]# bin&#x2F;kafka-topics.sh --describe --zookeeper gs-kafka1:2181 --topic WATCH-LOCATION</span><br><span class="line">Topic:WATCH-LOCATION	PartitionCount:18	ReplicationFactor:3	Configs:</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 0	Leader: 4	Replicas: 4,2,3	Isr: 3,4,2</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 1	Leader: 5	Replicas: 5,3,4	Isr: 5,3,4</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 2	Leader: 0	Replicas: 0,4,5	Isr: 5,0,4</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 3	Leader: 1	Replicas: 1,5,0	Isr: 5,0,1</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 4	Leader: 2	Replicas: 2,0,1	Isr: 2,0,1</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 5	Leader: 3	Replicas: 3,1,2	Isr: 3,2,1</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 6	Leader: 4	Replicas: 4,3,5	Isr: 5,3,4</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 7	Leader: 5	Replicas: 5,4,0	Isr: 5,0,4</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 8	Leader: 0	Replicas: 0,5,1	Isr: 5,0,1</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 9	Leader: 1	Replicas: 1,0,2	Isr: 0,1,2</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 10	Leader: 2	Replicas: 2,1,3	Isr: 2,3,1</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 11	Leader: 3	Replicas: 3,2,4	Isr: 3,2,4</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 12	Leader: 4	Replicas: 4,5,0	Isr: 5,0,4</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 13	Leader: 5	Replicas: 5,0,1	Isr: 5,0,1</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 14	Leader: 0	Replicas: 0,1,2	Isr: 0,2,1</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 15	Leader: 1	Replicas: 1,2,3	Isr: 1,3,2</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 16	Leader: 2	Replicas: 2,3,4	Isr: 2,4,3</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 17	Leader: 3	Replicas: 3,4,5	Isr: 5,3,4</span><br></pre></td></tr></table></figure>

<p>因为现有topic的副本及其leader都在 0,1,2,3,4,5 broker，所以我们需要迁移三分之一的副本到新broker，并且将三分之一的leader转到新broker，才能做到网络和磁盘IO的扩容。</p>
<h2 id="迁移"><a href="#迁移" class="headerlink" title="迁移"></a>迁移</h2><h6 id="参考扩容方案中的方法，我们直接编辑-topic-result-json，格式如下"><a href="#参考扩容方案中的方法，我们直接编辑-topic-result-json，格式如下" class="headerlink" title="参考扩容方案中的方法，我们直接编辑 topic-result.json，格式如下"></a>参考扩容方案中的方法，我们直接编辑 topic-result.json，格式如下</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:0, &quot;replicas&quot;:[4,2,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:1, &quot;replicas&quot;:[5,3,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:2, &quot;replicas&quot;:[0,4,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:3, &quot;replicas&quot;:[1,5,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:4, &quot;replicas&quot;:[2,0,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:5, &quot;replicas&quot;:[3,1,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:6, &quot;replicas&quot;:[4,3,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:7, &quot;replicas&quot;:[5,4,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:8, &quot;replicas&quot;:[0,5,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:9, &quot;replicas&quot;:[1,0,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:10,&quot;replicas&quot;:[2,1,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:11,&quot;replicas&quot;:[3,2,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:12,&quot;replicas&quot;:[4,5,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:13,&quot;replicas&quot;:[5,0,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:14,&quot;replicas&quot;:[0,1,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:15,&quot;replicas&quot;:[1,2,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:16,&quot;replicas&quot;:[2,3,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:17,&quot;replicas&quot;:[3,4,8]&#125;</span><br><span class="line">                           ]&#125;</span><br></pre></td></tr></table></figure>

<p>但是上述只考虑到了副本迁移，未考虑到leader迁移，参考扩容方案，我们迁移的时候顺便 修改replicas顺序，修改如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:0, &quot;replicas&quot;:[4,2,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:1, &quot;replicas&quot;:[5,3,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:2, &quot;replicas&quot;:[0,4,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:3, &quot;replicas&quot;:[1,5,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:4, &quot;replicas&quot;:[2,0,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:5, &quot;replicas&quot;:[3,1,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:6, &quot;replicas&quot;:[4,3,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:7, &quot;replicas&quot;:[5,4,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:8, &quot;replicas&quot;:[0,5,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:9, &quot;replicas&quot;:[6,0,1]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:10,&quot;replicas&quot;:[7,1,2]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:11,&quot;replicas&quot;:[8,2,3]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:12,&quot;replicas&quot;:[6,5,4]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:13,&quot;replicas&quot;:[7,0,5]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:14,&quot;replicas&quot;:[8,1,0]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:15,&quot;replicas&quot;:[1,2,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:16,&quot;replicas&quot;:[2,3,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:17,&quot;replicas&quot;:[3,4,8]&#125;</span><br><span class="line">                           ]&#125;</span><br></pre></td></tr></table></figure>

<p>其他 topic 也可参考此方法一起修改副本位置和副本顺序。然后开始迁移</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;kafka-reassign-partitions.sh --zookeeper gs-kafka1:2181 --reassignment-json-file topic-result.json  --execute</span><br></pre></td></tr></table></figure>


<h2 id="等所有topic迁移完成后，开始leader迁移，两个方法"><a href="#等所有topic迁移完成后，开始leader迁移，两个方法" class="headerlink" title="等所有topic迁移完成后，开始leader迁移，两个方法"></a>等所有topic迁移完成后，开始leader迁移，两个方法</h2><ul>
<li>一次性平衡所有topic<br><code>bin/kafka-preferred-replica-election.sh --zookeeper gs-kafka1:2181</code></li>
<li>每次平衡部分topic，编写 topicPartition.json ，格式如下。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:0&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:1&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:2&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:3&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:4&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:5&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:6&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:7&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:8&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:9&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:10&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:11&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:12&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:13&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:14&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:15&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:16&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:17&#125;</span><br><span class="line">                           ]&#125;</span><br></pre></td></tr></table></figure>

<p>然后平衡 <code>bin/kafka-preferred-replica-election.sh --zookeeper gs-kafka1:2181 --path-to-json-file topicPartition.json</code></p>
]]></content>
      <categories>
        <category>kafka</category>
        <category>基础操作</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka集群扩容</title>
    <url>/2020/10/08/Kafka%E9%9B%86%E7%BE%A4%E6%89%A9%E5%AE%B9/</url>
    <content><![CDATA[<p>根据官方脚本：kafka-reassign-partitions.sh<br>直接看脚本帮助  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost bin]# .&#x2F;kafka-reassign-partitions.sh </span><br><span class="line">This command moves topic partitions between replicas.</span><br><span class="line">Option                                Description                           </span><br><span class="line">------                                -----------                           </span><br><span class="line">--broker-list &lt;String: brokerlist&gt;    The list of brokers to which the      </span><br><span class="line">                                        partitions need to be reassigned in </span><br><span class="line">                                        the form &quot;0,1,2&quot;. This is required  </span><br><span class="line">                                        if --topics-to-move-json-file is    </span><br><span class="line">                                        used to generate reassignment       </span><br><span class="line">                                        configuration                       </span><br><span class="line">--disable-rack-aware                  Disable rack aware replica assignment </span><br><span class="line">--execute                             Kick off the reassignment as specified</span><br><span class="line">                                        by the --reassignment-json-file     </span><br><span class="line">                                        option.                             </span><br><span class="line">--generate                            Generate a candidate partition        </span><br><span class="line">                                        reassignment configuration. Note    </span><br><span class="line">                                        that this only generates a candidate</span><br><span class="line">                                        assignment, it does not execute it. </span><br><span class="line">--reassignment-json-file &lt;String:     The JSON file with the partition      </span><br><span class="line">  manual assignment json file path&gt;     reassignment configurationThe format</span><br><span class="line">                                        to use is -                         </span><br><span class="line">                                      &#123;&quot;partitions&quot;:                        </span><br><span class="line">                                      	[&#123;&quot;topic&quot;: &quot;foo&quot;,                    </span><br><span class="line">                                      	  &quot;partition&quot;: 1,                    </span><br><span class="line">                                      	  &quot;replicas&quot;: [1,2,3] &#125;],            </span><br><span class="line">                                      &quot;version&quot;:1                           </span><br><span class="line">                                      &#125;                                     </span><br><span class="line">--throttle &lt;Long: throttle&gt;           The movement of partitions will be    </span><br><span class="line">                                        throttled to this value (bytes&#x2F;sec).</span><br><span class="line">                                        Rerunning with this option, whilst a</span><br><span class="line">                                        rebalance is in progress, will alter</span><br><span class="line">                                        the throttle value. The throttle    </span><br><span class="line">                                        rate should be at least 1 KB&#x2F;s.     </span><br><span class="line">                                        (default: -1)                       </span><br><span class="line">--topics-to-move-json-file &lt;String:   Generate a reassignment configuration </span><br><span class="line">  topics to reassign json file path&gt;    to move the partitions of the       </span><br><span class="line">                                        specified topics to the list of     </span><br><span class="line">                                        brokers specified by the --broker-  </span><br><span class="line">                                        list option. The format to use is - </span><br><span class="line">                                      &#123;&quot;topics&quot;:                            </span><br><span class="line">                                      	[&#123;&quot;topic&quot;: &quot;foo&quot;&#125;,&#123;&quot;topic&quot;: &quot;foo1&quot;&#125;],</span><br><span class="line">                                      &quot;version&quot;:1                           </span><br><span class="line">                                      &#125;                                     </span><br><span class="line">--verify                              Verify if the reassignment completed  </span><br><span class="line">                                        as specified by the --reassignment- </span><br><span class="line">                                        json-file option. If there is a     </span><br><span class="line">                                        throttle engaged for the replicas   </span><br><span class="line">                                        specified, and the rebalance has    </span><br><span class="line">                                        completed, the throttle will be     </span><br><span class="line">                                        removed                             </span><br><span class="line">--zookeeper &lt;String: urls&gt;            REQUIRED: The connection string for   </span><br><span class="line">                                        the zookeeper connection in the form</span><br><span class="line">                                        host:port. Multiple URLS can be     </span><br><span class="line">                                        given to allow fail-over.</span><br></pre></td></tr></table></figure>



<h2 id="几个重要的参数"><a href="#几个重要的参数" class="headerlink" title="几个重要的参数"></a>几个重要的参数</h2><h4 id="三种模式"><a href="#三种模式" class="headerlink" title="三种模式"></a>三种模式</h4><h6 id="–generate"><a href="#–generate" class="headerlink" title="–generate"></a>–generate</h6><p>给定需要重新分配的Topic，自动生成reassign plan，并不会执行</p>
<h6 id="–execute"><a href="#–execute" class="headerlink" title="–execute"></a>–execute</h6><p>根据指定的reassign plan重新分配Partition</p>
<h6 id="–verify"><a href="#–verify" class="headerlink" title="–verify"></a>–verify</h6><p>验证重新分配Partition是否成功</p>
<h4 id="两个文件参数"><a href="#两个文件参数" class="headerlink" title="两个文件参数"></a>两个文件参数</h4><h6 id="–topics-to-move-json-file"><a href="#–topics-to-move-json-file" class="headerlink" title="–topics-to-move-json-file"></a>–topics-to-move-json-file</h6><p>需要进行重新分配的Topic配置文件，格式如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&quot;partitions&quot;:                        </span><br><span class="line">    [&#123;&quot;topic&quot;: &quot;foo&quot;,                    </span><br><span class="line">      &quot;partition&quot;: 1,                    </span><br><span class="line">      &quot;replicas&quot;: [1,2,3] &#125;],            </span><br><span class="line">      &quot;version&quot;:1                           </span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>文件名不重要，保证内容是json格式就行了。</p>
<h6 id="–reassignment-json-file"><a href="#–reassignment-json-file" class="headerlink" title="–reassignment-json-file"></a>–reassignment-json-file</h6><p>根据上面的配置文件生成的reassign plan。格式如下，可以自己修改。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&quot;topics&quot;:                            </span><br><span class="line">  [&#123;&quot;topic&quot;: &quot;foo&quot;&#125;,&#123;&quot;topic&quot;: &quot;foo1&quot;&#125;],</span><br><span class="line">    &quot;version&quot;:1                           </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="两个集群参数"><a href="#两个集群参数" class="headerlink" title="两个集群参数"></a>两个集群参数</h4><h6 id="–zookeeper"><a href="#–zookeeper" class="headerlink" title="–zookeeper"></a>–zookeeper</h6><p>zk1:2181,zk2:2181</p>
<h6 id="–broker-list"><a href="#–broker-list" class="headerlink" title="–broker-list"></a>–broker-list</h6><p>具体数值为各个broker下面的配置ID，”1,2,3,4,5”。该参数为指定新分配到的broker节点，即，假设原始broker是 “1,2,3,4”，扩容一台，则为”1,2,3,4,5”。</p>
<h2 id="官方方案"><a href="#官方方案" class="headerlink" title="官方方案"></a>官方方案</h2><blockquote>
<ul>
<li>写好 topics-to-move-json-file 配置文件 tp-file.json</li>
<li>使用命令生成 reassign plan即 reassignment-json-file</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;.&#x2F;kafka-reassign-partitions.sh --zookeeper zk1:2181 --topics-to-move-json-file tp-file.json --broker-list &quot;1,2,3,4&quot; --generate</span><br></pre></td></tr></table></figure>

<ul>
<li>(可选)根据需求自己定义或修改上面的 reassignment-json-file 文件</li>
<li>使用命令触发分配计划执行</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;.&#x2F;kafka-reassign-partitions.sh --zookeeper zk1:2181 --reassignment-json-file reassignment-json-file.json --execute</span><br></pre></td></tr></table></figure>

<ul>
<li>校验分配结果</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;.&#x2F;kafka-reassign-partitions.sh --zookeeper zk1:2181 --reassignment-json-file reassignment-json-file.json --verify</span><br></pre></td></tr></table></figure>
</blockquote>
<h2 id="扩容原理"><a href="#扩容原理" class="headerlink" title="扩容原理"></a>扩容原理</h2><p>一般来说，我们Kafka集群上的Topic的Partition数是大于broker数的，所以基本上，一个Topic的partition基本是遍布所有broker的。<br>所以我们扩容的根本原理就在于，将原来的Partition的副本进行迁移。整个迁移过程可能会涉及到副本leader和follower的迁移。  </p>
<h6 id="整体原理如下"><a href="#整体原理如下" class="headerlink" title="整体原理如下:"></a>整体原理如下:</h6><blockquote>
<ol>
<li>将副本数增加一份(无论扩容多少台)</li>
<li>新增副本开始从副本leader开始从头开始复制，即从earliest offset开始</li>
<li>等新增副本跟随上最新offset，将新增的副本添加到 ISR 列表</li>
<li>移除需要移除的broker上的副本</li>
</ol>
</blockquote>
<h6 id="这种方案的缺陷"><a href="#这种方案的缺陷" class="headerlink" title="这种方案的缺陷"></a>这种方案的缺陷</h6><blockquote>
<p>在数据量大的时候进行扩容是，因为要从头拷贝数据，会造成大量读原磁盘，消耗大量的I/O，造成producer操作缓慢，容易产生抖动。</p>
</blockquote>
<h2 id="改良方案"><a href="#改良方案" class="headerlink" title="改良方案"></a>改良方案</h2><p>优化点主要在第二步的从earliest offset。假设我们从latest offset开始复制数据，然后等待新副本保持稳定一段时间后，添加到ISR列表，再移除就副本。<br>具体代码可以看官方的issues，链接：<br><a href="https://issues.apache.org/jira/browse/KAFKA-8328" target="_blank" rel="noopener">官方代码：https://issues.apache.org/jira/browse/KAFKA-8328</a></p>
<h2 id="就官方方案进行测试"><a href="#就官方方案进行测试" class="headerlink" title="就官方方案进行测试"></a>就官方方案进行测试</h2><h6 id="查看-topic-信息"><a href="#查看-topic-信息" class="headerlink" title="查看 topic 信息"></a>查看 topic 信息</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost bin]# .&#x2F;kafka-topics.sh --describe --topic FLINK-TOPIC-1  --zookeeper tnode3:2181</span><br><span class="line">Topic:FLINK-TOPIC-1	PartitionCount:4	ReplicationFactor:2	Configs:</span><br><span class="line">	Topic: FLINK-TOPIC-1	Partition: 0	Leader: 0	Replicas: 0,2	Isr: 2,0</span><br><span class="line">	Topic: FLINK-TOPIC-1	Partition: 1	Leader: 1	Replicas: 1,0	Isr: 1,0</span><br><span class="line">	Topic: FLINK-TOPIC-1	Partition: 2	Leader: 2	Replicas: 2,1	Isr: 2,1</span><br><span class="line">	Topic: FLINK-TOPIC-1	Partition: 3	Leader: 0	Replicas: 0,1	Isr: 1,0</span><br></pre></td></tr></table></figure>

<h6 id="编辑-topics-to-move-json-file-需要的json格式文件"><a href="#编辑-topics-to-move-json-file-需要的json格式文件" class="headerlink" title="编辑 topics-to-move-json-file 需要的json格式文件"></a>编辑 topics-to-move-json-file 需要的json格式文件</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@tnode3 kafka_2.11-0.10.2.1]# cat flink-topic-json </span><br><span class="line">&#123;&quot;topics&quot;: [&#123;&quot;topic&quot;: &quot;FLINK-TOPIC-1&quot;&#125;],</span><br><span class="line"> &quot;version&quot;:1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="生成重分配计划-即-reassignment-json-file-所需要的文件"><a href="#生成重分配计划-即-reassignment-json-file-所需要的文件" class="headerlink" title="生成重分配计划 即 reassignment-json-file 所需要的文件"></a>生成重分配计划 即 reassignment-json-file 所需要的文件</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@tnode3 kafka_2.11-0.10.2.1]# bin&#x2F;kafka-reassign-partitions.sh --zookeeper tnode3:2181 --topics-to-move-json-file flink-topic-json --broker-list &quot;0,1,2,3&quot; --generate</span><br><span class="line">Current partition replica assignment</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[0,1]&#125;,                                &#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[2,1]&#125;,                                &#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[1,0]&#125;,                                &#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[0,2]&#125;</span><br><span class="line">                          ]&#125;</span><br><span class="line"></span><br><span class="line">Proposed partition reassignment configuration</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[2,3]&#125;,                                &#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,2]&#125;,                                &#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[0,1]&#125;,                                &#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[3,0]&#125;</span><br><span class="line">                          ]&#125;</span><br></pre></td></tr></table></figure>

<p>将 Proposed partition reassignment configuration 下面的内容保存到 flink-topic-result.json</p>
<h6 id="执行修改计划"><a href="#执行修改计划" class="headerlink" title="执行修改计划"></a>执行修改计划</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@tnode3 kafka_2.11-0.10.2.1]# bin&#x2F;kafka-reassign-partitions.sh --zookeeper tnode3:2181 --reassignment-json-file flink-topic-result.json --execute</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[0,1]&#125;,                                &#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[2,1]&#125;,                                &#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[1,0]&#125;,                                &#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[0,2]&#125;</span><br><span class="line">                          ]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started reassignment of partitions.</span><br></pre></td></tr></table></figure>

<p>上面有提到。可以再讲 当前的分配情况保存成新的计划文件，用来回滚重分配的操作。</p>
<h6 id="校验重分配是否成功"><a href="#校验重分配是否成功" class="headerlink" title="校验重分配是否成功"></a>校验重分配是否成功</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@tnode3 kafka_2.11-0.10.2.1]# bin&#x2F;kafka-reassign-partitions.sh --zookeeper tnode3:2181 --reassignment-json-file flink-topic-result.json --verify</span><br><span class="line">Status of partition reassignment: </span><br><span class="line">Reassignment of partition [FLINK-TOPIC-1,3] completed successfully</span><br><span class="line">Reassignment of partition [FLINK-TOPIC-1,2] completed successfully</span><br><span class="line">Reassignment of partition [FLINK-TOPIC-1,1] completed successfully</span><br><span class="line">Reassignment of partition [FLINK-TOPIC-1,0] completed successfully</span><br></pre></td></tr></table></figure>

<h6 id="修改副本顺序-利用kafka自动生成计划的跳过下面部分，因为自动生成的计划，已经调整了副本顺序"><a href="#修改副本顺序-利用kafka自动生成计划的跳过下面部分，因为自动生成的计划，已经调整了副本顺序" class="headerlink" title="修改副本顺序   (利用kafka自动生成计划的跳过下面部分，因为自动生成的计划，已经调整了副本顺序)"></a>修改副本顺序   (利用kafka自动生成计划的跳过下面部分，因为自动生成的计划，已经调整了副本顺序)</h6><p>此时，topic的副本已经迁移完成，但是所有副本的leader还在原来的broker上，我们需要分配一定的leader到新的broker上，所以还需要进行平衡leader。</p>
<p>手动编辑的计划(也可在编辑的时候就调整顺序，此处为只迁移副本的计划，不含调整副本顺序)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:0, &quot;replicas&quot;:[4,2,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:1, &quot;replicas&quot;:[5,3,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:2, &quot;replicas&quot;:[0,4,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:3, &quot;replicas&quot;:[1,5,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:4, &quot;replicas&quot;:[2,0,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:5, &quot;replicas&quot;:[3,1,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:6, &quot;replicas&quot;:[4,3,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:7, &quot;replicas&quot;:[5,4,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:8, &quot;replicas&quot;:[0,5,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:9, &quot;replicas&quot;:[1,0,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:10,&quot;replicas&quot;:[2,1,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:11,&quot;replicas&quot;:[3,2,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:12,&quot;replicas&quot;:[4,5,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:13,&quot;replicas&quot;:[5,0,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:14,&quot;replicas&quot;:[0,1,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:15,&quot;replicas&quot;:[1,2,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:16,&quot;replicas&quot;:[2,3,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:17,&quot;replicas&quot;:[3,4,8]&#125;</span><br><span class="line">                           ]&#125;</span><br></pre></td></tr></table></figure>

<p>修改副本顺序，因为副本顺序中，kafka默认第一个为首选leader，经过平衡后会首选使用第一个副本作为leader。topicPartition.json 格式，只是调整了副本顺序</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:0, &quot;replicas&quot;:[4,2,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:1, &quot;replicas&quot;:[5,3,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:2, &quot;replicas&quot;:[0,4,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:3, &quot;replicas&quot;:[1,5,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:4, &quot;replicas&quot;:[2,0,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:5, &quot;replicas&quot;:[3,1,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:6, &quot;replicas&quot;:[4,3,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:7, &quot;replicas&quot;:[5,4,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:8, &quot;replicas&quot;:[0,5,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:9, &quot;replicas&quot;:[6,0,1]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:10,&quot;replicas&quot;:[7,1,2]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:11,&quot;replicas&quot;:[8,2,3]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:12,&quot;replicas&quot;:[6,5,4]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:13,&quot;replicas&quot;:[7,0,5]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:14,&quot;replicas&quot;:[8,1,0]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:15,&quot;replicas&quot;:[1,2,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:16,&quot;replicas&quot;:[2,3,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:17,&quot;replicas&quot;:[3,4,8]&#125;</span><br><span class="line">                           ]&#125;</span><br></pre></td></tr></table></figure>

<p>以上两步也可一次性编辑完成。</p>
<h6 id="平衡leader"><a href="#平衡leader" class="headerlink" title="平衡leader"></a>平衡leader</h6><p>一次性平衡所有Topic：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;kafka-preferred-replica-election.sh --zookeeper gs-kafka1:2181</span><br></pre></td></tr></table></figure>

<p>或者 添加配置 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">auto.leader.rebalance.enable&#x3D;true</span><br></pre></td></tr></table></figure>

<p>或者平衡单个Topic</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;kafka-preferred-replica-election.sh --zookeeper gs-kafka1:2181 --path-to-json-file topicPartition.json</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>kafka</category>
        <category>基础操作</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
</search>
