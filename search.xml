<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>atlas架构及原理</title>
    <url>/2020/07/01/Apache-Atlas/01.%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>Atlas 是Hadoop的数据治理和元数据框架，是一组可伸缩和可扩展的核心基础治理服务。</p>
<p>Atlas 提供开放的元数据管理和治理功能，以构建数据资产的目录，对这些资产进行分类和治理，并为数据科学家、分析师和数据治理团队提供围绕这些数据资产的协作功能。</p>
<p><strong>特性</strong></p>
<ul>
<li><p>Metadata types &amp; instances 元数据类型和实例</p>
<p>预定义各种 Hadoop 元数据和非 Hadoop 的元数据类型，类型可以有各种属性，还能继承其他类型。</p>
<p>类型的实例即实体，是具体的元数据对象信息和血缘关系信息等。</p>
</li>
<li><p>Classification 分类</p>
<p>可以动态的创建分类，也可以有子类</p>
</li>
<li><p>Lineage 血缘</p>
<p>有直观的用户图形界面来查看数据血缘，但是没有全局的血缘界面。可以用 Reset API来构建，也可以用Kafka 接口来构建</p>
</li>
<li><p>Security &amp; Data Masking 安全性和数据屏蔽</p>
<p>支持对实体实例的访问控制，以及对分类Classification 的增删改</p>
<p>还可以和 Apache Ranger 集成，进行权限控制。</p>
</li>
</ul>
<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p><img src="http://atlas.apache.org/public/images/twiki/architecture.png" alt="img"></p>
<p><strong>Core</strong> 核心层</p>
<ul>
<li>Type System：atlas 中自定义的类型系统，类似面向对象中的 类和对象。可以自定义类型 (类) 和实体 (对象)，比如 hive表 default.test_table ，在 atlas 中对应的类型就是 hive_table，其实体就该表的元数据信息。</li>
<li>Ingest/Export：ingest 组件用来新增元数据，export用来更新元数据。</li>
<li>Graph Engine：图形引擎，负责 元数据对象之间的血缘关系，基础图形模型等。</li>
</ul>
<p><strong>Integration</strong> 整合层</p>
<ul>
<li><p>API：Atlas 所有功能都可以通过其Rest API 来实现。</p>
</li>
<li><p>Messaging：主要是基于kafka的消息接口。此接口与 Atlas 有更好的松散耦合，更好的扩展，更好的可靠性。</p>
<p>​    ATLAS_HOOK：来自 各个组件的Hook 的元数据通知事件通过写入到名为 ATLAS_HOOK 的 Kafka topic 发送到 Atlas</p>
<p>​    ATLAS_ENTITIES：从 Atlas 到其他集成组件（如Ranger）的事件写入到名为 ATLAS_ENTITIES 的 Kafka topic</p>
</li>
</ul>
<p><strong>Metadata sources</strong>  元数据来源</p>
<ul>
<li><p>Hive：有两个，一个是全量批量导入的，一个是基于hive hook接口实时更新元数据。包括DDL、DML</p>
</li>
<li><p>HBase：有两个，一个是全量批量导入的，一个基于 HBase 协处理器做的实时更新元数据。包括namespace、table、columnFamily等的增删改。</p>
</li>
<li><p>Sqoop：没看</p>
</li>
<li><p>Kafka：只能批量导入，即读取kafka在Zookeeper上存的相关Topic信息，进行导入。</p>
</li>
<li><p>Storm：没看</p>
</li>
<li><p>Spark：有一个 spark 的连接器，支持 spark 端的 血缘关系，但是有点复杂。</p>
</li>
<li><p>RDBMS：只有 RDBMS 的类型系统，需要自己实现 bridge 进行导入</p>
</li>
</ul>
<p><strong>Applications</strong> atlas应用层</p>
<ul>
<li>admin UI：用户界面</li>
<li>Tag Based Policies：基于标签和Apache Ranger 的权限管理。</li>
</ul>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2>]]></content>
      <categories>
        <category>Apache-Atlas</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Atlas</tag>
      </tags>
  </entry>
  <entry>
    <title>atlas安装部署</title>
    <url>/2020/07/01/Apache-Atlas/02.%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<h3 id="安装部署"><a href="#安装部署" class="headerlink" title="安装部署"></a>安装部署</h3><p>单机版内嵌HBase和Solr安装参照  <a href="https://blog.csdn.net/czq850114000/article/details/89518215" target="_blank" rel="noopener">https://blog.csdn.net/czq850114000/article/details/89518215</a></p>
<h4 id="Atlas-编译"><a href="#Atlas-编译" class="headerlink" title="Atlas 编译"></a>Atlas 编译</h4><p>参考官网 ：<a href="http://atlas.apache.org/#/BuildInstallation" target="_blank" rel="noopener">http://atlas.apache.org/#/BuildInstallation</a></p>
<h4 id="下载解压-源码包"><a href="#下载解压-源码包" class="headerlink" title="下载解压 源码包"></a>下载解压 源码包</h4><p>下载：<a href="http://atlas.apache.org/#/Downloads" target="_blank" rel="noopener">http://atlas.apache.org/#/Downloads</a></p>
<p>这里选择的是2.0.0版</p>
<p>解压：tar -xfz apache-atlas-2.0.0-sources.tar.gz</p>
<h4 id="编译、打包"><a href="#编译、打包" class="headerlink" title="编译、打包"></a>编译、打包</h4><p>编译  <code>mvn clean -DskipTests install</code></p>
<p>打包： 分两种</p>
<ul>
<li>只打包 atlas： <code>mvn clean -DskipTests package -Pdist</code></li>
<li>内嵌 Hbase 和 Solr 的 ：<code>mvn clean -DskipTests package -Pdist,embedded-hbase-solr</code></li>
</ul>
<p>这里采取的是第一种，只打包 atlas</p>
<h4 id="Solr-安装部署"><a href="#Solr-安装部署" class="headerlink" title="Solr 安装部署"></a>Solr 安装部署</h4><p>下载：<a href="http://archive.apache.org/dist/lucene/solr/7.5.0/solr-7.5.0.tgz" target="_blank" rel="noopener">http://archive.apache.org/dist/lucene/solr/7.5.0/solr-7.5.0.tgz</a></p>
<p>解压：tar -zxf solr-7.5.0.tgz</p>
<h5 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h5><p>配置 SOLR_HOME</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@tnode3 solr-7.5.0]# vi &#x2F;etc&#x2F;profile	</span><br><span class="line"></span><br><span class="line">export SOLR_HOME&#x3D;&#x2F;data&#x2F;solr&#x2F;solr-7.5.0</span><br><span class="line">export PATH&#x3D;$PATH:$SOLR_HOME&#x2F;bin</span><br><span class="line"></span><br><span class="line">[root@tnode3 solr-7.5.0]# source &#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure>

<p>添加 solr.xml 文件 。如果不添加此文件到 <code>$SOLR_HOME</code>，启动报错 corecontainer is either not initializ，日志在 <code>$SOLR_HOME/server/logs/solr.log</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@tnode3 solr-7.5.0]# cd $SOLR_HOME</span><br><span class="line">[root@tnode3 solr-7.5.0]# cp server&#x2F;solr&#x2F;solr.xml .</span><br></pre></td></tr></table></figure>

<p>修改配置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@tnode3 solr-7.5.0]# vi solr.in.sh</span><br><span class="line"></span><br><span class="line">ZK_HOST&#x3D;&quot;tnode3:2181&#x2F;solr4atlas&quot;</span><br><span class="line">SOLR_PORT&#x3D;9838</span><br></pre></td></tr></table></figure>

<p>ZK 创建对应目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;zkCli.sh</span><br><span class="line"></span><br><span class="line">create &#x2F;solr4atlas &quot;solr4atlas&quot;</span><br></pre></td></tr></table></figure>

<p>需要修改系统最大进程数 ，修改完成之后重新连接会话窗口。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi &#x2F;etc&#x2F;security&#x2F;limits.d&#x2F;90-nproc.conf</span><br><span class="line"></span><br><span class="line">*          soft    nproc     65000</span><br><span class="line">*          hard    nproc     65000</span><br><span class="line">root       soft    nproc     unlimited</span><br></pre></td></tr></table></figure>

<p>启动 Solr </p>
<p><code>bin/solr start -force</code></p>
<h5 id="将-Solr-集成到-Atlas"><a href="#将-Solr-集成到-Atlas" class="headerlink" title="将 Solr 集成到 Atlas"></a>将 Solr 集成到 Atlas</h5><p>apache atlas 配置的solr文件夹拷贝到solr各个节点,并重命名为：apache-atlas-conf </p>
<p>创建 collection</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;solr create -c vertex_index -d $SOLR_HOME&#x2F;apache-atlas-conf -shards 2 -replicationFactor 2 -force</span><br><span class="line">bin&#x2F;solr create -c edge_index -d $SOLR_HOME&#x2F;apache-atlas-conf -shards 2 -replicationFactor 2 -force</span><br><span class="line">bin&#x2F;solr create -c fulltext_index -d $SOLR_HOME&#x2F;apache-atlas-conf -shards 2 -replicationFactor 2 -force</span><br></pre></td></tr></table></figure>



<h5 id="Atlas-集成-独立-Solr-集群"><a href="#Atlas-集成-独立-Solr-集群" class="headerlink" title="Atlas 集成 独立 Solr 集群"></a>Atlas 集成 独立 Solr 集群</h5><p>修改atlas-application.properties solr 相关配置 </p>
<p>配置文件位于：$ATLAS_HOME/conf/atlas-application.properties</p>
<h4 id="HBase-2-1-7-安装部署"><a href="#HBase-2-1-7-安装部署" class="headerlink" title="HBase 2.1.7 安装部署"></a>HBase 2.1.7 安装部署</h4><p>解压：tar -zxf hbase-2.1.7-bin.tar.gz</p>
<h5 id="安装部署："><a href="#安装部署：" class="headerlink" title="安装部署："></a>安装部署：</h5><p>三个配置文件：</p>
<p>hbase-env.sh</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;default</span><br><span class="line">export HBASE_MANAGES_ZK&#x3D;false</span><br><span class="line">export HBASE_LOG_DIR&#x3D;&#x2F;data&#x2F;hbase-2.1.7&#x2F;logs</span><br><span class="line">export HBASE_PID_DIR&#x3D;&#x2F;data&#x2F;hbase-2.1.7&#x2F;pids</span><br></pre></td></tr></table></figure>

<p>hbase-site.xml</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.rootdir&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;hdfs:&#x2F;&#x2F;tbbkhd&#x2F;apps&#x2F;hbase2&#x2F;data&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.cluster.distributed&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">     &lt;property&gt;</span><br><span class="line">   	&lt;name&gt;hbase.zookeeper.property.clientPort&lt;&#x2F;name&gt;</span><br><span class="line">    	&lt;value&gt;2181&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.zookeeper.quorum&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;tnode3&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;zookeeper.znode.parent&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;&#x2F;hbase&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>

<p>需要创建 <code>hdfs://tbbkhd/apps/hbase2/data</code> 此目录。</p>
<p>regionservers</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">localhost</span><br></pre></td></tr></table></figure>

<h5 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;start-hbase.sh</span><br></pre></td></tr></table></figure>



<h5 id="Atlas-集成已有-HBase-集群"><a href="#Atlas-集成已有-HBase-集群" class="headerlink" title="Atlas 集成已有 HBase 集群"></a>Atlas 集成已有 HBase 集群</h5><p>修改 atlas-applications 文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#Hbase</span><br><span class="line">#For standalone mode , specify localhost</span><br><span class="line">#for distributed mode, specify zookeeper quorum here</span><br><span class="line">atlas.graph.storage.hostname&#x3D;tnode3</span><br><span class="line">atlas.graph.storage.hbase.regions-per-server&#x3D;1</span><br><span class="line">atlas.graph.storage.lock.wait-time&#x3D;10000</span><br><span class="line"></span><br><span class="line"># 该项不知道要不要改，这里改了</span><br><span class="line">atlas.audit.hbase.zookeeper.quorum&#x3D;tnode3:2181</span><br></pre></td></tr></table></figure>

<p>修改 atlas-env.sh 文件      </p>
<p>加上 export HBASE_CONF_DIR=/data/hbase/hbase-2.1.7/conf</p>
<h5 id="Atlas-集成已有-Kafka-集群"><a href="#Atlas-集成已有-Kafka-集群" class="headerlink" title="Atlas 集成已有 Kafka 集群"></a>Atlas 集成已有 Kafka 集群</h5><p>修改 atlas-applications 文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#########  Notification Configs  #########</span><br><span class="line"># 主要注释调前面两项，不然无法连接上外部的 kafka 集群</span><br><span class="line">#atlas.notification.embedded&#x3D;true</span><br><span class="line">#atlas.kafka.data&#x3D;$&#123;sys:atlas.home&#125;&#x2F;data&#x2F;kafka</span><br><span class="line">atlas.kafka.zookeeper.connect&#x3D;tnode3:2181&#x2F;kafka</span><br><span class="line">atlas.kafka.bootstrap.servers&#x3D;tnode1:9092,tnode2:9092,tnode3:9092</span><br><span class="line">atlas.kafka.zookeeper.session.timeout.ms&#x3D;400</span><br><span class="line">atlas.kafka.zookeeper.connection.timeout.ms&#x3D;200</span><br><span class="line">atlas.kafka.zookeeper.sync.time.ms&#x3D;20</span><br><span class="line">atlas.kafka.auto.commit.interval.ms&#x3D;1000</span><br><span class="line">atlas.kafka.hook.group.id&#x3D;atlas</span><br></pre></td></tr></table></figure>



<h5 id="Atlas-配置"><a href="#Atlas-配置" class="headerlink" title="Atlas 配置"></a>Atlas 配置</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 需要指定 节点。方便以后其他机器 cli 访问</span><br><span class="line">atlas.rest.address&#x3D;http:&#x2F;&#x2F;tmaster:21000</span><br><span class="line"># If enabled and set to true, this will run setup steps when the server starts</span><br><span class="line">#atlas.server.run.setup.on.start&#x3D;false</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Apache-Atlas</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Atlas</tag>
        <tag>Solr</tag>
        <tag>Hbase</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>atlas类型系统</title>
    <url>/2020/07/03/Apache-Atlas/04.%E7%B1%BB%E5%9E%8B%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<p>未做记录。</p>
]]></content>
      <categories>
        <category>Apache-Atlas</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Atlas</tag>
      </tags>
  </entry>
  <entry>
    <title>atlas后续计划</title>
    <url>/2020/07/03/Apache-Atlas/09.%E5%90%8E%E7%BB%AD%E8%AE%A1%E5%88%92/</url>
    <content><![CDATA[<h4 id="初步计划"><a href="#初步计划" class="headerlink" title="初步计划"></a>初步计划</h4><p>1、安装或升级HBase：由于集群中的版本有点低，Atlas 2.0 需要 HBase2.0。</p>
<p>2、安装Atlas：需要集成集群的，还需要Solr7.5集群。初步不考虑HA，HA需要更多机器。由于HDP已经安装了 Atlas 0.5，需要卸载掉。不然会对新的 Atlas产生不必要的影响。</p>
<h4 id="第二步计划"><a href="#第二步计划" class="headerlink" title="第二步计划"></a>第二步计划</h4><p>1、rdbms 元数据的 atlas bridge。</p>
<p>2、血缘关系部分：包括 spark-sql 和 rdbms 两部分，都是跟hive之间的血缘。 mysql 血缘已经通过 canal 简单实现。</p>
]]></content>
      <categories>
        <category>Apache-Atlas</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Atlas</tag>
      </tags>
  </entry>
  <entry>
    <title>Druid初探</title>
    <url>/2020/06/15/Apache-Druid/00.Druid%E5%88%9D%E6%8E%A2/</url>
    <content><![CDATA[<h3 id="Apache-Druid-是什么"><a href="#Apache-Druid-是什么" class="headerlink" title="Apache Druid 是什么"></a>Apache Druid 是什么</h3><p>是一个为在大数据集之上做实时统计分析而设计的开源数据存储。这个系统集合了一个面向列存储的层，一个分布式、shared-nothing的架构，和一个高级的索引结构，来达成在秒级以内对十亿行级别的表进行任意的探索分析。</p>
<h4 id="Apache-Druid-架构"><a href="#Apache-Druid-架构" class="headerlink" title="Apache Druid 架构"></a>Apache Druid 架构</h4><p>一个Druid集群包含不同类型的节点，而每种节点都被设计来做好某组事情。我们相信这样的设计可以隔离关注并简化整个系统的复杂度。不同节点的运转几乎都是独立的并且和其他的节点有着最小化的交互，因此集群内的通信故障对于数据可用性的影响微乎其微。</p>
<p>为了解决复杂的数据分析问题，把不同类型的节点组合在一起，就形成了一个完整的系统。架构图来自官网</p>
<p><img src="https://github.com/QLeelulu/research/raw/master/druid/White_Paper/1.png" alt="apache druid 架构"></p>
<h4 id="实时节点"><a href="#实时节点" class="headerlink" title="实时节点"></a>实时节点</h4><p>实时节点封装了<strong>导入</strong>和<strong>查询事件数据</strong>的功能，经由这些节点导入的事件数据可以立刻被查询。实时节点只关心一小段时间内的事件数据，并定期把这段时间内收集的这批不可变事件数据导入到Druid集群里面另外一个专门负责处理不可变的批量数据的节点中去。实时节点通过Zookeeper的协调和Druid集群的其他节点协调工作。实时节点通过Zookeeper来宣布他们的在线状态和他们提供的数据。</p>
<p>实时节点为所有传入的事件数据维持一个内存中的<strong>索引缓存</strong>。为了避免堆溢出问题，实时节点会定期地、或者在达到设定的最大行限制的时候，把内存中的索引持久化到磁盘去。这个持久化进程会把保存于内存缓存中的数据转换为基于列存储的格式</p>
<p>所有的实时节点都会周期性的启动后台的计划任务搜索本地的<strong>持久化索引</strong>，后台计划任务将这些持久化的索引合并到一起并生成一块不可变的数据</p>
<p>实时节点<strong>是一个数据的消费者</strong>，需要有相应的生产商为其提供数据流。通常，为了数据耐久性的目的，会在生产商与实时节点间放置一个类似于Kafka这样的消息总线来进行连接。消息总线作为传入数据的缓冲区。类似于Kafka这样的消息总线会维持一个指示当前消费者（实时节点）从事件数据流中已经读取数据的位置偏移量，实时节点每次持久化内存中的缓存到磁盘的时候，都会更新这个偏移量。多个实时节点可以从数据总线导入同一组数据，为数据创建一个副本。这样当一个节点完全挂掉并且磁盘上的数据也丢失了，副本可以确保不会丢失任何数据。</p>
<h4 id="历史节点"><a href="#历史节点" class="headerlink" title="历史节点"></a>历史节点</h4><p>历史节点封装了加载和处理由实时节点创建的不可变数据块（<code>segment</code>）的功能。</p>
<p>类似于实时节点，历史节点在<code>Zookeeper</code> 中通告它们的在线状态和为哪些数据提供服务。</p>
<h4 id="Broker节点"><a href="#Broker节点" class="headerlink" title="Broker节点"></a>Broker节点</h4><p>Broker 节点扮演着历史节点和实时节点的查询路由的角色。Broker节点知道发布于<code>Zookeeper</code> 中的关于哪些<code>segment</code> 是可查询的和这些 <code>segment</code> 是保存在哪里的，Broker节点就可以将到来的查询请求路由到正确的历史节点或者是实时节点，Broker节点也会将历史节点和实时节点的局部结果进行合并，然后返回最终的合并后的结果给调用者。</p>
<h4 id="协调节点"><a href="#协调节点" class="headerlink" title="协调节点"></a>协调节点</h4><p>Druid的协调节点主要负责数据的管理和在历史节点上的分布。协调节点告诉历史节点加载新数据、卸载过期数据、复制数据和为了负载均衡移动数据。</p>
<h4 id="存储格式"><a href="#存储格式" class="headerlink" title="存储格式"></a>存储格式</h4><p>Druid中的数据表（称为数据源）是一个时间序列事件数据的集合，并分割到一组segment中，而每一个segment通常是0.5-1千万行。在形式上，我们定义一个segment为跨越一段时间的数据行的集合。Segment是Druid里面的基本存储单元，复制和分布都是在segment基础之上进行的。</p>
<h4 id="其他依赖"><a href="#其他依赖" class="headerlink" title="其他依赖"></a>其他依赖</h4><p>Druid 包含3个外部依赖 ：Mysql、Deep storage、Zookeeper</p>
<ul>
<li><h4 id="Mysql"><a href="#Mysql" class="headerlink" title="Mysql"></a>Mysql</h4><p>Mysql：存储关于Druid中的metadata而不是存储实际数据，包含3张表：<code>druid_config</code> （通常是空的）,  <code>druid_rules</code>（协作节点使用的一些规则信息，比如哪个segment从哪个node去load）和 <code>druid_segments</code> （存储 每个segment的metadata信息）</p>
</li>
<li><h4 id="Deep-storage"><a href="#Deep-storage" class="headerlink" title="Deep storage"></a>Deep storage</h4><p>Deep storage: 存储segments，Druid目前已经支持本地磁盘、NFS挂载磁盘、HDFS、S3等。Deep Storage的数据有2个来源，一个是批数据摄入, 另一个来自实时节点</p>
</li>
<li><h4 id="ZooKeeper"><a href="#ZooKeeper" class="headerlink" title="ZooKeeper"></a>ZooKeeper</h4><p>ZooKeeper: 被Druid用于管理当前cluster的状态，比如记录哪些segments从实时节点移到了历史节点</p>
</li>
</ul>
<h4 id="查询API"><a href="#查询API" class="headerlink" title="查询API"></a>查询API</h4><p>可以使用 SQL 和 Curl 的方式</p>
<h3 id="单机版安装"><a href="#单机版安装" class="headerlink" title="单机版安装"></a>单机版安装</h3><p>下载需要的安装包</p>
<ul>
<li><strong>Zookeeper</strong>：<code>wget https://apache.org/dist/zookeeper/zookeeper-3.4.14/zookeeper-3.4.14.tar.gz</code> </li>
<li><strong>Druid</strong> ：<code>wget http://mirror.bit.edu.cn/apache/incubator/druid/0.15.1-incubating/apache-druid-0.15.1-incubating-bin.tar.gz</code></li>
</ul>
<p>解压安装包</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -zxf  apache-druid-0.15.1-incubating-bin.tar.gz -C /data/druid/druid-0.15.1</span><br><span class="line">tar -zxf  zookeeper-3.4.14.tar.gz -C /data/druid/</span><br></pre></td></tr></table></figure>



<p>Zookeeper 配置：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /data/druid/zookeeper-3.4.14 </span><br><span class="line">mv conf/zoo_sample.cfg conf/zoo.cfg</span><br></pre></td></tr></table></figure>



<p>druid 单机启动准备:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ln -s /data/druid/zookeeper-3.4.14 /data/druid/druid-0.15.1/zk</span><br></pre></td></tr></table></figure>



<p>事前不需要启动ZK，直接启动Druid集群：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@calcnode3 druid-0.15.1]# bin/start-micro-quickstart</span><br><span class="line">[Tue Sep 10 15:39:09 2019] Running command[zk], logging to[/data/druid/druid-0.15.1/var/sv/zk.log]: bin/run-zk conf</span><br><span class="line">[Tue Sep 10 15:39:09 2019] Running command[coordinator-overlord], logging to[/data/druid/druid-0.15.1/var/sv/coordinator-overlord.log]: bin/run-druid coordinator-overlord conf/druid/single-server/micro-quickstart</span><br><span class="line">[Tue Sep 10 15:39:09 2019] Running command[broker], logging to[/data/druid/druid-0.15.1/var/sv/broker.log]: bin/run-druid broker conf/druid/single-server/micro-quickstart</span><br><span class="line">[Tue Sep 10 15:39:09 2019] Running command[router], logging to[/data/druid/druid-0.15.1/var/sv/router.log]: bin/run-druid router conf/druid/single-server/micro-quickstart</span><br><span class="line">[Tue Sep 10 15:39:09 2019] Running command[historical], logging to[/data/druid/druid-0.15.1/var/sv/historical.log]: bin/run-druid historical conf/druid/single-server/micro-quickstart</span><br><span class="line">[Tue Sep 10 15:39:09 2019] Running command[middleManager], logging to[/data/druid/druid-0.15.1/var/sv/middleManager.log]: bin/run-druid middleManager conf/druid/single-server/micro-quickstart</span><br></pre></td></tr></table></figure>

<p>如果需要，可以根据启动脚本使用的配置，在配置文件中修改配置信息。</p>
<p>其中 bin/start-micro-quickstart 主要内容是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">exec &quot;$WHEREAMI&#x2F;supervise&quot; -c &quot;$WHEREAMI&#x2F;..&#x2F;conf&#x2F;supervise&#x2F;single-server&#x2F;micro-quickstart.conf&quot;</span><br></pre></td></tr></table></figure>

<p>可知，启动配置在 conf/supervise/single-server/micro-quickstart.conf，查看一下主要内容：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">:verify bin/verify-java           // 校验 java</span><br><span class="line">:verify bin/verify-default-ports  // 校验 端口</span><br><span class="line">:kill-timeout 10                  </span><br><span class="line"></span><br><span class="line">!p10 zk bin/run-zk conf   // 启动 zk</span><br><span class="line">coordinator-overlord bin/run-druid coordinator-overlord conf/druid/single-server/micro-quickstart   // 启动 coordinator-overlord</span><br><span class="line">broker bin/run-druid broker conf/druid/single-server/micro-quickstart  // 启动 broker</span><br><span class="line">router bin/run-druid router conf/druid/single-server/micro-quickstart  // 启动 router</span><br><span class="line">historical bin/run-druid historical conf/druid/single-server/micro-quickstart // 启动 historical</span><br><span class="line">!p90 middleManager bin/run-druid middleManager conf/druid/single-server/micro-quickstart  // 启动 middleManager</span><br></pre></td></tr></table></figure>

<p>看样子，单机配置全在 conf/druid/single-server/micro-quickstart 下面：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@calcnode3 micro-quickstart]# ll</span><br><span class="line">总用量 24</span><br><span class="line">drwxr-xr-x. 2 hive games 4096 9月  12 11:13 broker</span><br><span class="line">drwxr-xr-x. 2 hive games 4096 9月  12 11:44 _common</span><br><span class="line">drwxr-xr-x. 2 hive games 4096 9月  12 11:34 coordinator-overlord</span><br><span class="line">drwxr-xr-x. 2 hive games 4096 9月  12 11:35 historical</span><br><span class="line">drwxr-xr-x. 2 hive games 4096 9月  12 11:13 middleManager</span><br><span class="line">drwxr-xr-x. 2 hive games 4096 9月  12 11:13 router</span><br></pre></td></tr></table></figure>

<p>配置项分别对应各自的节点。</p>
<p>此次只修改了 _common/common.runtime.properties 文件。包括启动的服务，主机名，ZK，元数据，深度存储等。</p>
<p>WebUI界面：<a href="http://xxx.xxx.xxx.108:8888/" target="_blank" rel="noopener">http://xxx.xxx.xxx.108:8888</a> </p>
<h3 id="简单使用"><a href="#简单使用" class="headerlink" title="简单使用"></a>简单使用</h3><p>先看看批量提交的脚本：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@calcnode3 druid-0.15.1]# cat bin&#x2F;post-index-task</span><br><span class="line">#!&#x2F;bin&#x2F;bash -eu</span><br><span class="line"></span><br><span class="line">PWD&#x3D;&quot;$(pwd)&quot;</span><br><span class="line">WHEREAMI&#x3D;&quot;$(dirname &quot;$0&quot;)&quot;</span><br><span class="line">WHEREAMI&#x3D;&quot;$(cd &quot;$WHEREAMI&quot; &amp;&amp; pwd)&quot;</span><br><span class="line"></span><br><span class="line">if [ -x &quot;$(command -v python2)&quot; ]</span><br><span class="line">then</span><br><span class="line">  exec python2 &quot;$WHEREAMI&#x2F;post-index-task-main&quot; &quot;$@&quot;</span><br><span class="line">else</span><br><span class="line">  exec &quot;$WHEREAMI&#x2F;post-index-task-main&quot; &quot;$@&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>脚本使用方法，如下。必须指定 一个文件。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@calcnode3 druid-0.15.1]#  bin&#x2F;post-index-task</span><br><span class="line">usage: post-index-task-main [-h] [--url url]</span><br><span class="line">                            [--coordinator-url COORDINATOR_URL] --file FILE</span><br><span class="line">                            [--submit-timeout SUBMIT_TIMEOUT]</span><br><span class="line">                            [--complete-timeout COMPLETE_TIMEOUT]</span><br><span class="line">                            [--load-timeout LOAD_TIMEOUT] [--quiet]</span><br><span class="line">                            [--user USER] [--password PASSWORD]</span><br></pre></td></tr></table></figure>

<p>一个官方给的文件：摄取本地文件到 Druid。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@calcnode3 druid-0.15.1]# cat quickstart/tutorial/wikipedia-index.json </span><br><span class="line">&#123;</span><br><span class="line">  "type" : "index",</span><br><span class="line">  "spec" : &#123;</span><br><span class="line">    "dataSchema" : &#123;</span><br><span class="line">      "dataSource" : "wikipedia",</span><br><span class="line">      "parser" : &#123;</span><br><span class="line">        "type" : "string",</span><br><span class="line">        "parseSpec" : &#123;</span><br><span class="line">          "format" : "json",</span><br><span class="line">          "dimensionsSpec" : &#123;</span><br><span class="line">            "dimensions" : [</span><br><span class="line">              "channel",</span><br><span class="line">              "cityName",</span><br><span class="line">              "comment",</span><br><span class="line">              "countryIsoCode",</span><br><span class="line">              "countryName",</span><br><span class="line">              "isAnonymous",</span><br><span class="line">              "isMinor",</span><br><span class="line">              "isNew",</span><br><span class="line">              "isRobot",</span><br><span class="line">              "isUnpatrolled",</span><br><span class="line">              "metroCode",</span><br><span class="line">              "namespace",</span><br><span class="line">              "page",</span><br><span class="line">              "regionIsoCode",</span><br><span class="line">              "regionName",</span><br><span class="line">              "user",</span><br><span class="line">              &#123; "name": "added", "type": "long" &#125;,</span><br><span class="line">              &#123; "name": "deleted", "type": "long" &#125;,</span><br><span class="line">              &#123; "name": "delta", "type": "long" &#125;</span><br><span class="line">            ]</span><br><span class="line">          &#125;,</span><br><span class="line">          "timestampSpec": &#123;</span><br><span class="line">            "column": "time",</span><br><span class="line">            "format": "iso"</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      "metricsSpec" : [],</span><br><span class="line">      "granularitySpec" : &#123;</span><br><span class="line">        "type" : "uniform",</span><br><span class="line">        "segmentGranularity" : "day",</span><br><span class="line">        "queryGranularity" : "none",</span><br><span class="line">        "intervals" : ["2015-09-12/2015-09-13"],</span><br><span class="line">        "rollup" : false</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    "ioConfig" : &#123;</span><br><span class="line">      "type" : "index",</span><br><span class="line">      "firehose" : &#123;</span><br><span class="line">        "type" : "local",</span><br><span class="line">        "baseDir" : "quickstart/tutorial/",</span><br><span class="line">        "filter" : "wikiticker-2015-09-12-sampled.json.gz"</span><br><span class="line">      &#125;,</span><br><span class="line">      "appendToExisting" : false</span><br><span class="line">    &#125;,</span><br><span class="line">    "tuningConfig" : &#123;</span><br><span class="line">      "type" : "index",</span><br><span class="line">      "maxRowsPerSegment" : 5000000,</span><br><span class="line">      "maxRowsInMemory" : 25000</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>执行:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@calcnode3 druid-0.15.1]# bin/post-index-task --file quickstart/tutorial/wikipedia-index.json --url http://calcnode3:8081</span><br><span class="line">Beginning indexing data for wikipedia</span><br><span class="line">Task started: index_wikipedia_2019-09-12T07:41:11.478Z</span><br><span class="line">Task log:     http://calcnode3:8081/druid/indexer/v1/task/index_wikipedia_2019-09-12T07:41:11.478Z/log</span><br><span class="line">Task status:  http://calcnode3:8081/druid/indexer/v1/task/index_wikipedia_2019-09-12T07:41:11.478Z/status</span><br><span class="line">Task index_wikipedia_2019-09-12T07:41:11.478Z still running...</span><br><span class="line">Task index_wikipedia_2019-09-12T07:41:11.478Z still running...</span><br><span class="line">Task finished with status: SUCCESS</span><br><span class="line">Completed indexing data for wikipedia. Now loading indexed data onto the cluster...</span><br><span class="line">wikipedia is 0.0% finished loading...</span><br><span class="line">wikipedia is 0.0% finished loading...</span><br><span class="line">wikipedia loading complete! You may now query your data</span><br></pre></td></tr></table></figure>

<p>查询：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">SELECT channel, SUM(added) as add_sum FROM wikipedia WHERE "__time" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY channel ORDER BY SUM(added) DESC LIMIT 5;</span><br></pre></td></tr></table></figure>



<h3 id="集群安装"><a href="#集群安装" class="headerlink" title="集群安装"></a>集群安装</h3><p>配置在 cluster 配置中，具体配置项和单机版一样，只不过不同的服务启动在不同的节点上。</p>
<h4 id="关于-Hadoop-集成"><a href="#关于-Hadoop-集成" class="headerlink" title="关于 Hadoop 集成"></a>关于 Hadoop 集成</h4><p>有两点，一个是 <code>DeepStorage</code>，一个是 <code>Hadoop Batch Ingestion</code>。</p>
<p><code>DeepStorage</code>：深度存储，即 Druid 本身的 segment 数据 和 indexing-log 的存储位置，配置很简单，在 _common 目录下的 common.runtime.properties 中进行修改。</p>
<p><code>Hadoop Batch Ingestion</code>：即源数据放在 HDFS 上，Druid 直接摄取 HDFS 上的对应数据。支持文本格式，SequenceFile，ORC，Parquet 等格式。下面是简单示例：使用前将 HDFS几个配置文件放到 _common 下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># my-hadoop.json</span></span></span><br><span class="line">&#123;</span><br><span class="line">  "type" : "index_hadoop",</span><br><span class="line">  "spec" : &#123;</span><br><span class="line">    "dataSchema" : &#123;</span><br><span class="line">      "dataSource" : "wikipedia_hadoop",</span><br><span class="line">      "parser" : &#123;</span><br><span class="line">        "type" : "hadoopyString",</span><br><span class="line">        "parseSpec" : &#123;</span><br><span class="line">          "format" : "json",</span><br><span class="line">          "dimensionsSpec" : &#123;</span><br><span class="line">            "dimensions" : [</span><br><span class="line">              "channel",</span><br><span class="line">              "cityName",</span><br><span class="line">              "comment",</span><br><span class="line">              "countryIsoCode",</span><br><span class="line">              "countryName",</span><br><span class="line">              "isAnonymous",</span><br><span class="line">              "isMinor",</span><br><span class="line">              "isNew",</span><br><span class="line">              "isRobot",</span><br><span class="line">              "isUnpatrolled",</span><br><span class="line">              "metroCode",</span><br><span class="line">              "namespace",</span><br><span class="line">              "page",</span><br><span class="line">              "regionIsoCode",</span><br><span class="line">              "regionName",</span><br><span class="line">              "user",</span><br><span class="line">              &#123; "name": "added", "type": "long" &#125;,</span><br><span class="line">              &#123; "name": "deleted", "type": "long" &#125;,</span><br><span class="line">              &#123; "name": "delta", "type": "long" &#125;</span><br><span class="line">            ]</span><br><span class="line">          &#125;,</span><br><span class="line">          "timestampSpec" : &#123;</span><br><span class="line">            "format" : "auto",</span><br><span class="line">            "column" : "time"</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      "metricsSpec" : [],</span><br><span class="line">      "granularitySpec" : &#123;</span><br><span class="line">        "type" : "uniform",</span><br><span class="line">        "segmentGranularity" : "day",</span><br><span class="line">        "queryGranularity" : "none",</span><br><span class="line">        "intervals" : ["2015-09-12/2015-09-13"],</span><br><span class="line">        "rollup" : false</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    "ioConfig" : &#123;</span><br><span class="line">      "type" : "hadoop",</span><br><span class="line">      "inputSpec" : &#123;</span><br><span class="line">        "type" : "static",</span><br><span class="line">        "paths" : "hdfs://bbkhd/tmp/wikiticker-2015-09-12-sampled.json.gz"</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    "tuningConfig" : &#123;</span><br><span class="line">      "type" : "hadoop",</span><br><span class="line">      "partitionsSpec" : &#123;</span><br><span class="line">        "type" : "hashed",</span><br><span class="line">        "targetPartitionSize" : 5000000</span><br><span class="line">      &#125;,</span><br><span class="line">      "forceExtendableShardSpecs" : true,</span><br><span class="line">      "jobProperties" : &#123;</span><br><span class="line">        "mapreduce.job.classloader" : "true"</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  "hadoopDependencyCoordinates": ["org.apache.hadoop:hadoop-client:2.8.3"]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>启动摄取任务：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/post-index-task --file my-hadoop.json --url http://calcnode3:8081</span><br></pre></td></tr></table></figure>



<h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul>
<li><p>亚秒级响应</p>
</li>
<li><p>实时导入-kafka</p>
</li>
<li><p>支持复杂的聚合</p>
</li>
<li><p>不支持大表 join</p>
</li>
<li><p>lookup 功能可以 join维表</p>
</li>
<li><p>预计算-使用 MR 导入数据</p>
</li>
</ul>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li><p>集群复杂</p>
<p>各个节点的配置太多，难以管理</p>
</li>
<li><p>SQL支持不完善</p>
</li>
<li><p>集成 BI 只能 superset 等几个</p>
</li>
<li><p>无法精确计算</p>
</li>
</ul>
]]></content>
      <categories>
        <category>Apache-Druid</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Druid</tag>
        <tag>实时数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink任务提交流程分析</title>
    <url>/2020/06/18/Apache-Flink/Flink%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h4 id="任务启动"><a href="#任务启动" class="headerlink" title="任务启动"></a>任务启动</h4><pre><code>启动流程见 $FLINK_HOME/bin/flink
可见其启动入口类为：org.apache.flink.client.cli.CliFrontend</code></pre><h4 id="CliFrontend-任务入口类"><a href="#CliFrontend-任务入口类" class="headerlink" title="CliFrontend 任务入口类"></a>CliFrontend 任务入口类</h4><h5 id="main方法"><a href="#main方法" class="headerlink" title="main方法"></a>main方法</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(<span class="keyword">final</span> String[] args)</span> </span>&#123;</span><br><span class="line">		EnvironmentInformation.logEnvironmentInfo(LOG, <span class="string">"Command Line Client"</span>, args);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1. find the configuration directory</span></span><br><span class="line">		<span class="keyword">final</span> String configurationDirectory = getConfigurationDirectoryFromEnv();</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 2. load the global configuration</span></span><br><span class="line">		<span class="comment">// 加载配置目录下的 flink-conf.yaml 文件，先判断目录是否有效，flink-conf.yaml 文件是否存在</span></span><br><span class="line">		<span class="comment">// 使用 loadYAMLResource 读取配置，返回 Configuration</span></span><br><span class="line">		<span class="keyword">final</span> Configuration configuration = GlobalConfiguration.loadConfiguration(configurationDirectory);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 3. load the custom command lines</span></span><br><span class="line">		<span class="comment">// 获取 客户端 命令行帮助 文档</span></span><br><span class="line">		<span class="comment">// 其中 loadCustomCommandLines 方法 负责加载两个不同的解析flink集群。一个是 flinkYarnSessionCLI ，另一个是 DefaultCLI</span></span><br><span class="line"></span><br><span class="line">		<span class="keyword">final</span> List&lt;CustomCommandLine&lt;?&gt;&gt; customCommandLines = loadCustomCommandLines(</span><br><span class="line">			configuration,</span><br><span class="line">			configurationDirectory);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 初始化 CliFrontend</span></span><br><span class="line">		<span class="keyword">try</span> &#123;</span><br><span class="line">			<span class="keyword">final</span> CliFrontend cli = <span class="keyword">new</span> CliFrontend(</span><br><span class="line">				configuration,</span><br><span class="line">				customCommandLines);</span><br><span class="line"></span><br><span class="line">			SecurityUtils.install(<span class="keyword">new</span> SecurityConfiguration(cli.configuration));</span><br><span class="line">			<span class="keyword">int</span> retCode = SecurityUtils</span><br><span class="line">				.getInstalledContext()</span><br><span class="line">				.runSecured( () -&gt; cli.parseParameters(args) );   <span class="comment">// 处理参数，即 run、stop、cancel等。并根据命令执行...</span></span><br><span class="line">			System.exit(retCode);</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">			<span class="keyword">final</span> Throwable strippedThrowable = ExceptionUtils.stripException(t, UndeclaredThrowableException<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">			LOG.error(<span class="string">"Fatal error while running command line interface."</span>, strippedThrowable);</span><br><span class="line">			strippedThrowable.printStackTrace();</span><br><span class="line">			System.exit(<span class="number">31</span>);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>

<h6 id="添加两种解析集群模式"><a href="#添加两种解析集群模式" class="headerlink" title="添加两种解析集群模式"></a>添加两种解析集群模式</h6><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> List&lt;CustomCommandLine&lt;?&gt;&gt; loadCustomCommandLines(Configuration configuration, String configurationDirectory) &#123;</span><br><span class="line">		List&lt;CustomCommandLine&lt;?&gt;&gt; customCommandLines = <span class="keyword">new</span> ArrayList&lt;&gt;(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">		<span class="comment">//	Command line interface of the YARN session, with a special initialization here</span></span><br><span class="line">		<span class="comment">//	to prefix all options with y/yarn.</span></span><br><span class="line">		<span class="comment">//	Tips: DefaultCLI must be added at last, because getActiveCustomCommandLine(..) will get the</span></span><br><span class="line">		<span class="comment">//	      active CustomCommandLine in order and DefaultCLI isActive always return true.</span></span><br><span class="line">		<span class="keyword">final</span> String flinkYarnSessionCLI = <span class="string">"org.apache.flink.yarn.cli.FlinkYarnSessionCli"</span>;</span><br><span class="line">		<span class="keyword">try</span> &#123;</span><br><span class="line">			customCommandLines.add(</span><br><span class="line">				loadCustomCommandLine(flinkYarnSessionCLI,</span><br><span class="line">					configuration,</span><br><span class="line">					configurationDirectory,</span><br><span class="line">					<span class="string">"y"</span>,</span><br><span class="line">					<span class="string">"yarn"</span>));</span><br><span class="line">		&#125; <span class="keyword">catch</span> (NoClassDefFoundError | Exception e) &#123;</span><br><span class="line">			LOG.warn(<span class="string">"Could not load CLI class &#123;&#125;."</span>, flinkYarnSessionCLI, e);</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		customCommandLines.add(<span class="keyword">new</span> DefaultCLI(configuration));</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> customCommandLines;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>


<h5 id="run方法-流程"><a href="#run方法-流程" class="headerlink" title="run方法 流程"></a>run方法 流程</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		LOG.info(<span class="string">"Running 'run' command."</span>);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 获取 run 指令 可选参数列表，并将传入参数设置进 run 任务</span></span><br><span class="line">		<span class="keyword">final</span> Options commandOptions = CliFrontendParser.getRunCommandOptions();</span><br><span class="line"></span><br><span class="line">		<span class="keyword">final</span> Options commandLineOptions = CliFrontendParser.mergeOptions(commandOptions, customCommandLineOptions);</span><br><span class="line"></span><br><span class="line">		<span class="keyword">final</span> CommandLine commandLine = CliFrontendParser.parse(commandLineOptions, args, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">		<span class="keyword">final</span> RunOptions runOptions = <span class="keyword">new</span> RunOptions(commandLine);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// evaluate help flag</span></span><br><span class="line">		<span class="comment">// 输出 run 命令 帮助文档</span></span><br><span class="line">		<span class="keyword">if</span> (runOptions.isPrintHelp()) &#123;</span><br><span class="line">			CliFrontendParser.printHelpForRun(customCommandLines);</span><br><span class="line">			<span class="keyword">return</span>;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 未指定 jar 包抛异常，退出</span></span><br><span class="line">		<span class="keyword">if</span> (runOptions.getJarFilePath() == <span class="keyword">null</span>) &#123;</span><br><span class="line">			<span class="keyword">throw</span> <span class="keyword">new</span> CliArgsException(<span class="string">"The program JAR file was not specified."</span>);</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">		<span class="keyword">final</span> PackagedProgram program;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 根据 jar 包和参数生成 program 即运行程序  根据 entryPointClass 通过 PackagedProgram 生成</span></span><br><span class="line">		<span class="keyword">try</span> &#123;</span><br><span class="line">			LOG.info(<span class="string">"Building program from JAR file"</span>);</span><br><span class="line">			program = buildProgram(runOptions);</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">catch</span> (FileNotFoundException e) &#123;</span><br><span class="line">			<span class="keyword">throw</span> <span class="keyword">new</span> CliArgsException(<span class="string">"Could not build the program from JAR file."</span>, e);</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 解析集群是 yarn cluster 还是 standalone 模式</span></span><br><span class="line">		<span class="comment">// 因为 在上一步 flinkYarnSessionCLI 是先添加的，所以会判断是否符合 flinkYarnSessionCLI 的条件，否则构建 standalone 集群</span></span><br><span class="line">		<span class="keyword">final</span> CustomCommandLine&lt;?&gt; customCommandLine = getActiveCustomCommandLine(commandLine);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 执行生成的 program</span></span><br><span class="line">		<span class="keyword">try</span> &#123;</span><br><span class="line">			runProgram(customCommandLine, commandLine, runOptions, program);</span><br><span class="line">		&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">			<span class="comment">// 删除抽取的依赖文件</span></span><br><span class="line">			program.deleteExtractedLibraries();</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Apache-Flink</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink集群搭建</title>
    <url>/2020/06/18/Apache-Flink/Flink%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA--standalone%20HA/</url>
    <content><![CDATA[<h4 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h4><h5 id="1-JAVA环境"><a href="#1-JAVA环境" class="headerlink" title="1. JAVA环境"></a>1. JAVA环境</h5><h6 id="卸载OpenJDK，安装OracleJDK"><a href="#卸载OpenJDK，安装OracleJDK" class="headerlink" title="卸载OpenJDK，安装OracleJDK"></a>卸载OpenJDK，安装OracleJDK</h6><blockquote>
<p>rpm -aq | grep java 命令 查看本地安装的JDK软件包</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# rpm -aq | grep java</span><br><span class="line">tzdata-java-2016c-1.el6.noarch</span><br><span class="line">java-1.6.0-openjdk-1.6.0.38-1.13.10.4.el6.x86_64</span><br><span class="line">java-1.7.0-openjdk-1.7.0.99-2.6.5.1.el6.x86_64</span><br></pre></td></tr></table></figure>
<p>rpm -i jdk-6u24-linux-amd64.rpm 命令安装jdk</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost packages]# ll</span><br><span class="line">总用量 458696</span><br><span class="line">-rw-r--r--. 1 root root 299718134 7月  16 14:17 &gt;flink-1.7.2-bin-hadoop27-scala_2.11.tgz</span><br><span class="line">-rw-r--r--. 1 root root 169983496 7月  16 14:16 jdk-8u131-linux-x64.rpm</span><br></pre></td></tr></table></figure>
</blockquote>
<h6 id="修改环境变量"><a href="#修改环境变量" class="headerlink" title="修改环境变量"></a>修改环境变量</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;vi &#x2F;etc&#x2F;profile </span><br><span class="line"></span><br><span class="line">&gt;添加以下内容：</span><br><span class="line">&gt;export JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;default</span><br><span class="line">&gt;export CLASSPATH&#x3D;.:$JAVA_HOME&#x2F;lib&#x2F;dt.jar:$JAVA_HOME&#x2F;lib&#x2F;tools.jar:$JAVA_HOME&#x2F;lib</span><br><span class="line">&gt;export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin</span><br><span class="line"></span><br><span class="line">&gt;source &#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure>
</blockquote>
<h5 id="2-主机名修改"><a href="#2-主机名修改" class="headerlink" title="2. 主机名修改"></a>2. 主机名修改</h5><h6 id="修改hosts文件，映射IP和主机名，顺便把集群的一起做了"><a href="#修改hosts文件，映射IP和主机名，顺便把集群的一起做了" class="headerlink" title="修改hosts文件，映射IP和主机名，顺便把集群的一起做了"></a>修改hosts文件，映射IP和主机名，顺便把集群的一起做了</h6><blockquote>
<p>vi /etc/hosts  </p>
<p>新增以下内容：<br>192.168.199.116 flink001<br>192.168.199.127 flink002<br>192.168.199.128 flink003<br>192.168.199.129 flink004<br>192.168.199.130 flink005<br>192.168.199.131 flink006<br>192.168.199.132 flink007<br>192.168.199.133 flink008<br>192.168.199.134 flink009  </p>
</blockquote>
<h6 id="修改主机名-centos6-8"><a href="#修改主机名-centos6-8" class="headerlink" title="修改主机名(centos6.8)"></a>修改主机名(centos6.8)</h6><blockquote>
<p>分两部分，第一个  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi &#x2F;etc&#x2F;sysconfig&#x2F;network</span><br><span class="line"></span><br><span class="line">修改这行 &#x3D;后部分修改为需要的主机名</span><br><span class="line">HOSTNAME&#x3D;flink001</span><br></pre></td></tr></table></figure>
<p>第二个，命令修改主机名</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hostname flink001</span><br></pre></td></tr></table></figure>
</blockquote>
<h5 id="3-免密登录"><a href="#3-免密登录" class="headerlink" title="3. 免密登录"></a>3. 免密登录</h5><h6 id="拷贝-etc-hosts-文件到其他主机"><a href="#拷贝-etc-hosts-文件到其他主机" class="headerlink" title="拷贝 /etc/hosts 文件到其他主机"></a>拷贝 /etc/hosts 文件到其他主机</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp &#x2F;etc&#x2F;hosts root@flink002:&#x2F;etc&#x2F;hosts</span><br><span class="line">或者</span><br><span class="line">for i in &#123;2..9&#125;;do scp &#x2F;etc&#x2F;hosts root@flink00$&#123;i&#125;:&#x2F;etc&#x2F; ;done</span><br></pre></td></tr></table></figure>
</blockquote>
<h6 id="ssh免密登录。JM免密到其他节点就OK"><a href="#ssh免密登录。JM免密到其他节点就OK" class="headerlink" title="ssh免密登录。JM免密到其他节点就OK"></a>ssh免密登录。JM免密到其他节点就OK</h6><blockquote>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ssh-keygen -rsa -t    // 然后一路回车</span><br><span class="line">ssh-copy-id flink002  // 按要求输入密码</span><br><span class="line">ssh flink002          // 验证是否成功</span><br></pre></td></tr></table></figure>
</blockquote>
<h5 id="4-Flink安装"><a href="#4-Flink安装" class="headerlink" title="4. Flink安装"></a>4. Flink安装</h5><ul>
<li>下载安装包到指定目录</li>
<li>修改 ./conf/flink-conf.yaml文件。注意格式中冒号后需要有一个空格，否则不生效，为无效配置项。<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> core 其实在HA模式下，前两个配置不会生效，所以无所谓修不修改，修改了最好</span></span><br><span class="line">jobmanager.rpc.address: flink001</span><br><span class="line"></span><br><span class="line">jobmanager.rpc.port: 6123</span><br><span class="line"></span><br><span class="line">jobmanager.heap.size: 20480m</span><br><span class="line"></span><br><span class="line">taskmanager.heap.size: 153600m</span><br><span class="line"></span><br><span class="line">taskmanager.numberOfTaskSlots: 32</span><br><span class="line"></span><br><span class="line">parallelism.default: 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">io.tmp.dirs:/data1/tmp</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> HDFS</span></span><br><span class="line">fs.default-scheme: hdfs://bbkhd</span><br><span class="line"></span><br><span class="line">fs.hdfs.hadoopconf:/data1/flink/flink-1.7.2/conf/</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> zookeeper HA </span></span><br><span class="line"></span><br><span class="line">high-availability: zookeeper</span><br><span class="line"></span><br><span class="line">high-availability.storageDir: hdfs:///apps/flink/ha</span><br><span class="line"></span><br><span class="line">high-availability.zookeeper.quorum: node31:2181, node32:2181, node33:2181</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> state.backend</span></span><br><span class="line"></span><br><span class="line">state.backend: filesystem</span><br><span class="line"></span><br><span class="line">state.checkpoints.dir: hdfs:///apps/flink/checkpoints</span><br><span class="line"></span><br><span class="line">state.savepoints.dir: hdfs:///apps/flink/savepoints</span><br><span class="line"></span><br><span class="line">state.backend.incremental: false</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> web上传的jar包路径</span></span><br><span class="line">web.upload.dir: /data1/flink/flink-1.7.2/uploadJarDir</span><br></pre></td></tr></table></figure></li>
<li>修改slaves文件，添加TaskManager主机列表<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">flink001</span><br><span class="line">flink002</span><br><span class="line">flink003</span><br><span class="line">flink004</span><br><span class="line">flink005</span><br><span class="line">flink006</span><br><span class="line">flink007</span><br><span class="line">flink008</span><br><span class="line">flink009</span><br></pre></td></tr></table></figure></li>
<li>修改masters文件，添加JobManager主机列表<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">flink001:8081</span><br><span class="line">flink002:8081</span><br></pre></td></tr></table></figure></li>
<li>检查是否配置 $HADOOP_CONF_DIR目录，如果没有，需要加上配置。如果不集成hadoop不需要关心</li>
<li>拷贝安装包到其他节点<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">for i in &#123;2..9&#125;;do scp -r &#x2F;data1&#x2F;flink&#x2F; root@flink00$&#123;i&#125;:&#x2F;data1&#x2F;flink&#x2F; ;done</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h5 id="5-启动-停止flink集群，有必要的话设置一下-FLINK-HOME"><a href="#5-启动-停止flink集群，有必要的话设置一下-FLINK-HOME" class="headerlink" title="5.启动/停止flink集群，有必要的话设置一下 $FLINK_HOME"></a>5.启动/停止flink集群，有必要的话设置一下 $FLINK_HOME</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;start-cluster.sh  </span><br><span class="line">.&#x2F;stop-cluster.sh</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Apache-Flink</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Hbase基础</title>
    <url>/2020/04/22/Apache-Hbase/HBase%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<h3 id="一、基础理论"><a href="#一、基础理论" class="headerlink" title="一、基础理论"></a>一、基础理论</h3><p>Hbase是一个非分布式的，面向列的开源数据库</p>
<p>基于BigTable</p>
<p>非结构化</p>
<p>存储在HDFS上，备份机制</p>
<p>线性扩展</p>
<p>cluster  /  slave ：Hmaster、Regionserver</p>
<p>Hbase架构图：</p>
<p><img src="F:%5CmyGit%5CDT-Learner%5CApache-Hbase%5Cimage-1565608391940.png" alt="img"></p>
<h3 id="二、组件功能："><a href="#二、组件功能：" class="headerlink" title="二、组件功能："></a>二、组件功能：</h3><h6 id="Hbase数据模型："><a href="#Hbase数据模型：" class="headerlink" title="Hbase数据模型："></a>Hbase数据模型：</h6><p>master负责维护表结构，regionserver负责存储数据：client通过 ZK 直连regionserver，即使master挂掉也能查询数据，但是不能建表。</p>
<p>每个regionserver服务器上有多个region。</p>
<p>每张表都有一个或多个region，但是一张表中的不同region可能存在不同的regionserver上，这也是不建议一个表多个列族的原因之一。</p>
<p>每张表的每个列族都存在一个文件上，经历从 WAL -&gt; MemStore -&gt; Hfile 的过程。通常读操作的时候会遵循 Block Cache -&gt; MemStore -&gt; Hfile的顺序进行查找，以便效率的提升。</p>
<p>MemStore到一定大小会flush成Hfile，而Hfile数量提升时，会进行Compaction操作：</p>
<p>Minor Compaction：较小的Hfile合并成较大的Hfile，即小文件 –&gt; 大文件。减少Hfile数量，提升Hbase读性能。</p>
<p>Major Compaction：将对应一个column Family的所有Hfile合并成 一个 大的Hfile，并且会删除已经删除或者过期的Cell。很大的提升Hbase效率。但是Compaction中，包含大量的磁盘I/O和网络通信（有些Hfile已经在其他regionserver上，而此操作会将所有其他regionserver上的Hfile下载到本地），会造成region处于不可访问的状态。</p>
<p>NameSpace：每个命名空间都有可以有多张表，类比 Oracle中的database，MySQL中的schema。</p>
<p>Table：</p>
<p>Rowkey：</p>
<p>family_columns ： hbase 表中的每个列，都归属与某个列族。列族是表的 schema 的一部分(而列不是)，必须在 使用表之前定义。 列名都以列族作为前缀。例如 courses:history ， courses:math 都属于 courses 这个列族。 访问控制、磁盘和内存的使用统计都是在列族层面进行的。列族越多，在取一行数据时所要参与 IO、搜寻的文件就越多，所以，如果没有必要，不要 设置太多的列族 </p>
<p><strong>（每个列族存放在不同的文件中，建表时列族越少越好）</strong></p>
<p>TimeStamp</p>
<p>cell   没有数据类型，都是字节码的形式，  cell有多版本，Rowkey和列唯一确定cell，cell的版本通过时间戳来索引</p>
<p>  为了避免数据存在过多版本造成的的管理 (包括存贮和索引)负担， hbase 提供了两种数据版 本回收方式： </p>
<p>​                  保存数据的最后 n 个版本</p>
<p>​                  保存最近一段时间内的版本（设置数据的生命周期 TTL）。</p>
<p>用户可以针对每个列族进行设置。</p>
<h3 id="三、-HBase-shell-基本操作"><a href="#三、-HBase-shell-基本操作" class="headerlink" title="三、 HBase shell 基本操作"></a>三、 HBase shell 基本操作</h3><p>HBase shell 正常进入 shell 的界面：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 ~]# hbase shell</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/usr/hdp/2.4.2.0-258/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/usr/hdp/2.4.2.0-258/zookeeper/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]</span><br><span class="line">HBase Shell; enter 'help&lt;RETURN&gt;' for list of supported commands.</span><br><span class="line">Type "exit&lt;RETURN&gt;" to leave the HBase Shell</span><br><span class="line">Version 1.1.2.2.4.2.0-258, rUnknown, Mon Apr 25 06:36:21 UTC 2016</span><br><span class="line"></span><br><span class="line">hbase(main):001:0&gt;</span><br></pre></td></tr></table></figure>



<h4 id="命名空间相关"><a href="#命名空间相关" class="headerlink" title="命名空间相关"></a>命名空间相关</h4><p>类似与 MySQL 的 schema</p>
<h6 id="创建命名空间"><a href="#创建命名空间" class="headerlink" title="创建命名空间"></a>创建命名空间</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase(main):004:0&gt; create_namespace &#39;testNS&#39;</span><br><span class="line">0 row(s) in 0.0980 seconds</span><br></pre></td></tr></table></figure>

<h6 id="删除命名空间"><a href="#删除命名空间" class="headerlink" title="删除命名空间"></a>删除命名空间</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase(main):012:0&gt; drop_namespace &#39;testNS&#39;</span><br><span class="line">0 row(s) in 0.0270 seconds</span><br></pre></td></tr></table></figure>

<p>删除之前要先确定命名空间为空，否则会报错</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ERROR: org.apache.hadoop.hbase.constraint.ConstraintException: Only empty namespaces can be removed. Namespace testNS has 1 tables</span><br><span class="line">	at org.apache.hadoop.hbase.master.TableNamespaceManager.remove(TableNamespaceManager.java:198)</span><br><span class="line">	at org.apache.hadoop.hbase.master.HMaster.deleteNamespace(HMaster.java:2507)</span><br><span class="line">	at org.apache.hadoop.hbase.master.MasterRpcServices.deleteNamespace(MasterRpcServices.java:481)</span><br><span class="line">	at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java:55453)</span><br><span class="line">	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2114)</span><br><span class="line">	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:101)</span><br><span class="line">	at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:130)</span><br><span class="line">	at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:107)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:748)</span><br><span class="line"></span><br><span class="line">Here is some help for this command:</span><br><span class="line">Drop the named namespace. The namespace must be empty.</span><br></pre></td></tr></table></figure>



<h4 id="表相关"><a href="#表相关" class="headerlink" title="表相关"></a>表相关</h4><h6 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h6><p>简单创建和设置列簇属性的创建</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase(main):005:0&gt; create &#39;userInfo&#39;,&#39;baseInfo&#39;</span><br><span class="line">0 row(s) in 1.4830 seconds</span><br><span class="line"></span><br><span class="line">&#x3D;&gt; Hbase::Table - userInfo</span><br><span class="line"></span><br><span class="line">hbase(main):030:0&gt; create &#39;test:userInfo&#39;, &#123; NAME&#x3D;&gt;&#39;baseInfo&#39;,VERSIONS&#x3D;&gt;6 &#125;, &#123; NAME&#x3D;&gt;&#39;extrInfo&#39;,CONFIGURATION&#x3D;&gt; &#123;&#39;hbase.hstore.blockingStoreFiles&#39;&#x3D;&gt;&#39;15&#39;&#125; &#125;</span><br><span class="line">0 row(s) in 1.2300 seconds</span><br><span class="line"></span><br><span class="line">&#x3D;&gt; Hbase::Table - test:userInfo</span><br></pre></td></tr></table></figure>

<h6 id="列出表"><a href="#列出表" class="headerlink" title="列出表"></a>列出表</h6><p>两种，一种是列出所有表，一种是列出符合正则表达式的表 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase(main):021:0&gt; list</span><br><span class="line">TABLE</span><br><span class="line">test:customerTable</span><br><span class="line">1 row(s) in 0.0040 seconds</span><br><span class="line"></span><br><span class="line">&#x3D;&gt; [&quot;test:customerTable&quot;]</span><br><span class="line"></span><br><span class="line">hbase(main):025:0&gt; list &#39;.*Table&#39;</span><br><span class="line">TABLE</span><br><span class="line">test:customerTable</span><br><span class="line">1 row(s) in 0.0030 seconds</span><br><span class="line"></span><br><span class="line">&#x3D;&gt; [&quot;test:customerTable&quot;]</span><br></pre></td></tr></table></figure>

<h6 id="查看表"><a href="#查看表" class="headerlink" title="查看表"></a>查看表</h6><p>desc 和 describe 一样</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase(main):024:0&gt; desc &#39;test:customerTable&#39;</span><br><span class="line">Table test:customerTable is ENABLED                                                   test:customerTable                                                                   COLUMN FAMILIES DESCRIPTION                                                           &#123;NAME &#x3D;&gt; &#39;info&#39;, BLOOMFILTER &#x3D;&gt; &#39;ROW&#39;, VERSIONS &#x3D;&gt; &#39;3&#39;, IN_MEMORY &#x3D;&gt; &#39;false&#39;, KEEP_DELETED_CELLS &#x3D;&gt; &#39;FALSE&#39;, DATA_BLOCK_ENCODING &#x3D;&gt; &#39;NONE&#39;, TTL &#x3D;&gt; &#39;FOREVER&#39;, COMPRESSION &#x3D;&gt; &#39;NONE&#39;, MIN_VERSIONS &#x3D;&gt; &#39;0&#39;, BLOCKCACHE &#x3D;&gt; &#39;true&#39;, BLOCKSIZE &#x3D;&gt; &#39;65536&#39;, REPLICATION_SCOPE &#x3D;&gt; &#39;0&#39;</span><br><span class="line">&#125;                                                                                     &#123;NAME &#x3D;&gt; &#39;other&#39;, BLOOMFILTER &#x3D;&gt; &#39;ROW&#39;, VERSIONS &#x3D;&gt; &#39;3&#39;, IN_MEMORY &#x3D;&gt; &#39;false&#39;, KEEP_DELETED_CELLS &#x3D;&gt; &#39;FALSE&#39;, DATA_BLOCK_ENCODING &#x3D;&gt; &#39;NONE&#39;, TTL &#x3D;&gt; &#39;FOREVER&#39;, COMPRESSION &#x3D;&gt; &#39;NONE&#39;, MIN_VERSIONS &#x3D;&gt; &#39;0&#39;, BLOCKCACHE &#x3D;&gt; &#39;true&#39;, BLOCKSIZE &#x3D;&gt; &#39;65536&#39;, REPLICATION_SCOPE &#x3D;&gt; &#39;0</span><br><span class="line">&#39;&#125;                                                                                     2 row(s) in 0.0180 seconds</span><br></pre></td></tr></table></figure>

<h6 id="删除表"><a href="#删除表" class="headerlink" title="删除表"></a>删除表</h6><p>删除单张和正则表达式批量删除两种</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 删除单张</span><br><span class="line">hbase(main):009:0&gt; drop &#39;testNS:userInfo&#39;</span><br><span class="line"></span><br><span class="line">ERROR: Table testNS:userInfo is enabled. Disable it first.</span><br><span class="line"></span><br><span class="line">Here is some help for this command:</span><br><span class="line">Drop the named table. Table must first be disabled:</span><br><span class="line">  hbase&gt; drop &#39;t1&#39;</span><br><span class="line">  hbase&gt; drop &#39;ns1:t1&#39;</span><br><span class="line"></span><br><span class="line">hbase(main):010:0&gt; disable &#39;testNS:userInfo&#39;</span><br><span class="line">0 row(s) in 2.2520 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):011:0&gt; drop &#39;testNS:userInfo&#39;</span><br><span class="line">0 row(s) in 1.2390 seconds</span><br><span class="line"></span><br><span class="line"># 删除一批</span><br><span class="line">hbase(main):020:0&gt; drop_all &#39;.*Info&#39;</span><br><span class="line">custInfo</span><br><span class="line">userInfo                                                                             </span><br><span class="line"></span><br><span class="line">Drop the above 2 tables (y&#x2F;n)?</span><br><span class="line">y</span><br><span class="line">2 tables successfully dropped</span><br></pre></td></tr></table></figure>

<p>删表之前需要先禁用表</p>
<h6 id="启停表"><a href="#启停表" class="headerlink" title="启停表"></a>启停表</h6><p>单表启停，批量启停，检查启停状态等六类</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase(main):032:0&gt; disable 'test:userInfo'</span><br><span class="line">0 row(s) in 2.2620 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):033:0&gt; enable 'test:userInfo'</span><br><span class="line">0 row(s) in 1.2350 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):034:0&gt; disable_all 'test:.*s.*'</span><br><span class="line">test:customerTable</span><br><span class="line">test:userInfo</span><br><span class="line"></span><br><span class="line">Disable the above 2 tables (y/n)?</span><br><span class="line">y</span><br><span class="line">2 tables successfully disabled</span><br><span class="line"></span><br><span class="line">hbase(main):035:0&gt; is_disabled 'test:userInfo'</span><br><span class="line">true                                                                                           </span><br><span class="line">0 row(s) in 0.0050 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):036:0&gt; is_enabled 'test:userInfo'</span><br><span class="line">false</span><br><span class="line">0 row(s) in 0.0050 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):037:0&gt; enable_all 'test:.*s.*'</span><br><span class="line">test:customerTable</span><br><span class="line">test:userInfo               </span><br><span class="line"></span><br><span class="line">Enable the above 2 tables (y/n)?</span><br><span class="line">y</span><br><span class="line">2 tables successfully enabled</span><br><span class="line"></span><br><span class="line">hbase(main):038:0&gt; is_disabled 'test:userInfo'</span><br><span class="line">false</span><br><span class="line"></span><br><span class="line">0 row(s) in 0.0110 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):039:0&gt; is_enabled 'test:userInfo'</span><br><span class="line">true</span><br><span class="line"></span><br><span class="line">0 row(s) in 0.0090 seconds</span><br></pre></td></tr></table></figure>

<h6 id="修改表"><a href="#修改表" class="headerlink" title="修改表"></a>修改表</h6><p><code>hbase.online.schema.update.enable</code> 该参数如果是 false，那么修改时必须先 </p>
<p>表。 如果为 true，那么可以直接修改表</p>
<p><strong>新增修改列簇</strong>（HBase中修改和新增是一个操作）。如果传入新的列族名，可以新建列族；如果传入已存在的列族名，可以修改列族属性。列族属性有： </p>
<ul>
<li>BLOOMFILTER </li>
<li>REPLICATION_SCOPE </li>
<li>MIN_VERSIONS </li>
<li>COMPRESSION </li>
<li>TTL </li>
<li>BLOCKSIZE </li>
<li>IN_MEMORY </li>
<li>IN_MEMORY_COMPACTION </li>
<li>BLOCKCACHE </li>
<li>KEEP_DELETED_CELLS</li>
<li>DATA_BLOCK_ENCODING </li>
<li>CACHE_DATA_ON_WRITE </li>
<li>CACHE_DATA_IN_L1 </li>
<li>CACHE_INDEX_ON_WRITE </li>
<li>CACHE_BLOOMS_ON_WRITE </li>
<li>EVICT_BLOCKS_ON_CLOSE </li>
<li>PREFETCH_BLOCKS_ON_OPEN </li>
<li>ENCRYPTION </li>
<li>ENCRYPTION_KEY </li>
<li>IS_MOB_BYTES </li>
<li>MOB_THRESHOLD_BYTES</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 直接新增</span><br><span class="line">hbase(main):013:0&gt; alter &#39;test:userInfo&#39;,&#123;NAME&#x3D;&gt;&#39;cccfff&#39;&#125;</span><br><span class="line">Updating all regions with the new schema...</span><br><span class="line">1&#x2F;1 regions updated.</span><br><span class="line">Done.</span><br><span class="line">0 row(s) in 1.9110 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):015:0&gt; alter &#39;test:userInfo&#39;,NAME&#x3D;&gt;&#39;cccfff&#39;</span><br><span class="line">Updating all regions with the new schema...</span><br><span class="line">1&#x2F;1 regions updated.</span><br><span class="line">Done.</span><br><span class="line">0 row(s) in 1.9050 seconds</span><br><span class="line"></span><br><span class="line"># 新增两个列簇</span><br><span class="line">hbase(main):020:0&gt; alter &#39;test:userInfo&#39;,&#39;cccfff&#39;,&#123;NAME&#x3D;&gt;&#39;cf123&#39;,VERSIONS&#x3D;&gt;4&#125;</span><br><span class="line">Updating all regions with the new schema...</span><br><span class="line">1&#x2F;1 regions updated.</span><br><span class="line">Done.</span><br><span class="line">Updating all regions with the new schema...</span><br><span class="line">1&#x2F;1 regions updated.</span><br><span class="line">^[[A^[[A^[[ADone.</span><br><span class="line">0 row(s) in 3.7370 seconds</span><br><span class="line"></span><br><span class="line"># 删除列簇</span><br><span class="line">hbase(main):005:0&gt; alter &#39;test:userInfo&#39;,&#39;delete&#39;&#x3D;&gt;&#39;cccfff&#39;</span><br><span class="line">Updating all regions with the new schema...</span><br><span class="line">1&#x2F;1 regions updated.</span><br><span class="line">Done.</span><br><span class="line">0 row(s) in 1.9010 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):009:0&gt; alter &#39;test:userInfo&#39;,METHOD&#x3D;&gt;&#39;delete&#39;,NAME&#x3D;&gt;&#39;cf111&#39;</span><br><span class="line">Updating all regions with the new schema...</span><br><span class="line">1&#x2F;1 regions updated.</span><br><span class="line">Done.</span><br><span class="line">0 row(s) in 1.8940 seconds</span><br><span class="line"></span><br><span class="line"># 新增的同时删除一个</span><br><span class="line"># 指定 METHOD 的方式</span><br><span class="line">hbase(main):022:0&gt; alter &#39;test:userInfo&#39;,&#39;cccfff&#39;,&#123;NAME&#x3D;&gt;&#39;cf123&#39;,METHOD&#x3D;&gt;&#39;delete&#39;&#125;</span><br><span class="line">Updating all regions with the new schema...</span><br><span class="line">1&#x2F;1 regions updated.</span><br><span class="line">Done.</span><br><span class="line">Updating all regions with the new schema...</span><br><span class="line">1&#x2F;1 regions updated.</span><br><span class="line">Done.</span><br><span class="line">0 row(s) in 3.7420 seconds</span><br><span class="line"># 直接使用 delete 方法指定需要删除的列簇</span><br><span class="line">hbase(main):025:0&gt; alter &#39;test:userInfo&#39;,&#39;cccfff&#39;,&#39;delete&#39;&#x3D;&gt;&#39;cf123&#39;</span><br><span class="line">Updating all regions with the new schema...</span><br><span class="line">1&#x2F;1 regions updated.</span><br><span class="line">Done.</span><br><span class="line">Updating all regions with the new schema...</span><br><span class="line">1&#x2F;1 regions updated.</span><br><span class="line">Done.</span><br><span class="line">0 row(s) in 3.7350 second</span><br></pre></td></tr></table></figure>

<p><strong>修改表级别属性</strong>。表级别的属性有：</p>
<ul>
<li>MAX_FILESIZE </li>
<li>READONLY </li>
<li>MEMSTORE_FLUSHSIZE </li>
<li>DEFERRED_LOG_FLUSH </li>
<li>DURABILITY </li>
<li>REGION_REPLICATION </li>
<li>NORMALIZATION_ENABLED </li>
<li>PRIORITY </li>
<li>IS_ROOT </li>
<li>IS_META</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase(main):027:0&gt; alter &#39;test:userInfo&#39;,MAX_FILESIZE &#x3D;&gt; &#39;123217728&#39;</span><br><span class="line">Updating all regions with the new schema...</span><br><span class="line">1&#x2F;1 regions updated.</span><br><span class="line">Done.</span><br><span class="line">0 row(s) in 1.9200 seconds</span><br></pre></td></tr></table></figure>

<p><strong>删除表级别属性。</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase(main):001:0&gt; alter &#39;test:userInfo&#39;,METHOD &#x3D;&gt; &#39;table_att_unset&#39;, NAME &#x3D;&gt; &#39;MAX_FILESIZE&#39;</span><br><span class="line">Updating all regions with the new schema...</span><br><span class="line">1&#x2F;1 regions updated.</span><br><span class="line">Done.</span><br><span class="line">0 row(s) in 2.1860 seconds</span><br></pre></td></tr></table></figure>

<p><strong>设置表配置。</strong> 即修改表/列簇在 hbase-site.xml 中的配置，而不影响其他表。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase(main):011:0&gt; alter &#39;test:userInfo&#39;,CONFIGURATION &#x3D;&gt; &#123;&#39;hbase.hstore.blockingStoreFiles&#39; &#x3D;&gt; &#39;10&#39; &#125;</span><br><span class="line">Updating all regions with the new schema...</span><br><span class="line">1&#x2F;1 regions updated.</span><br><span class="line">Done.</span><br><span class="line">0 row(s) in 1.8880 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):014:0&gt; alter &#39;test:userInfo&#39;,&#123; NAME &#x3D;&gt; &#39;extrInfo&#39;, CONFIGURATION &#x3D;&gt; &#123;&#39;hbase.hstore.blockingStoreFiles&#39; &#x3D;&gt; &#39;10&#39; &#125;&#125;</span><br><span class="line">Updating all regions with the new schema...</span><br><span class="line">1&#x2F;1 regions updated.</span><br><span class="line">Done.</span><br><span class="line">0 row(s) in 1.8850 seconds</span><br></pre></td></tr></table></figure>



<h4 id="查询相关"><a href="#查询相关" class="headerlink" title="查询相关"></a>查询相关</h4><h6 id="scan"><a href="#scan" class="headerlink" title="scan"></a>scan</h6><p>遍历所有数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; scan &#39;test:userInfo&#39;</span><br></pre></td></tr></table></figure>

<p>指定列</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; scan &#39;test:userInfo&#39;,&#123;COLUMNS&#x3D;&gt;&#39;cf1:name&#39;&#125;</span><br></pre></td></tr></table></figure>

<p>限制返回行数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; scan &#39;test:userInfo&#39;,&#123; LIMIT&#x3D;&gt;10 &#125;</span><br><span class="line">&gt; scan &#39;test:userInfo&#39;,&#123; COLUMNS&#x3D;&gt;&#39;cf1:name&#39;, LIMIT&#x3D;&gt;10&#125;</span><br></pre></td></tr></table></figure>

<p>指定 rowkey 范围。包含 STARTROW，不包含 ENDROW数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; scan &#39;test:userInfo&#39;,&#123;STARTROW &#x3D;&gt; &#39;0001&#39;, ENDROW &#x3D;&gt; &#39;00002&#39;&#125;</span><br></pre></td></tr></table></figure>

<p>指定时间戳范围。左闭右开范围。会查询到历史版本信息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; scan &#39;test:userInfo&#39;,&#123;TIMERANGE&#x3D;&gt;[1569306706585, 1569316490296]&#125;</span><br></pre></td></tr></table></figure>

<p>查询多版本数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; scan &#39;test:userInfo&#39;,&#123;VERSIONS&#x3D;&gt;3&#125;</span><br></pre></td></tr></table></figure>

<p>显示原始记录。即查询所有数据，包含已经标记为删除的数据。必须配合 VERSIONS 使用，不能和 COLUMNS 一起使用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; scan &#39;test:userInfo&#39;,&#123;VERSIONS&#x3D;&gt;3,RAW&#x3D;&gt;true&#125;</span><br></pre></td></tr></table></figure>

<p>指定过滤器。 如果多个，使用 AND 和 OR 来进行连接</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; scan &#39;test:userInfo&#39;, &#123;FILTER &#x3D;&gt; &quot;FamilyFilter(&#x3D;,&#39;substring:f&#39;)&quot;&#125;</span><br><span class="line">&gt; scan &#39;test:userInfo&#39;, &#123;FILTER &#x3D;&gt; &quot;FamilyFilter(!&#x3D;,&#39;substring:info&#39;) AND ValueFilter(&gt;&#x3D;,&#39;binary:X&#39;)&quot;&#125;</span><br></pre></td></tr></table></figure>



<h6 id="get"><a href="#get" class="headerlink" title="get"></a>get</h6><p>获取对应 rowkey 所有数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; get &#39;test:userInfo&#39;,&#39;00002&#39;</span><br></pre></td></tr></table></figure>

<p>获取对应 rowkey 部分列数据，多个列可以用 [] 组合起来。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; get &#39;test:userInfo&#39;,&#39;00002&#39;, &#123;COLUMN&#x3D;&gt; &#39;cf1&#39;&#125;</span><br><span class="line">&gt; get &#39;test:userInfo&#39;,&#39;00002&#39;, &#123;COLUMN&#x3D;&gt; [&#39;extrInfo&#39;,&#39;baseInfo&#39;,&#39;cf1&#39;] &#125;</span><br></pre></td></tr></table></figure>

<p>获取对应 rowkey 时间戳范围的数据。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; get &#39;test:userInfo&#39;,&#39;00002&#39;, &#123;TIMERANGE&#x3D;&gt;[1569317161496,1569317463581]&#125;</span><br></pre></td></tr></table></figure>

<p>获取对应 rowkey 数据，指定版本数。不能单独使用 VERSIONS，必须配合 column 或者 timerange一起用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase(main):075:0&gt; get &#39;test:userInfo&#39;,&#39;00002&#39;, &#123; VERSIONS &#x3D;&gt; 3&#125;</span><br><span class="line">COLUMN                                CELL</span><br><span class="line"></span><br><span class="line">ERROR: Failed parse of &#123;&quot;VERSIONS&quot;&#x3D;&gt;3&#125;, Hash</span><br><span class="line"></span><br><span class="line">&gt; get &#39;test:userInfo&#39;,&#39;00002&#39;, &#123;TIMERANGE&#x3D;&gt;[1569317161495,1569317463582],VERSIONS&#x3D;&gt;3&#125;</span><br><span class="line">&gt; get &#39;test:userInfo&#39;,&#39;00002&#39;, &#123;COLUMN&#x3D;&gt;&#39;cf1&#39;, VERSIONS &#x3D;&gt; 3&#125;</span><br></pre></td></tr></table></figure>

<p>获取对应 rowkey 的数据，使用过滤器过滤。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; get &#39;test:userInfo&#39;,&#39;00002&#39;, &#123;FILTER&#x3D;&gt;&quot;ValueFilter(&#x3D;,&#39;substring:XG&#39;)&quot;&#125;</span><br><span class="line"></span><br><span class="line">&gt; get &#39;test:userInfo&#39;,&#39;00002&#39;, &#123;TIMERANGE&#x3D;&gt;[1569317161495,1569317463582],VERSIONS&#x3D;&gt;3,FILTER&#x3D;&gt;&quot;ValueFilter(&#x3D;,&#39;substring:X&#39;)&quot;&#125;</span><br></pre></td></tr></table></figure>



<h6 id="count"><a href="#count" class="headerlink" title="count"></a>count</h6><p>计算表行数，用法三种：</p>
<p>1、直接出结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; count &#39;test:userInfo&#39;</span><br></pre></td></tr></table></figure>

<p>2、指定步长，每步打印当前结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; count &#39;test:userInfo&#39;, INTERVAL&#x3D;1</span><br></pre></td></tr></table></figure>

<p>3、指定缓存</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; count &#39;test:userInfo&#39;, INTERVAL&#x3D;1, CACHE&#x3D;2</span><br></pre></td></tr></table></figure>



<h4 id="删除相关"><a href="#删除相关" class="headerlink" title="删除相关"></a>删除相关</h4><h6 id="delete"><a href="#delete" class="headerlink" title="delete"></a>delete</h6><p>删除指定行中某列的数据，或者某列的某个版本的数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">delete &#39;test:userInfo&#39;,&#39;00002&#39;,&#39;cf1:name&#39;</span><br><span class="line">delete &#39;test:userInfo&#39;,&#39;00002&#39;,&#39;cf1:name&#39;, 1569399092124</span><br></pre></td></tr></table></figure>



<h6 id="deleteall"><a href="#deleteall" class="headerlink" title="deleteall"></a>deleteall</h6><p>delete加强版，除原有功能，还可以删除整行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deleteall &#39;test:userInfo&#39;,&#39;00006&#39;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase(main):004:0&gt; scan &#39;test:userInfo&#39;,&#123;VERSIONS&#x3D;&gt;5,RAW&#x3D;&gt;true&#125;</span><br><span class="line">ROW                                                                  COLUMN+CELL</span><br><span class="line"> 00001                                                               column&#x3D;cf1:name, timestamp&#x3D;1569306706585, value&#x3D;Wade111</span><br><span class="line"> 00002                                                               column&#x3D;cf1:name, timestamp&#x3D;1569400533054, type&#x3D;DeleteColumn</span><br><span class="line"> 00002                                                               column&#x3D;cf1:name, timestamp&#x3D;1569399423111, value&#x3D;lxmkkk</span><br><span class="line"> 00003                                                               column&#x3D;cf1:name, timestamp&#x3D;1569396888055, value&#x3D;lalalla</span><br><span class="line"> 00004                                                               column&#x3D;cf1:name, timestamp&#x3D;1569396894851, value&#x3D;lalalla2222</span><br><span class="line"> 00005                                                               column&#x3D;cf1:name, timestamp&#x3D;1569396907028, value&#x3D;lalalla222333333</span><br><span class="line"> 00006                                                               column&#x3D;MAX_FILESIZE:, timestamp&#x3D;1569400663358, type&#x3D;DeleteFamily</span><br><span class="line"> 00006                                                               column&#x3D;baseInfo:, timestamp&#x3D;1569400663358, type&#x3D;DeleteFamily</span><br><span class="line"> 00006                                                               column&#x3D;cf1:, timestamp&#x3D;1569400663358, type&#x3D;DeleteFamily</span><br><span class="line"> 00006                                                               column&#x3D;cf1:name, timestamp&#x3D;1569396916208, value&#x3D;lalalla2223333335555555</span><br><span class="line"> 00006                                                               column&#x3D;extrInfo:, timestamp&#x3D;1569400663358, type&#x3D;DeleteFamily</span><br><span class="line"> 00006                                                               column&#x3D;table_att_unset:, timestamp&#x3D;1569400663358, type&#x3D;DeleteFamily</span><br><span class="line">6 row(s) in 0.0180 seconds</span><br></pre></td></tr></table></figure>



<h4 id="过滤器相关"><a href="#过滤器相关" class="headerlink" title="过滤器相关"></a>过滤器相关</h4><h6 id="过滤器基础"><a href="#过滤器基础" class="headerlink" title="过滤器基础"></a>过滤器基础</h6><p>过滤器的<strong>操作符</strong> </p>
<table>
<thead>
<tr>
<th align="center">API</th>
<th align="center">shell</th>
</tr>
</thead>
<tbody><tr>
<td align="center">LESS</td>
<td align="center">&lt;</td>
</tr>
<tr>
<td align="center">LESS_OR_EQUAL</td>
<td align="center">&lt;=</td>
</tr>
<tr>
<td align="center">EQUAL</td>
<td align="center">=</td>
</tr>
<tr>
<td align="center">NOT_EQUAL</td>
<td align="center">&lt;&gt;   or   !=</td>
</tr>
<tr>
<td align="center">GREATER_OR_EQUAL</td>
<td align="center">&gt;=</td>
</tr>
<tr>
<td align="center">GREATER</td>
<td align="center">&gt;</td>
</tr>
<tr>
<td align="center">NO_OP</td>
<td align="center"></td>
</tr>
</tbody></table>
<p>过滤器中的<strong>比较器</strong> </p>
<table>
<thead>
<tr>
<th align="center">API</th>
<th align="center">shell</th>
</tr>
</thead>
<tbody><tr>
<td align="center">BinaryComparator<br />按字节索引顺序比较指定字节数组</td>
<td align="center">binary</td>
</tr>
<tr>
<td align="center">BinaryPrefixComparator<br />和第一个一样的效果，只不过比较前几个</td>
<td align="center">binaryprefix</td>
</tr>
<tr>
<td align="center">NullComparator<br />判断是否为空</td>
<td align="center">null</td>
</tr>
<tr>
<td align="center">BitComparator<br />位比较</td>
<td align="center">bit</td>
</tr>
<tr>
<td align="center">RegexStringComparator<br />提供一个正则的比较器，仅支持 EQUAL 和非EQUAL</td>
<td align="center">regexstring</td>
</tr>
<tr>
<td align="center">SubstringComparator<br />判断提供的子串是否出现在table的value中，仅支持 EQUAL 和非EQUAL</td>
<td align="center">substring</td>
</tr>
</tbody></table>
<p>show_filters 命令能查看已有<strong>过滤器</strong>。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase(main):052:0&gt; show_filters</span><br><span class="line">DependentColumnFilter</span><br><span class="line">KeyOnlyFilter</span><br><span class="line">ColumnCountGetFilter</span><br><span class="line">SingleColumnValueFilter</span><br><span class="line">PrefixFilter</span><br><span class="line">SingleColumnValueExcludeFilter</span><br><span class="line">FirstKeyOnlyFilter</span><br><span class="line">ColumnRangeFilter</span><br><span class="line">TimestampsFilter</span><br><span class="line">FamilyFilter</span><br><span class="line">QualifierFilter</span><br><span class="line">ColumnPrefixFilter</span><br><span class="line">RowFilter</span><br><span class="line">MultipleColumnPrefixFilter</span><br><span class="line">InclusiveStopFilter</span><br><span class="line">PageFilter</span><br><span class="line">ValueFilter</span><br><span class="line">ColumnPaginationFilter</span><br></pre></td></tr></table></figure>



<h6 id="RowFilter"><a href="#RowFilter" class="headerlink" title="RowFilter"></a>RowFilter</h6><p>基于rowkey来过滤数据。如果知道起始的 rowkey，建议是用 scan 的 STARTROW 和 ENDROW，速度快很多。因为 RowFilter是遍历所有 rowkey 的。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scan &#39;test:userInfo&#39;,&#123;FILTER&#x3D;&gt;&quot;RowFilter(&#x3D;,&#39;substring:0000&#39;)&quot;&#125;</span><br><span class="line">scan &#39;test:userInfo&#39;,&#123;FILTER&#x3D;&gt;&quot;RowFilter(&lt;&#x3D;,&#39;binary:00009&#39;)&quot;&#125;</span><br><span class="line">scan &#39;test:userInfo&#39;, FILTER&#x3D;&gt;&quot;RowFilter(&#x3D;,&#39;substring:0000&#39;)&quot;</span><br><span class="line">scan &#39;test:userInfo&#39;, FILTER&#x3D;&gt;&quot;RowFilter(&lt;&#x3D;,&#39;binary:00009&#39;)&quot;</span><br></pre></td></tr></table></figure>



<h6 id="FamilyFilter"><a href="#FamilyFilter" class="headerlink" title="FamilyFilter"></a>FamilyFilter</h6><p>基于列簇来过滤数据。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scan &#39;test:userInfo&#39;,FILTER&#x3D;&gt;&quot;FamilyFilter(&#x3D;,&#39;substring:info&#39;)&quot;</span><br><span class="line">scan &#39;test:userInfo&#39;,&#123;FILTER&#x3D;&gt;&quot;FamilyFilter(&#x3D;,&#39;substring:info&#39;)&quot;&#125;</span><br></pre></td></tr></table></figure>



<h6 id="QualifierFilter"><a href="#QualifierFilter" class="headerlink" title="QualifierFilter"></a>QualifierFilter</h6><p>基于列来过滤数据。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scan &#39;test:userInfo&#39;,&#123;FILTER&#x3D;&gt;&quot;QualifierFilter(&#x3D;,&#39;substring:name&#39;)&quot;&#125;</span><br><span class="line">scan &#39;test:userInfo&#39;, FILTER&#x3D;&gt;&quot;QualifierFilter(&#x3D;,&#39;substring:name&#39;)&quot;</span><br></pre></td></tr></table></figure>



<h6 id="ValueFilter"><a href="#ValueFilter" class="headerlink" title="ValueFilter"></a>ValueFilter</h6><p>基于值来过滤数据。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scan &#39;test:userInfo&#39;, FILTER&#x3D;&gt;&quot;ValueFilter(&#x3D;,&#39;substring:ll&#39;)&quot;</span><br><span class="line">scan &#39;test:userInfo&#39;,&#123;FILTER&#x3D;&gt;&quot;ValueFilter(&#x3D;,&#39;substring:ll&#39;)&quot;&#125;</span><br></pre></td></tr></table></figure>



<h6 id="DependentColumnFilter"><a href="#DependentColumnFilter" class="headerlink" title="DependentColumnFilter"></a>DependentColumnFilter</h6><p>过滤指定列簇里面，返回与参考列具有相同时间戳的数据。前两个参数指定 列簇和列，第三个参数布尔值，是否返回参考列的数据，true表示不返回，false表示返回，两个参数的就是默认false。第四五个参数就是比较运算符和比较器了，用来过滤 value。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scan &#39;test:userInfo&#39;,&#123;FILTER&#x3D;&gt;&quot;DependentColumnFilter(&#39;cf1&#39;,&#39;name&#39;)&quot;&#125;</span><br><span class="line">scan &#39;test:userInfo&#39;,&#123;FILTER&#x3D;&gt;&quot;DependentColumnFilter(&#39;cf1&#39;,&#39;name&#39;,true)&quot;&#125;</span><br><span class="line">scan &#39;test:userInfo&#39;,&#123;FILTER&#x3D;&gt;&quot;DependentColumnFilter(&#39;cf1&#39;,&#39;name&#39;,true,&#x3D;,&#39;substring:d&#39;)&quot;&#125;</span><br></pre></td></tr></table></figure>



<h6 id="SingleColumnValueFilter-和-SingleColumnValueExcludeFilter"><a href="#SingleColumnValueFilter-和-SingleColumnValueExcludeFilter" class="headerlink" title="SingleColumnValueFilter 和 SingleColumnValueExcludeFilter"></a>SingleColumnValueFilter 和 SingleColumnValueExcludeFilter</h6><p>用来查找并返回指定条件的列的数据，四个或六个参数，分别为 列簇、列、比较符、比较器、是否跳过无该列的行、是否查询历史版本，后面两个默认分别是 false、true 即 不跳过、不查询。：</p>
<ol>
<li>如果遍历到某行时，该行没有此 列簇:列 ，返回所有数据。如果选择跳过则不返回</li>
<li>如果遍历到某行时，该行有此 列簇:列 ，但是不符合条件，则该行所有数据都不返回</li>
<li>如果遍历到某行时，该行有此 列簇:列 ，并且也符合条件，则前者返回该行所有数据，后者返回除该列以外的所有数据</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase(main):011:0&gt; scan &#39;test:userInfo&#39;</span><br><span class="line">ROW                                                                  COLUMN+CELL</span><br><span class="line"> 00001                                                               column&#x3D;baseInfo:name, timestamp&#x3D;1569403548008, value&#x3D;Michil</span><br><span class="line"> 00001                                                               column&#x3D;cf1:name, timestamp&#x3D;1569306706585, value&#x3D;Wade111</span><br><span class="line"> 00002                                                               column&#x3D;baseInfo:name, timestamp&#x3D;1569396888055, value&#x3D;dddd</span><br><span class="line"> 00003                                                               column&#x3D;cf1:name, timestamp&#x3D;1569396888055, value&#x3D;lalalla</span><br><span class="line"> 00004                                                               column&#x3D;cf1:name, timestamp&#x3D;1569396894851, value&#x3D;lalalla2222</span><br><span class="line"> 00005                                                               column&#x3D;cf1:name, timestamp&#x3D;1569396907028, value&#x3D;lalalla222333333</span><br><span class="line"> 00009                                                               column&#x3D;cf1:age, timestamp&#x3D;1569396888055, value&#x3D;dddd</span><br><span class="line"> 00009                                                               column&#x3D;cf1:name, timestamp&#x3D;1569396888055, value&#x3D;dddd</span><br><span class="line"> 00010                                                               column&#x3D;cf1:age, timestamp&#x3D;1569396888055, value&#x3D;1111</span><br><span class="line"> 00011                                                               column&#x3D;baseInfo:age, timestamp&#x3D;1569396888055, value&#x3D;1111</span><br><span class="line"> 00012                                                               column&#x3D;baseInfo:name, timestamp&#x3D;1569461934129, value&#x3D;ddd</span><br><span class="line"> 00012                                                               column&#x3D;cf1:name, timestamp&#x3D;1569462029439, value&#x3D;ddd</span><br><span class="line">9 row(s) in 0.0270 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):012:0&gt;  scan &#39;test:userInfo&#39;,&#123;FILTER&#x3D;&gt;&quot;SingleColumnValueFilter(&#39;cf1&#39;,&#39;name&#39;,&#x3D;,&#39;substring:d&#39;)&quot;&#125;</span><br><span class="line">ROW                                                                  COLUMN+CELL</span><br><span class="line"> 00001                                                               column&#x3D;baseInfo:name, timestamp&#x3D;1569403548008, value&#x3D;Michil</span><br><span class="line"> 00001                                                               column&#x3D;cf1:name, timestamp&#x3D;1569306706585, value&#x3D;Wade111</span><br><span class="line"> 00002                                                               column&#x3D;baseInfo:name, timestamp&#x3D;1569396888055, value&#x3D;dddd</span><br><span class="line"> 00009                                                               column&#x3D;cf1:age, timestamp&#x3D;1569396888055, value&#x3D;dddd</span><br><span class="line"> 00009                                                               column&#x3D;cf1:name, timestamp&#x3D;1569396888055, value&#x3D;dddd</span><br><span class="line"> 00010                                                               column&#x3D;cf1:age, timestamp&#x3D;1569396888055, value&#x3D;1111</span><br><span class="line"> 00011                                                               column&#x3D;baseInfo:age, timestamp&#x3D;1569396888055, value&#x3D;1111</span><br><span class="line"> 00012                                                               column&#x3D;baseInfo:name, timestamp&#x3D;1569461934129, value&#x3D;ddd</span><br><span class="line"> 00012                                                               column&#x3D;cf1:name, timestamp&#x3D;1569462029439, value&#x3D;ddd</span><br><span class="line">6 row(s) in 0.0300 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):013:0&gt;  scan &#39;test:userInfo&#39;,&#123;FILTER&#x3D;&gt;&quot;SingleColumnValueFilter(&#39;cf1&#39;,&#39;name&#39;,&#x3D;,&#39;substring:d&#39;,true,false)&quot;&#125;</span><br><span class="line">ROW                                                                  COLUMN+CELL</span><br><span class="line"> 00001                                                               column&#x3D;baseInfo:name, timestamp&#x3D;1569403548008, value&#x3D;Michil</span><br><span class="line"> 00001                                                               column&#x3D;cf1:name, timestamp&#x3D;1569306706585, value&#x3D;Wade111</span><br><span class="line"> 00009                                                               column&#x3D;cf1:age, timestamp&#x3D;1569396888055, value&#x3D;dddd</span><br><span class="line"> 00009                                                               column&#x3D;cf1:name, timestamp&#x3D;1569396888055, value&#x3D;dddd</span><br><span class="line"> 00012                                                               column&#x3D;baseInfo:name, timestamp&#x3D;1569461934129, value&#x3D;ddd</span><br><span class="line"> 00012                                                               column&#x3D;cf1:name, timestamp&#x3D;1569462029439, value&#x3D;ddd</span><br><span class="line">3 row(s) in 0.0100 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):014:0&gt;  scan &#39;test:userInfo&#39;,&#123;FILTER&#x3D;&gt;&quot;SingleColumnValueExcludeFilter(&#39;cf1&#39;,&#39;name&#39;,&#x3D;,&#39;substring:d&#39;)&quot;&#125;</span><br><span class="line">ROW                                                                  COLUMN+CELL</span><br><span class="line"> 00001                                                               column&#x3D;baseInfo:name, timestamp&#x3D;1569403548008, value&#x3D;Michil</span><br><span class="line"> 00002                                                               column&#x3D;baseInfo:name, timestamp&#x3D;1569396888055, value&#x3D;dddd</span><br><span class="line"> 00009                                                               column&#x3D;cf1:age, timestamp&#x3D;1569396888055, value&#x3D;dddd</span><br><span class="line"> 00010                                                               column&#x3D;cf1:age, timestamp&#x3D;1569396888055, value&#x3D;1111</span><br><span class="line"> 00011                                                               column&#x3D;baseInfo:age, timestamp&#x3D;1569396888055, value&#x3D;1111</span><br><span class="line"> 00012                                                               column&#x3D;baseInfo:name, timestamp&#x3D;1569461934129, value&#x3D;ddd</span><br><span class="line">6 row(s) in 0.0080 seconds</span><br></pre></td></tr></table></figure>



<h6 id="PrefixFilter"><a href="#PrefixFilter" class="headerlink" title="PrefixFilter"></a>PrefixFilter</h6><p>针对 rowkey 前缀来进行匹配的过滤器</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scan &#39;test:userInfo&#39;,&#123;FILTER&#x3D;&gt;&quot;PrefixFilter(&#39;0001&#39;)&quot;&#125;     # rowkey 前缀为 0001的所有行</span><br><span class="line">scan &#39;test:userInfo&#39;,FILTER&#x3D;&gt;&quot;PrefixFilter(&#39;0000&#39;)&quot;       # rowkey 前缀为 0000的所有行</span><br></pre></td></tr></table></figure>



<h6 id="PageFilter"><a href="#PageFilter" class="headerlink" title="PageFilter"></a>PageFilter</h6><p>取回N条数据。在shell中用法就是这样，API不知道能不能实现分页</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scan &#39;test:userInfo&#39;,FILTER&#x3D;&gt;&quot;PageFilter(8)&quot;     # 取回 8 条数据</span><br></pre></td></tr></table></figure>



<h6 id="KeyOnlyFilter"><a href="#KeyOnlyFilter" class="headerlink" title="KeyOnlyFilter"></a>KeyOnlyFilter</h6><p>一个参数 lenAsVal，默认为 false。false 时value返回为 空， true 时返回的是value的val值。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scan &#39;test:userInfo&#39;,FILTER&#x3D;&gt;&quot;KeyOnlyFilter(true)&quot;</span><br></pre></td></tr></table></figure>



<h6 id="FirstKeyOnlyFilter"><a href="#FirstKeyOnlyFilter" class="headerlink" title="FirstKeyOnlyFilter"></a>FirstKeyOnlyFilter</h6><p>返回每行第一个KV对，即每行的第一个 rowkey:列簇:列:value 对。对于count 和 sum 场景，可以带来性能提升。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scan &#39;test:userInfo&#39;,FILTER&#x3D;&gt;&quot;FirstKeyOnlyFilter()&quot;</span><br></pre></td></tr></table></figure>



<h6 id="InclusiveStopFilter"><a href="#InclusiveStopFilter" class="headerlink" title="InclusiveStopFilter"></a>InclusiveStopFilter</h6><p>设置 stoprow，返回stoprow之前的数据（包括 stoprow 行）。可以配合 startrow使用，相当于stoprow的闭区间用法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scan &#39;test:userInfo&#39;,&#123;FILTER&#x3D;&gt;&quot;InclusiveStopFilter(&#39;00009&#39;)&quot;&#125;   # 起始行到 00009 行</span><br><span class="line">scan &#39;test:userInfo&#39;,&#123;STARTROW&#x3D;&gt;&#39;00002&#39;,FILTER&#x3D;&gt;&quot;InclusiveStopFilter(&#39;00009&#39;)&quot;&#125; # 00002 到 00009 行</span><br></pre></td></tr></table></figure>



<h6 id="TimestampsFilter"><a href="#TimestampsFilter" class="headerlink" title="TimestampsFilter"></a>TimestampsFilter</h6><p>基于 时间戳来过滤。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scan &#39;test:userInfo&#39;,&#123;FILTER&#x3D;&gt;&quot;TimestampsFilter(1569403548008,1569396894851)&quot;&#125;</span><br></pre></td></tr></table></figure>



<h6 id="ColumnCountGetFilter"><a href="#ColumnCountGetFilter" class="headerlink" title="ColumnCountGetFilter"></a>ColumnCountGetFilter</h6><p>限制每行最多返回多少列。 更适用于 get，不是很适合 scan，但能用。</p>
<p>get 时，每行最多返回多少列。scan 时的用法很乱，没他看懂…</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">get &#39;test:userInfo&#39; , &#39;00003&#39; ,&#123;FILTER&#x3D;&gt;&quot;ColumnCountGetFilter(1)&quot;&#125;</span><br></pre></td></tr></table></figure>



<h6 id="ColumnPaginationFilter"><a href="#ColumnPaginationFilter" class="headerlink" title="ColumnPaginationFilter"></a>ColumnPaginationFilter</h6><p>限制返回列数，第一个参数为限制的列数，第二个参数为偏移量，即从第几列开始返回（0开始，）。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scan &#39;test:userInfo&#39;,&#123;FILTER&#x3D;&gt;&quot;ColumnPaginationFilter(1,1)&quot;&#125;  # 从第二列开始返回，返回一列</span><br></pre></td></tr></table></figure>



<h6 id="ColumnPrefixFilter"><a href="#ColumnPrefixFilter" class="headerlink" title="ColumnPrefixFilter"></a>ColumnPrefixFilter</h6><p>列前缀过滤器。过滤出列前缀为 XX 的所有数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scan &#39;test:userInfo&#39;,&#123;FILTER&#x3D;&gt;&quot;ColumnPrefixFilter(&#39;ag&#39;)&quot;&#125;</span><br></pre></td></tr></table></figure>



<h6 id="MultipleColumnPrefixFilter"><a href="#MultipleColumnPrefixFilter" class="headerlink" title="MultipleColumnPrefixFilter"></a>MultipleColumnPrefixFilter</h6><p>ColumnPrefixFilter 加强版，支持一种和多种前缀</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scan &#39;test:userInfo&#39;,&#123;FILTER&#x3D;&gt;&quot;MultipleColumnPrefixFilter(&#39;ag&#39;,&#39;na&#39;)&quot;&#125;</span><br></pre></td></tr></table></figure>



<h6 id="ColumnRangeFilter"><a href="#ColumnRangeFilter" class="headerlink" title="ColumnRangeFilter"></a>ColumnRangeFilter</h6><p>列范围过滤器。在指定的范围内查找所有列数据。四个参数：最小列、是否包含最小列、最大列、是否包含最大列。大小是按字典排序的。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scan &#39;test:userInfo&#39;,&#123;FILTER&#x3D;&gt;&quot;ColumnRangeFilter(&#39;ag&#39;,true,&#39;name&#39;,true)&quot;&#125;</span><br></pre></td></tr></table></figure>



<h6 id="RandomRowFilter-—"><a href="#RandomRowFilter-—" class="headerlink" title="RandomRowFilter —"></a>RandomRowFilter —</h6><p>shell不支持。</p>
<p>参数小于0，不返回数据。参数大于1，返回所有数据。参数位于 0-1，随机返回数据。</p>
<h6 id="SkipFilter-—"><a href="#SkipFilter-—" class="headerlink" title="SkipFilter —-"></a>SkipFilter —-</h6><p>shell不支持</p>
<p>一行中，只要存在一列不满足条件，整行都会被过滤</p>
<h6 id="WhileMatchFilters-—"><a href="#WhileMatchFilters-—" class="headerlink" title="WhileMatchFilters —-"></a>WhileMatchFilters —-</h6><p>相当于 while。直到 不 match ，然后 break 返回。</p>
]]></content>
      <categories>
        <category>Apache-Hbase</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink日常问题</title>
    <url>/2020/06/17/Apache-Flink/%E6%97%A5%E5%B8%B8%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h2 id="问题一："><a href="#问题一：" class="headerlink" title="问题一："></a>问题一：</h2><h6 id="flink日志一直报错如下："><a href="#flink日志一直报错如下：" class="headerlink" title="flink日志一直报错如下："></a>flink日志一直报错如下：</h6><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">2019-07-29 16:41:42,634 ERROR org.apache.flink.runtime.rest.handler.job.JobDetailsHandler   - Exception occurred in REST handler: Job 4b1ab21b418e7b4128838ee4efbde4dc not found</span><br><span class="line">2019-07-29 16:41:45,632 ERROR org.apache.flink.runtime.rest.handler.job.JobDetailsHandler   - Exception occurred in REST handler: Job 4b1ab21b418e7b4128838ee4efbde4dc not found</span><br><span class="line">2019-07-29 16:41:48,633 ERROR org.apache.flink.runtime.rest.handler.job.JobDetailsHandler   - Exception occurred in REST handler: Job 4b1ab21b418e7b4128838ee4efbde4dc not found</span><br><span class="line">2019-07-29 16:41:51,634 ERROR org.apache.flink.runtime.rest.handler.job.JobDetailsHandler   - Exception occurred in REST handler: Job 4b1ab21b418e7b4128838ee4efbde4dc not found</span><br><span class="line">2019-07-29 16:41:54,634 ERROR org.apache.flink.runtime.rest.handler.job.JobDetailsHandler   - Exception occurred in REST handler: Job 4b1ab21b418e7b4128838ee4efbde4dc not found</span><br><span class="line">2019-07-29 16:41:57,633 ERROR org.apache.flink.runtime.rest.handler.job.JobDetailsHandler   - Exception occurred in REST handler: Job 4b1ab21b418e7b4128838ee4efbde4dc not found</span><br><span class="line">2019-07-29 16:42:00,634 ERROR org.apache.flink.runtime.rest.handler.job.JobDetailsHandler   - Exception occurred in REST handler: Job 4b1ab21b418e7b4128838ee4efbde4dc not found</span><br><span class="line">2019-07-29 16:42:03,639 ERROR org.apache.flink.runtime.rest.handler.job.JobDetailsHandler   - Exception occurred in REST handler: Job 4b1ab21b418e7b4128838ee4efbde4dc not found</span><br><span class="line">2019-07-29 16:42:06,634 ERROR org.apache.flink.runtime.rest.handler.job.JobDetailsHandler   - Exception occurred in REST handler: Job 4b1ab21b418e7b4128838ee4efbde4dc not found</span><br><span class="line">2019-07-29 16:42:09,645 ERROR org.apache.flink.runtime.rest.handler.job.JobDetailsHandler   - Exception occurred in REST handler: Job 4b1ab21b418e7b4128838ee4efbde4dc not found</span><br><span class="line">2019-07-29 16:42:12,633 ERROR org.apache.flink.runtime.rest.handler.job.JobDetailsHandler   - Exception occurred in REST handler: Job 4b1ab21b418e7b4128838ee4efbde4dc not found</span><br><span class="line">2019-07-29 16:42:15,587 ERROR org.apache.flink.runtime.rest.handler.job.JobDetailsHandler   - Exception occurred in REST handler: Job 4b1ab21b418e7b4128838ee4efbde4dc not found</span><br><span class="line">2019-07-29 16:42:17,872 ERROR org.apache.flink.runtime.rest.handler.job.JobDetailsHandler   - Exception occurred in REST handler: Job 4b1ab21b418e7b4128838ee4efbde4dc not found</span><br></pre></td></tr></table></figure>
<h6 id="原因："><a href="#原因：" class="headerlink" title="原因："></a>原因：</h6><p> 有查看该任务的web页面未关闭，关闭之后就不报错了。</p>
<h2 id="问题二："><a href="#问题二：" class="headerlink" title="问题二："></a>问题二：</h2><h6 id="flink启动之后，checkpoint报错。看起来是kafka问题"><a href="#flink启动之后，checkpoint报错。看起来是kafka问题" class="headerlink" title="flink启动之后，checkpoint报错。看起来是kafka问题"></a>flink启动之后，checkpoint报错。看起来是kafka问题</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">org.apache.kafka.common.errors.TimeoutException</span><br></pre></td></tr></table></figure>
<h6 id="原因：-1"><a href="#原因：-1" class="headerlink" title="原因："></a>原因：</h6><p> 因为该任务sink是kafkaproducer，但是未创建topic，所以超时异常。</p>
<h2 id="问题三"><a href="#问题三" class="headerlink" title="问题三"></a>问题三</h2><p>之前flink checkpoint存在本地目录，存在两个问题。<br>一个是，每个运行任务的taskManager都有保存其checkpoint<br>第二个是，运行一段时间以后，总是会无法生存新的chk-n文件，然后下次一checkpoint就一直找不到文件，失败。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">java.lang.Exception: Exception while creating StreamOperatorStateContext.</span><br><span class="line"> at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:195)</span><br><span class="line"> at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:250)</span><br><span class="line"> at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeState(StreamTask.java:738)</span><br><span class="line"> at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:289)</span><br><span class="line"> at org.apache.flink.runtime.taskmanager.Task.run(Task.java:704)</span><br><span class="line"> at java.lang.Thread.run(Thread.java:748)</span><br><span class="line">Caused by: org.apache.flink.util.FlinkException: Could not restore operator state backend for StreamSource_6cdc5bb954874d922eaee11a8e7b5dd5_(9/18) from any of the 1 provided restore options.</span><br><span class="line"> at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:137)</span><br><span class="line"> at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.operatorStateBackend(StreamTaskStateInitializerImpl.java:245)</span><br><span class="line"> at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:143)</span><br><span class="line"> ... 5 more</span><br><span class="line">Caused by: java.io.FileNotFoundException: /data/flink/checkpoints/BDP_WATCH_ANDROID_PKSP_GROUP_2/8898aa4e9a78e55bd99c65313a077d78/chk-1955/6da428d6-9219-41f8-8b45-abcb7baff483 (没有那个文件或目录)</span><br><span class="line"> at java.io.FileInputStream.open0(Native Method)</span><br><span class="line"> at java.io.FileInputStream.open(FileInputStream.java:195)</span><br><span class="line"> at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:138)</span><br><span class="line"> at org.apache.flink.core.fs.local.LocalDataInputStream.&lt;init&gt;(LocalDataInputStream.java:50)</span><br><span class="line"> at org.apache.flink.core.fs.local.LocalFileSystem.open(LocalFileSystem.java:142)</span><br><span class="line"> at org.apache.flink.core.fs.SafetyNetWrapperFileSystem.open(SafetyNetWrapperFileSystem.java:85)</span><br><span class="line"> at org.apache.flink.runtime.state.filesystem.FileStateHandle.openInputStream(FileStateHandle.java:68)</span><br><span class="line"> at org.apache.flink.runtime.state.OperatorStreamStateHandle.openInputStream(OperatorStreamStateHandle.java:66)</span><br><span class="line"> at org.apache.flink.runtime.state.DefaultOperatorStateBackend.restore(DefaultOperatorStateBackend.java:286)</span><br><span class="line"> at org.apache.flink.runtime.state.DefaultOperatorStateBackend.restore(DefaultOperatorStateBackend.java:62)</span><br><span class="line"> at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:151)</span><br><span class="line"> at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:123)</span><br><span class="line"> ... 7 more</span><br></pre></td></tr></table></figure>
<h6 id="解决办法："><a href="#解决办法：" class="headerlink" title="解决办法："></a>解决办法：</h6><p> 将状态后端修改为 hdfs，然后将checkpoint保存到hdfs路径下</p>
]]></content>
      <categories>
        <category>Apache-Flink</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Hbase基础API</title>
    <url>/2020/05/01/Apache-Hbase/Hbase%20%E5%9F%BA%E7%A1%80API/</url>
    <content><![CDATA[<h5 id="全是代码"><a href="#全是代码" class="headerlink" title="全是代码"></a>全是代码</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.<span class="type">Hbase</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase._</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client._</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.filter.<span class="type">CompareFilter</span>.<span class="type">CompareOp</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.filter.<span class="type">SingleColumnValueFilter</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HbaseDDL</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 确认表是否存在</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ensureHbaseTableExist</span></span>(tableName:<span class="type">TableName</span>) = &#123;</span><br><span class="line">      <span class="comment">// 配置 Hbase</span></span><br><span class="line">      <span class="keyword">val</span> hbaseconf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">      <span class="comment">//val zkConn = "bigdata:2181, bigdata:2182, bigdata:2183"</span></span><br><span class="line">      <span class="comment">//hbaseconf.set("hbase.zookeeper.quorum", zkConn)</span></span><br><span class="line">      <span class="keyword">val</span> <span class="type">HbaseConn</span> = <span class="type">ConnectionFactory</span>.createConnection(hbaseconf)</span><br><span class="line">      <span class="keyword">val</span> adminHbase = <span class="type">HbaseConn</span>.getAdmin()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> ifExist = adminHbase.tableExists(tableName)</span><br><span class="line"></span><br><span class="line">      ifExist</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 确认表是否存在 测试</span></span><br><span class="line"><span class="comment">//    val result = ensureHbaseTableExist("user_info1")</span></span><br><span class="line"><span class="comment">//    if (result) &#123;</span></span><br><span class="line"><span class="comment">//      println("表存在")</span></span><br><span class="line"><span class="comment">//    &#125; else &#123;</span></span><br><span class="line"><span class="comment">//      println("表不存在")</span></span><br><span class="line"><span class="comment">//    &#125;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * Hbase建表  两个参数</span></span><br><span class="line"><span class="comment">      * @param tableName   形式为 ns:tb  或者  tb    API 创建 namespace 机会不多，一般通过 hbase shell 创建</span></span><br><span class="line"><span class="comment">      * @param columnFamilys  cf1,cf2,cf3</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">createHbaseTable</span></span>(tableName:<span class="type">String</span>, columnFamilys:<span class="type">String</span>) = &#123;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 配置 Hbase</span></span><br><span class="line">      <span class="keyword">val</span> hbaseconf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">      <span class="comment">//val zkConn = "bigdata:2181, bigdata:2182, bigdata:2183"</span></span><br><span class="line">      <span class="comment">//hbaseconf.set("hbase.zookeeper.quorum", zkConn)</span></span><br><span class="line">      <span class="keyword">val</span> <span class="type">HbaseConn</span> = <span class="type">ConnectionFactory</span>.createConnection(hbaseconf)</span><br><span class="line">      <span class="keyword">val</span> adminHbase = <span class="type">HbaseConn</span>.getAdmin()</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 列簇逗号分隔</span></span><br><span class="line">      <span class="keyword">val</span> <span class="type">CFS</span> = columnFamilys.split(<span class="string">","</span>)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 表名，判断是否带了namespace，带了则判断是否存在 namespace, 不存在则创建</span></span><br><span class="line">      <span class="keyword">val</span> nameSpace = tableName.split(<span class="string">":"</span>)(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (nameSpace != tableName) &#123;</span><br><span class="line">        adminHbase.createNamespace(<span class="type">NamespaceDescriptor</span>.create(tableName.split(<span class="string">":"</span>)(<span class="number">0</span>)).build())</span><br><span class="line">        println(<span class="string">"NameSpace 创建成功！"</span>)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 判断表是否存在，不存在新建，存在则提示</span></span><br><span class="line">      <span class="keyword">if</span> (!ensureHbaseTableExist(<span class="type">TableName</span>.valueOf(tableName))) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 实例化 HTableDescriptor</span></span><br><span class="line">        <span class="keyword">val</span> htable = <span class="keyword">new</span> <span class="type">HTableDescriptor</span>(<span class="type">TableName</span>.valueOf(tableName))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 循环添加所有列簇</span></span><br><span class="line">        <span class="keyword">for</span> ( columnFamily &lt;- <span class="type">CFS</span>) &#123;</span><br><span class="line"></span><br><span class="line">          <span class="comment">// 实例化 HColumnDescriptor</span></span><br><span class="line">          <span class="keyword">val</span> htableColumnFamily1 = <span class="keyword">new</span> <span class="type">HColumnDescriptor</span>((columnFamily))</span><br><span class="line">          <span class="comment">// 调用 HColumnDescriptor 设置列簇属性</span></span><br><span class="line">          htableColumnFamily1.setMaxVersions(<span class="number">3</span>)</span><br><span class="line">          <span class="comment">// 表增加列族</span></span><br><span class="line">          htable.addFamily(<span class="keyword">new</span> <span class="type">HColumnDescriptor</span>(columnFamily))</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 创建表</span></span><br><span class="line">        adminHbase.createTable(htable)</span><br><span class="line">        println(<span class="string">"表创建成功"</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        println(<span class="string">"表已存在"</span>)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">       adminHbase.close()</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 测试建表</span></span><br><span class="line">   <span class="comment">// createHbaseTable("scTable3", "info,base")</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      *  列出所有表</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">listAllHbaseTable</span></span>() =&#123;</span><br><span class="line">      <span class="comment">// 配置 Hbase</span></span><br><span class="line">      <span class="keyword">val</span> hbaseconf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">      <span class="comment">//val zkConn = "bigdata:2181, bigdata:2182, bigdata:2183"</span></span><br><span class="line">      <span class="comment">//hbaseconf.set("hbase.zookeeper.quorum", zkConn)</span></span><br><span class="line">      <span class="keyword">val</span> <span class="type">HbaseConn</span> = <span class="type">ConnectionFactory</span>.createConnection(hbaseconf)</span><br><span class="line">      <span class="keyword">val</span> adminHbase = <span class="type">HbaseConn</span>.getAdmin()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> listTables =  adminHbase.listTableNames()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">for</span>(table &lt;- listTables)&#123;</span><br><span class="line">        println(table)</span><br><span class="line">      &#125;</span><br><span class="line">      adminHbase.close()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//listAllHbaseTable()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 删除一张表，输入表名</span></span><br><span class="line"><span class="comment">      * 判断是否存在，是否失效，否则不能删除</span></span><br><span class="line"><span class="comment">      *</span></span><br><span class="line"><span class="comment">      * @param tableName</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">deleteHbaseTable</span></span>(tableName: <span class="type">String</span>) =&#123;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> hbaseconf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">      <span class="keyword">val</span> <span class="type">HbaseConn</span> = <span class="type">ConnectionFactory</span>.createConnection(hbaseconf)</span><br><span class="line">      <span class="keyword">val</span> adminHbase = <span class="type">HbaseConn</span>.getAdmin()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> tbName = <span class="type">TableName</span>.valueOf(tableName)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span>(ensureHbaseTableExist(tbName))&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 若表不失效，则使失效</span></span><br><span class="line">        <span class="keyword">if</span>(!adminHbase.isTableDisabled(tbName))&#123;</span><br><span class="line">          adminHbase.disableTable(tbName)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        adminHbase.deleteTable(tbName)</span><br><span class="line">        println(<span class="string">"删除成功"</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        println(<span class="string">"表不存在"</span>)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      adminHbase.close()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//deleteHbaseTable("scTable3")</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      *</span></span><br><span class="line"><span class="comment">      * 删除表的某个列族  ，得到 HTableDescriptor , 调用该类的 removeFamily 方法</span></span><br><span class="line"><span class="comment">      * @param tableName   表名  ---&gt;   ns:tb   or    tb   [String]</span></span><br><span class="line"><span class="comment">      * @param columnFamily   列族名  ---&gt;   String</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">deleteHbaseColumnFamily</span></span>(tableName:<span class="type">String</span>, columnFamily:<span class="type">String</span>) =&#123;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> hbaseconf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">      <span class="keyword">val</span> <span class="type">HbaseConn</span> = <span class="type">ConnectionFactory</span>.createConnection(hbaseconf)</span><br><span class="line">      <span class="keyword">val</span> adminHbase = <span class="type">HbaseConn</span>.getAdmin()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> tbName = <span class="type">TableName</span>.valueOf(tableName)</span><br><span class="line">      <span class="comment">// disable table</span></span><br><span class="line">      adminHbase.disableTable(tbName)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// get HTableDescriptor</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> htd = adminHbase.getTableDescriptor(tbName)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// delete family</span></span><br><span class="line">      htd.removeFamily(columnFamily.getBytes())</span><br><span class="line"></span><br><span class="line">      <span class="comment">// apply htd to table</span></span><br><span class="line">      adminHbase.modifyTable(tbName, htd)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// enable table</span></span><br><span class="line">      adminHbase.enableTable(tbName)</span><br><span class="line"></span><br><span class="line">      println(<span class="string">"删除成功"</span>)</span><br><span class="line"></span><br><span class="line">      adminHbase.close()</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// deleteHbaseColumnFamily("scTable3", "base")</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 给表增加列族    先得到表的 HTableDescriptor, 然后使用 HColumnDescriptor 初始化 新增列，并设置属性</span></span><br><span class="line"><span class="comment">      * 调用 HTableDescriptor 的 addFamily 方法，将初始化好的 HCD 添加到  HTableDescriptor ,然后使用admin 的 modifyTable 方法将修改应用</span></span><br><span class="line"><span class="comment">      * @param tableName</span></span><br><span class="line"><span class="comment">      * @param columnFamily</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addHbaseColumnFamily</span></span>(tableName:<span class="type">String</span>, columnFamily:<span class="type">String</span>) =&#123;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> hbaseconf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">      <span class="keyword">val</span> <span class="type">HbaseConn</span> = <span class="type">ConnectionFactory</span>.createConnection(hbaseconf)</span><br><span class="line">      <span class="keyword">val</span> adminHbase = <span class="type">HbaseConn</span>.getAdmin()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> tbName = <span class="type">TableName</span>.valueOf(tableName)</span><br><span class="line">      <span class="comment">// disable table</span></span><br><span class="line">      adminHbase.disableTable(tbName)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// get HTableDescriptor</span></span><br><span class="line">      <span class="keyword">val</span> htd = adminHbase.getTableDescriptor(tbName)</span><br><span class="line"></span><br><span class="line">      <span class="comment">//</span></span><br><span class="line">      <span class="keyword">val</span> hcd = <span class="keyword">new</span> <span class="type">HColumnDescriptor</span>(columnFamily)</span><br><span class="line">      hcd.setMaxVersions(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// add family</span></span><br><span class="line">      htd.addFamily(hcd)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// apply htd to table</span></span><br><span class="line">      adminHbase.modifyTable(tbName, htd)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// enable table</span></span><br><span class="line">      adminHbase.enableTable(tbName)</span><br><span class="line"></span><br><span class="line">      println(<span class="string">"添加成功"</span>)</span><br><span class="line"></span><br><span class="line">      adminHbase.close()</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//addHbaseColumnFamily("scTable3", "base")</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 修改列簇功能  get 到  HTableDescriptor ，再 get 到 Family ，设置 Family ，admin modifyTable  应用</span></span><br><span class="line"><span class="comment">      * @param tableName</span></span><br><span class="line"><span class="comment">      * @param columnFamily</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">modifyHbaseTableColumnFamily</span></span>(tableName:<span class="type">String</span>, columnFamily:<span class="type">String</span>)  =&#123;</span><br><span class="line">      <span class="keyword">val</span> hbaseconf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">      <span class="keyword">val</span> <span class="type">HbaseConn</span> = <span class="type">ConnectionFactory</span>.createConnection(hbaseconf)</span><br><span class="line">      <span class="keyword">val</span> adminHbase = <span class="type">HbaseConn</span>.getAdmin()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> tbName = <span class="type">TableName</span>.valueOf(tableName)</span><br><span class="line"></span><br><span class="line">      adminHbase.disableTable(tbName)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> htd = adminHbase.getTableDescriptor(tbName)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> modifyCol  = htd.getFamily(columnFamily.getBytes())</span><br><span class="line">      modifyCol.setMaxVersions(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">      adminHbase.modifyTable(tbName,htd)</span><br><span class="line"></span><br><span class="line">      adminHbase.enableTable(tbName)</span><br><span class="line"></span><br><span class="line">      adminHbase.close()</span><br><span class="line"></span><br><span class="line">      println(<span class="string">"修改成功！"</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//modifyHbaseTableColumnFamily("scTable3", "info")</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 插入数据，五个参数</span></span><br><span class="line"><span class="comment">      * @param tableName</span></span><br><span class="line"><span class="comment">      * @param columnFamily</span></span><br><span class="line"><span class="comment">      * @param column</span></span><br><span class="line"><span class="comment">      * @param rowkey</span></span><br><span class="line"><span class="comment">      * @param value</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">putDataHbaseTable</span></span>(tableName:<span class="type">String</span>, columnFamily:<span class="type">String</span>, column:<span class="type">String</span>,</span><br><span class="line">                          rowkey:<span class="type">String</span>, value:<span class="type">String</span>) =&#123;</span><br><span class="line">      <span class="keyword">val</span> hbaseconf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line"></span><br><span class="line">      <span class="comment">// table</span></span><br><span class="line">      <span class="keyword">val</span> hTable = <span class="keyword">new</span> <span class="type">HTable</span>(hbaseconf, tableName)</span><br><span class="line">      <span class="comment">// row key</span></span><br><span class="line">      <span class="keyword">val</span> putData = <span class="keyword">new</span> <span class="type">Put</span>(rowkey.getBytes())</span><br><span class="line">      <span class="comment">// put value</span></span><br><span class="line">      putData.add(columnFamily.getBytes(), column.getBytes(), value.getBytes())</span><br><span class="line"></span><br><span class="line">      <span class="comment">/**</span></span><br><span class="line"><span class="comment">        * 插入方式</span></span><br><span class="line"><span class="comment">        * ASYNC_WAL ： 当数据变动时，异步写WAL日志</span></span><br><span class="line"><span class="comment">        * SYNC_WAL ： 当数据变动时，同步写WAL日志</span></span><br><span class="line"><span class="comment">        * FSYNC_WAL ： 当数据变动时，同步写WAL日志，并且，强制将数据写入磁盘</span></span><br><span class="line"><span class="comment">        * SKIP_WAL ： 不写WAL日志</span></span><br><span class="line"><span class="comment">        * USE_DEFAULT ： 使用HBase全局默认的WAL写入级别，即 SYNC_WAL</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">      putData.setDurability(<span class="type">Durability</span>.<span class="type">SYNC_WAL</span>)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// put data to table</span></span><br><span class="line">      hTable.put(putData)</span><br><span class="line"></span><br><span class="line">      println(<span class="string">"插入数据成功！"</span>)</span><br><span class="line">      <span class="comment">// close table</span></span><br><span class="line">      hTable.close()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//putDataHbaseTable("scTable3", "info", "name", "rk0002", "lalala")</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">deleteDataHbaseTable</span></span>(tableName: <span class="type">String</span>, rowkey:<span class="type">String</span>, columnFamily:<span class="type">String</span>,</span><br><span class="line">                             column:<span class="type">String</span> = <span class="literal">null</span>)=&#123;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> hbaseConf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> hTable = <span class="keyword">new</span> <span class="type">HTable</span>(hbaseConf, tableName)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 初始化 Delete ，表名后可接时间戳</span></span><br><span class="line">      <span class="keyword">val</span> deletaData = <span class="keyword">new</span> <span class="type">Delete</span>(rowkey.getBytes())</span><br><span class="line"></span><br><span class="line">      <span class="comment">/**</span></span><br><span class="line"><span class="comment">        *   1).删除指定列的 最新版本 的数据：Delete addColumn (byte[] family, byte[] qualifier)</span></span><br><span class="line"><span class="comment">        *   2).删除指定列的 指定版本 的数据：Delete addColumn (byte[] family, byte[] qualifier, long timestamp )</span></span><br><span class="line"><span class="comment">        *   3).删除指定列的 所有版本 的数据：Delete addColumns (byte[] family, byte[] qualifier)</span></span><br><span class="line"><span class="comment">        *   4).删除指定列的，时间戳 小于等于 给定时间戳的 所有版本 的数据：Delete addColumns (byte[] family, byte[] qualifier, long timestamp )</span></span><br><span class="line"><span class="comment">        *   5).删除指定列族的所有列的 所有版本 数据：Delete addFamily (byte[] family)    默认使用当前时间的时间戳，时间戳大于当前时间的数据删除不掉</span></span><br><span class="line"><span class="comment">        *   6).删除指定列族的所有列中 时间戳 小于等于 指定时间戳 的所有数据：Delete addFamily (byte[] family, long timestamp)</span></span><br><span class="line"><span class="comment">        *   7).删除指定列族中 所有列的时间戳 等于 指定时间戳的版本数据：Delete addFamilyVersion (byte[] family, long timestamp)</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">      deletaData.addColumn(columnFamily.getBytes(),column.getBytes())</span><br><span class="line">      <span class="comment">//deletaData.addColumns(columnFamily.getBytes(),column.getBytes())</span></span><br><span class="line">      <span class="comment">//deletaData.addFamily(columnFamily.getBytes())</span></span><br><span class="line">      hTable.delete(deletaData)</span><br><span class="line"></span><br><span class="line">      println(<span class="string">"删除成功"</span>)</span><br><span class="line">      hTable.close()</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// deleteDataHbaseTable("scTable3", "rk0002", "info")</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getDataHbaseTable</span></span>(tableName:<span class="type">String</span>, rowkey:<span class="type">String</span>, columnFamily:<span class="type">String</span>, column:<span class="type">String</span> = <span class="literal">null</span>)=&#123;</span><br><span class="line">      <span class="keyword">val</span> hbaseCOnf =  <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> hTable = <span class="keyword">new</span> <span class="type">HTable</span>(hbaseCOnf, tableName)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> getData = <span class="keyword">new</span> <span class="type">Get</span>(rowkey.getBytes())</span><br><span class="line"></span><br><span class="line">      <span class="comment">/**</span></span><br><span class="line"><span class="comment">        *    1). Get addFamily(byte[] family) 指定希望获取的列族</span></span><br><span class="line"><span class="comment">        *    2). Get addColumn(byte[] family, byte[] qualifier) 指定希望获取的列</span></span><br><span class="line"><span class="comment">        *    3). Get setTimeRange(long minStamp, long maxStamp) 设置获取数据的 时间戳范围</span></span><br><span class="line"><span class="comment">        *    4). Get setTimeStamp(long timestamp) 设置获取数据的时间戳</span></span><br><span class="line"><span class="comment">        *    5). Get setMaxVersions(int maxVersions) 设定获取数据的版本数</span></span><br><span class="line"><span class="comment">        *    6). Get setMaxVersions() 设定获取数据的所有版本</span></span><br><span class="line"><span class="comment">        *    7). Get setFilter(Filter filter) 为Get对象添加过滤器</span></span><br><span class="line"><span class="comment">        *    8). void setCacheBlocks(boolean cacheBlocks) 设置该Get获取的数据是否缓存在内存中</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">      <span class="comment">//getData.addFamily(columnFamily.getBytes())</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">//getData.addColumn(columnFamily.getBytes(), column.getBytes())</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">//getData.setTimeStamp("1535680422860".toLong)</span></span><br><span class="line"></span><br><span class="line">      getData.setMaxVersions()</span><br><span class="line">      <span class="keyword">val</span> results = hTable.get(getData)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">for</span> (result &lt;- results.rawCells())&#123;</span><br><span class="line"></span><br><span class="line">        println( <span class="keyword">new</span> <span class="type">String</span>(<span class="type">CellUtil</span>.cloneRow(result)) + <span class="string">"\t"</span> +</span><br><span class="line">          <span class="keyword">new</span> <span class="type">String</span>(<span class="type">CellUtil</span>.cloneFamily(result)) + <span class="string">"\t"</span> +</span><br><span class="line">          <span class="keyword">new</span> <span class="type">String</span>(<span class="type">CellUtil</span>.cloneQualifier(result)) + <span class="string">"\t"</span> +</span><br><span class="line">          <span class="keyword">new</span> <span class="type">String</span>(<span class="type">CellUtil</span>.cloneValue(result)) + <span class="string">"\t"</span> +</span><br><span class="line">          result.getTimestamp)</span><br><span class="line"></span><br><span class="line">      &#125;</span><br><span class="line">      hTable.close()</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//getDataHbaseTable("scTable", "rk0002", "info", "age")</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">scanDataHbaseTable</span></span>(tableName:<span class="type">String</span>, startRow:<span class="type">String</span>, stopRow:<span class="type">String</span>,</span><br><span class="line">                           columnFamily:<span class="type">String</span>, column:<span class="type">String</span>)=&#123;</span><br><span class="line">      <span class="keyword">val</span> hBaseConf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">      <span class="keyword">val</span> hTable = <span class="keyword">new</span> <span class="type">HTable</span>(hBaseConf, tableName)</span><br><span class="line"></span><br><span class="line">      <span class="comment">/**</span></span><br><span class="line"><span class="comment">        *         1). 创建扫描所有行的Scan：Scan()</span></span><br><span class="line"><span class="comment">        * 　　2). 创建Scan，从指定行开始扫描：Scan(byte[] startRow)</span></span><br><span class="line"><span class="comment">        * 　　注意：如果指定行不存在，从下一个最近的行开始</span></span><br><span class="line"><span class="comment">        * 　　3). 创建Scan，指定起止行：Scan(byte[] startRow, byte[] stopRow)</span></span><br><span class="line"><span class="comment">        * 　　注意： startRow &lt;= 结果集 &lt; stopRow</span></span><br><span class="line"><span class="comment">        * 　　4). 创建Scan，指定起始行和过滤器：Scan(byte[] startRow, Filter filter)</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">      <span class="keyword">val</span> scanData = <span class="keyword">new</span> <span class="type">Scan</span>()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> filter1 = <span class="keyword">new</span> <span class="type">SingleColumnValueFilter</span>(columnFamily.getBytes(), column.getBytes(), <span class="type">CompareOp</span>.<span class="type">GREATER_OR_EQUAL</span>, <span class="string">"60"</span>.getBytes() )</span><br><span class="line"></span><br><span class="line">      <span class="comment">/**</span></span><br><span class="line"><span class="comment">        * Scan setStartRow (byte[] startRow) 设置Scan的开始行，默认 结果集 包含该行。如果希望结果集不包含该行，可以在行键末尾加上0。</span></span><br><span class="line"><span class="comment">        * Scan setStopRow (byte[] stopRow) 设置Scan的结束行，默认 结果集 不包含该行。如果希望结果集包含该行，可以在行键末尾加上0。</span></span><br><span class="line"><span class="comment">        * Scan setBatch(int batch) 指定最多返回的Cell数目.用于防止一行中有过多的数据，导致OutofMemory错误</span></span><br><span class="line"><span class="comment">        * Scan setTimeRange (long minStamp, long maxStamp) 扫描指定 时间范围 的数据</span></span><br><span class="line"><span class="comment">        * Scan setTimeStamp (long timestamp) 扫描 指定时间 的数据</span></span><br><span class="line"><span class="comment">        * Scan addColumn (byte[] family, byte[] qualifier) 指定扫描的列</span></span><br><span class="line"><span class="comment">        * Scan addFamily (byte[] family) 指定扫描的列族</span></span><br><span class="line"><span class="comment">        * Scan setFilter (Filter filter) 为Scan设置过滤器，详见HBase API Filter过滤器</span></span><br><span class="line"><span class="comment">        * Scan setReversed (boolean reversed) 设置Scan的扫描顺序，默认是正向扫描（false），可以设置为逆向扫描（true）。注意：该方法0.98版本以后才可用！！</span></span><br><span class="line"><span class="comment">        * Scan setMaxVersions () 获取所有版本的数据</span></span><br><span class="line"><span class="comment">        * Scan setMaxVersions (int maxVersions) 设置获取的最大版本数! 不调用上下两个setMaxVersions() 方法,只返回最新版本数据</span></span><br><span class="line"><span class="comment">        * void setCaching (int caching) 设定缓存在内存中的行数，缓存得越多，以后查询结果越快，同时也消耗更多内存</span></span><br><span class="line"><span class="comment">        * void setRaw (boolean raw) 激活或者禁用raw模式。如果raw模式被激活，Scan将返回 所有已经被打上删除标记但尚未被真正删除 的数据。该功能仅用于激活了 KEEP_DELETED_ROWS的列族，即列族开启了 hcd.setKeepDeletedCells(true)</span></span><br><span class="line"><span class="comment">        * Scan激活raw模式后，只能浏览所有的列，而不能指定任意的列，否则会报错</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">      scanData.setFilter(filter1)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> resultsScan:<span class="type">ResultScanner</span> = hTable.getScanner(scanData)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      <span class="keyword">while</span> (resultsScan.iterator().hasNext)&#123;</span><br><span class="line">        <span class="keyword">val</span> results = resultsScan.iterator().next()</span><br><span class="line">        <span class="keyword">for</span> (result:<span class="type">Cell</span> &lt;- results.rawCells()) &#123;</span><br><span class="line"></span><br><span class="line">                    println(<span class="keyword">new</span> <span class="type">String</span>(<span class="type">CellUtil</span>.cloneRow(result)) + <span class="string">"\t"</span> +</span><br><span class="line">                      <span class="keyword">new</span> <span class="type">String</span>(<span class="type">CellUtil</span>.cloneFamily(result)) + <span class="string">"\t"</span> +</span><br><span class="line">                      <span class="keyword">new</span> <span class="type">String</span>(<span class="type">CellUtil</span>.cloneQualifier(result)) + <span class="string">"\t"</span> +</span><br><span class="line">                      <span class="keyword">new</span> <span class="type">String</span>(<span class="type">CellUtil</span>.cloneValue(result)) + <span class="string">"\t"</span> +</span><br><span class="line">                      result.getTimestamp)</span><br><span class="line">                  &#125;</span><br><span class="line"></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">/**</span></span><br><span class="line"><span class="comment">        * for 循环无法直接遍历 ResultScanner 暂无办法</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line"><span class="comment">//      for(results:Result &lt;- resultsScan)&#123;</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//        for (result:Cell &lt;- results.rawCells()) &#123;</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//          println(new String(CellUtil.cloneRow(result)) + "\t" +</span></span><br><span class="line"><span class="comment">//            new String(CellUtil.cloneFamily(result)) + "\t" +</span></span><br><span class="line"><span class="comment">//            new String(CellUtil.cloneQualifier(result)) + "\t" +</span></span><br><span class="line"><span class="comment">//            new String(CellUtil.cloneValue(result)) + "\t" +</span></span><br><span class="line"><span class="comment">//            result.getTimestamp)</span></span><br><span class="line"><span class="comment">//        &#125;</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//      &#125;</span></span><br><span class="line">      hTable.close()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//scanDataHbaseTable("scTable", "rk0001", "rk0002", "info", "age")</span></span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Apache-Hbase</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>Hbase刷写分析</title>
    <url>/2020/05/22/Apache-Hbase/Hbase%E5%88%B7%E5%86%99%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h3 id="Hbase-MemStore-Flush"><a href="#Hbase-MemStore-Flush" class="headerlink" title="Hbase MemStore Flush"></a>Hbase MemStore Flush</h3><p>熟悉Hbase写流程的同学一定知道，我们Hbase写数据的时候，都是先写WAL日志的，然后再写到一个叫 MemStore 的内存结构里面，最终，MemStore 里面的数据需要 flush 持久化到磁盘，生成 HFile。有次来说，MemStore 的flush触发和flush策略极其重要。</p>
<h4 id="MemStore-Flush-的触发条件"><a href="#MemStore-Flush-的触发条件" class="headerlink" title="MemStore Flush 的触发条件"></a>MemStore Flush 的触发条件</h4><ul>
<li>单个 Region 中所有 MemStore 占用的内存总和超过相关阈值</li>
<li>整个 RegionServer 的 MemStore 占用内存中和超过相关阈值</li>
<li>WAL 数据量超过相关阈值</li>
<li>触发了 MemStore 的定期 Flush 时间</li>
<li>数据更新超过一定阈值</li>
<li>手动触发 Flush<br>先一个一个来看。<h5 id="单个-Region-中所有-MemStore-占用的内存总和超过相关阈值"><a href="#单个-Region-中所有-MemStore-占用的内存总和超过相关阈值" class="headerlink" title="单个 Region 中所有 MemStore 占用的内存总和超过相关阈值"></a>单个 Region 中所有 MemStore 占用的内存总和超过相关阈值</h5>当一个 Region 中的所有 MemStore 占用的内存大小超过 <code>hbase.hregion.memstore.flush.size</code> 值时，会触发 Flush。该值默认为 128M。 每次调用 put，delete等操作均会检查此条件。</li>
</ul>
<p>另外，如果数据增加的很快，达到了上面该值的多倍(即 <code>hbase.hregion.memstore.flush.size</code> * <code>hbase.hregion.memstore.block.multiplier</code>  ) 时，除了触发 Flush 外，还会阻塞所有写入该 MemStore 的请求，如果还继续写就会抛 <code>RegionTooBusyException</code> 异常。</p>
<p>其中 <code>hbase.hregion.memstore.block.multiplier</code> 控制 MemStore 自我保护的参数，默认为4。即写入数据超过MemStore 四倍大小时，MemStore 会开启自我保护。</p>
<h5 id="整个-RegionServer-的-MemStore-占用内存中和超过相关阈值"><a href="#整个-RegionServer-的-MemStore-占用内存中和超过相关阈值" class="headerlink" title="整个 RegionServer 的 MemStore 占用内存中和超过相关阈值"></a>整个 RegionServer 的 MemStore 占用内存中和超过相关阈值</h5><ul>
<li><p>每个 RegionServer的写缓存：Hbase 为 RegionServer 的 MemStore 分配了一定大小的写缓存，大小等于 RegionServer 的堆内存( <code>hbase_heapsize</code> 。 ) * <code>hbase.regionserver.global.memstore.size</code>。<code>hbase.regionserver.global.memstore.size</code> 的默认值是 0.4，即写缓存占用了整个 JVM 内存的 40%。  </p>
</li>
<li><p>如果 RegionServer 的所有 MemStore 占用内存总和超过了 写缓存的一定比例，则会触发 Flush。即 总占用内存超过 <code>hbase.regionserver.global.memstore.size.lower.limit</code> * <code>hbase.regionserver.global.memstore.size</code> * <code>hbase_heapsize</code> 。其中， <code>hbase.regionserver.global.memstore.size.lower.limit</code> 的默认值是 0.95。</p>
</li>
<li><p>RegionServer 级别的 Flush 是每次找到 RegionServer 中占用内存最大的 Region 对之进行 Flush。此操作是循环的，直到 占用总内存小于 总写缓存的比例。</p>
<p>  <strong>注意！！</strong> <strong><em>如果 触发 的是 RegionServer 级别的 Flush，那么此 RegionServer 的所有写操作均会被阻塞。</em></strong></p>
</li>
</ul>
<h5 id="WAL-数据量超过相关阈值"><a href="#WAL-数据量超过相关阈值" class="headerlink" title="WAL 数据量超过相关阈值"></a>WAL 数据量超过相关阈值</h5><p>  WAL (Write-ahead log，预写日志)，目的是解决宕机之后的操作恢复问题。数据写入到 MemStore 之前，是需要先写到 WAL 的，如果 WAL 的数量越来越多，也就是 MemStore 中未 Flush 的数据越来越多。如果 RegionServer 此时宕机，那么恢复操作的时间就会变得很长，因此，在 WAL 达到一定数量的时候，需要进行一次 Flush。</p>
<p>触发条件为 <code>hbase.regionserver.maxlogs</code>，如果未设置这个值，那么取 <code>max(32, hbase_heapsize * hbase.regionserver.global.memstore.size * 2 / logRollSize)</code> 。其中，<code>logRollSize</code> 是每个 WAL 文件的大小，取值为 <code>hbase.regionserver.logroll.multiplier * hbase.regionserver.hlog.blocksize</code>，即 每个 WAL 文件达到块的百分之多少就生成新的 WAL （这个块的大小不是HDFS块的大小，默认是HDFS块大小的50%，2.0之后是 95%）。</p>
<h5 id="定期自动-Flush"><a href="#定期自动-Flush" class="headerlink" title="定期自动 Flush"></a>定期自动 Flush</h5><p>RegionServer 在启动的时候会启动一个线程去检查此 RegionServer 的 Region 是否达到了定期 Flush 的时间，这个时间由配置 <code>hbase.regionserver.optionalcacheflushinterval</code> 控制的，默认是36000000，即一小时。需要注意的是，定期刷写是有几分钟的延迟的，目的是防止同时有过多的 MemStore Flush。</p>
<h5 id="数据更新超过一定阈值"><a href="#数据更新超过一定阈值" class="headerlink" title="数据更新超过一定阈值"></a>数据更新超过一定阈值</h5><p>如果 RegionServer 的某个 Region 数据更新很频繁，既没有达到自动 Flush 的要求，又没达到内存的使用限制，但是数据条数够多。此时也是会触发 MemStore Flush 的。这个条数的限制是 <code>hbase.regionserver.flush.per.changes</code>  控制的，默认是 30000000 。</p>
<h5 id="手动触发-Flush"><a href="#手动触发-Flush" class="headerlink" title="手动触发 Flush"></a>手动触发 Flush</h5><h4 id="那么什么操作会触发-MemStore-Flush-呢"><a href="#那么什么操作会触发-MemStore-Flush-呢" class="headerlink" title="那么什么操作会触发 MemStore Flush 呢"></a>那么什么操作会触发 MemStore Flush 呢</h4><p>常见的操作有put、delete、append、increment、flush、Region 分裂、Region Merge、bulkLoad HFiles 以及表做快照，此类操作都会检查是否符合 Flush 的条件。</p>
<h5 id="MemStore-Flush-策略（FlushPolicy）"><a href="#MemStore-Flush-策略（FlushPolicy）" class="headerlink" title="MemStore Flush 策略（FlushPolicy）"></a>MemStore Flush 策略（FlushPolicy）</h5><p>在 HBase 1.1 之前，MemStore 刷写是 Region 级别的。就是说，如果要刷写某个 MemStore ，MemStore 所在的 Region 中其他 MemStore 也是会被一起刷写的！这会造成一定的问题，比如小文件问题。针对这个问题，<a href="https://www.iteblog.com/redirect.php?url=aHR0cHM6Ly9pc3N1ZXMuYXBhY2hlLm9yZy9qaXJhL2Jyb3dzZS9IQkFTRS0xMDIwMQ==&article=true" target="_blank" rel="noopener">HBASE-10201</a>/<a href="https://www.iteblog.com/redirect.php?url=aHR0cHM6Ly9pc3N1ZXMuYXBhY2hlLm9yZy9qaXJhL2Jyb3dzZS9IQkFTRS0zMTQ5&article=true" target="_blank" rel="noopener">HBASE-3149</a> 引入列族级别的刷写。我们可以通过 <code>hbase.regionserver.flush.policy</code> 参数选择不同的刷写策略。</p>
<p>目前 HBase 2.0.2 的刷写策略全部都是实现 <code>FlushPolicy</code> 抽象类的。并且自带三种刷写策略：<code>FlushAllLargeStoresPolicy</code>、<code>FlushNonSloppyStoresFirstPolicy</code> 以及 <code>FlushAllStoresPolicy</code>。</p>
<h6 id="FlushAllStoresPolicy"><a href="#FlushAllStoresPolicy" class="headerlink" title="FlushAllStoresPolicy"></a>FlushAllStoresPolicy</h6><p>这种刷写策略实现最简单，直接返回当前 Region 对应的所有 MemStore。也就是每次刷写都是对 Region 里面所有的 MemStore 进行的，这个行为和 HBase 1.1 之前是一样的。</p>
<h6 id="FlushAllLargeStoresPolicy"><a href="#FlushAllLargeStoresPolicy" class="headerlink" title="FlushAllLargeStoresPolicy"></a>FlushAllLargeStoresPolicy</h6><p>在 HBase 2.0 之前版本是 <code>FlushLargeStoresPolicy</code>，后面被拆分成分 <code>FlushAllLargeStoresPolicy</code>和<code>FlushNonSloppyStoresFirstPolicy</code>，参见 <a href="https://www.iteblog.com/redirect.php?url=aHR0cHM6Ly9pc3N1ZXMuYXBhY2hlLm9yZy9qaXJhL2Jyb3dzZS9IQkFTRS0xNDkyMA==&article=true" target="_blank" rel="noopener">HBASE-14920</a>。</p>
<p>这种策略会先判断 Region 中每个 MemStore 的使用内存（ＯnHeap +　OffHeap）是否大于某个阀值，大于这个阀值的 MemStore 将会被刷写。阀值的计算是由 <code>hbase.hregion.percolumnfamilyflush.size.lower.bound</code>、<code>hbase.hregion.percolumnfamilyflush.size.lower.bound.min</code> 以及 <code>hbase.hregion.memstore.flush.size</code> 参数决定的。计算逻辑如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#96;&#x2F;&#x2F;region.getMemStoreFlushSize() &#x2F; familyNumber&#96;&#96;&#x2F;&#x2F;就是 hbase.hregion.memstore.flush.size 参数的值除以相关表列族的个数&#96;&#96;flushSizeLowerBound &#x3D; max(region.getMemStoreFlushSize() &#x2F; familyNumber, hbase.hregion.percolumnfamilyflush.size.lower.bound.min)&#96; &#96;&#x2F;&#x2F;如果设置了 hbase.hregion.percolumnfamilyflush.size.lower.bound&#96;&#96;flushSizeLowerBound &#x3D; hbase.hregion.percolumnfamilyflush.size.lower.bound&#96;</span><br></pre></td></tr></table></figure>

<p>计算逻辑上面已经很清晰的描述了。<code>hbase.hregion.percolumnfamilyflush.size.lower.bound.min</code> 默认值为 16MB，而 <code>hbase.hregion.percolumnfamilyflush.size.lower.bound</code> 没有设置。</p>
<p>比如当前表有3个列族，其他用默认的值，那么 <code>flushSizeLowerBound = max((long)128 / 3, 16) = 42</code>。</p>
<p>如果当前 Region 中没有 MemStore 的使用内存大于上面的阀值，<code>FlushAllLargeStoresPolicy</code> 策略就退化成 <code>FlushAllStoresPolicy</code> 策略了，也就是会对 Region 里面所有的 MemStore 进行 Flush。</p>
<h6 id="FlushNonSloppyStoresFirstPolicy"><a href="#FlushNonSloppyStoresFirstPolicy" class="headerlink" title="FlushNonSloppyStoresFirstPolicy"></a>FlushNonSloppyStoresFirstPolicy</h6><p>HBase 2.0 引入了 in-memory compaction，参见 <a href="https://www.iteblog.com/redirect.php?url=aHR0cHM6Ly9pc3N1ZXMuYXBhY2hlLm9yZy9qaXJhL2Jyb3dzZS9IQkFTRS0xMzQwOA==&article=true" target="_blank" rel="noopener">HBASE-13408</a>。如果我们对相关列族 <code>hbase.hregion.compacting.memstore.type</code> 参数的值不是 <code>NONE</code>，那么这个 MemStore 的 <code>isSloppyMemStore</code> 值就是 true，否则就是 false。</p>
<p><code>FlushNonSloppyStoresFirstPolicy</code> 策略将 Region 中的 MemStore 按照 <code>isSloppyMemStore</code> 分到两个 HashSet 里面（<code>sloppyStores</code> 和 <code>regularStores</code>）。然后</p>
<ul>
<li>判断 <code>regularStores</code> 里面是否有 MemStore 内存占用大于相关阀值的 MemStore ，有的话就会对这些 MemStore 进行刷写，其他的不做处理，这个阀值计算和 <code>FlushAllLargeStoresPolicy</code> 的阀值计算逻辑一致。</li>
<li>如果 <code>regularStores</code> 里面没有 MemStore 内存占用大于相关阀值的 MemStore，这时候就开始在 <code>sloppyStores</code> 里面寻找是否有 MemStore 内存占用大于相关阀值的 MemStore，有的话就会对这些 MemStore 进行刷写，其他的不做处理。</li>
<li>如果上面 <code>sloppyStores</code> 和 <code>regularStores</code> 都没有满足条件的 MemStore 需要刷写，这时候就 <code>FlushNonSloppyStoresFirstPolicy</code> 策略久退化成 <code>FlushAllStoresPolicy</code> 策略了。</li>
</ul>
<h4 id="Flush-过程"><a href="#Flush-过程" class="headerlink" title="Flush 过程"></a>Flush 过程</h4><p>MemStore 的刷写过程很复杂，很多操作都可能触发，但是这些条件触发的刷写最终都是调用 <code>HRegion</code> 类中的 <code>internalFlushcache</code> 方法。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">protected FlushResultImpl internalFlushcache(WAL wal, long myseqid,</span><br><span class="line">      Collection&lt;HStore&gt; storesToFlush, MonitoredTask status, boolean writeFlushWalMarker,</span><br><span class="line">      FlushLifeCycleTracker tracker) throws IOException &#123;</span><br><span class="line">    PrepareFlushResult result &#x3D;</span><br><span class="line">        internalPrepareFlushCache(wal, myseqid, storesToFlush, status, writeFlushWalMarker, tracker);</span><br><span class="line">    if (result.result &#x3D;&#x3D; null) &#123;</span><br><span class="line">      return internalFlushCacheAndCommit(wal, status, result, storesToFlush);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      return result.result; &#x2F;&#x2F; early exit due to failure from prepare stage</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从上面的实现可以看出，Flush 操作主要分以下几步做的</p>
<ul>
<li><strong>prepareFlush 阶段</strong>：刷写的第一步是对 MemStore 做 snapshot，为了防止刷写过程中更新的数据同时在 snapshot 和 MemStore 中而造成后续处理的困难，所以在刷写期间需要持有 updateLock 。持有了 updateLock 之后，这将阻塞客户端的写操作。所以只在创建 snapshot 期间持有 updateLock，而且 snapshot 的创建非常快，所以此锁期间对客户的影响一般非常小。对 MemStore 做 snapshot 是 <code>internalPrepareFlushCache</code> 里面进行的。</li>
<li><strong>flushCache 阶段</strong>：如果创建快照没问题，那么返回的 <code>result.result</code> 将为 null。这时候我们就可以进行下一步 <code>internalFlushCacheAndCommit</code>。其实 <code>internalFlushCacheAndCommit</code> 里面包含两个步骤：<code>flushCache</code> 和 <code>commit</code> 阶段。flushCache 阶段其实就是将 <code>prepareFlush</code> 阶段创建好的快照写到临时文件里面，临时文件是存放在对应 Region 文件夹下面的 <code>.tmp</code> 目录里面。</li>
<li><strong>commit 阶段</strong>：将 <code>flushCache</code> 阶段生产的临时文件移到（<code>rename</code>）对应的列族目录下面，并做一些清理工作，比如删除第一步生成的 snapshot。</li>
</ul>
]]></content>
      <categories>
        <category>Apache-Hbase</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Hbase</tag>
        <tag>flush</tag>
      </tags>
  </entry>
  <entry>
    <title>Hbase基础API</title>
    <url>/2020/05/01/Apache-Hbase/Hbase%EF%BC%8C%E4%BB%8E%E4%B8%8D%E7%9D%A1%E8%A7%89%E8%AF%B4%E8%B5%B7%E2%80%A6/</url>
    <content><![CDATA[<p>谨用作读《HBase不睡觉书》笔记</p>
<h3 id="CAP理论-–-gt-Base理论-–-gt-NoSql"><a href="#CAP理论-–-gt-Base理论-–-gt-NoSql" class="headerlink" title="CAP理论 –&gt; Base理论 –&gt; NoSql"></a>CAP理论 –&gt; Base理论 –&gt; NoSql</h3><h6 id="CAP-理论"><a href="#CAP-理论" class="headerlink" title="CAP 理论"></a>CAP 理论</h6><p>在一个分布式系统中， Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容忍性），三者不可得兼，这是是NOSQL数据库的基石。</p>
<ul>
<li><p>一致性（C）：数据一致更新，所有数据变动都是同步的     （等同于所有节点访问同一份最新的数据副本）</p>
</li>
<li><p>可用性（A）：良好的响应性能，即便部分节点出现故障      （对数据更新具备高可用性）</p>
</li>
<li><p>分区容忍性（P）：可靠性，即分区相当于通信的时限要求（系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况）</p>
</li>
</ul>
<p>​         <strong><em>任何分布式系统，只能同时满足其中两种；不需要考虑三者兼顾，更重要的是根据实际要求进行取舍</em></strong></p>
<p>因为当前网络硬件肯定会出现数据延迟和丢包问题，因而分区容忍性是必须实现的，关键就在于C和A之间的取舍和权衡，这就有了Base理论。</p>
<h6 id="Base-理论"><a href="#Base-理论" class="headerlink" title="Base 理论"></a>Base 理论</h6><p>是对CAP中C和A进行权衡后的结果，即基本可用（Basically Available）、软状态（Soft State）和最终一致性（Eventually consistent）。核心思想是，无法做到强一致性，根据业务特点，采用适当的方式来达到最终一致性。</p>
<p><strong>基本可用</strong>：指分布式系统出现不可预知的故障时，允许损失部分可用性，但并非是系统不可用，可能是出现响应延迟或者分流等</p>
<p><strong>软状态</strong>：允许系统中的数据存在中间状态（即允许不同节点之间的副本进行数据同步存在延时），并且该中间状态不会影响系统整体可用</p>
<p><strong>最终一致性</strong>：经过一段延时后，所有副本数据同步最终一致。</p>
<p><strong>因果一致性</strong>：因果一致性是指，如果进程A在更新完某个数据项后通知了进程B，那么进程B之后对该数据项的访问都应该能够获取到进程A更新后的最新值，并且如果进程B要对该数据项进行更新操作的话，务必基于进程A更新后的最新值，即不能发生丢失更新情况。与此同时，与进程A无因果关系的进程C的数据访问则没有这样的限制。</p>
<p><strong>读己之所写</strong>：读己之所写是指，进程A更新一个数据项之后，它自己总是能够访问到更新过的最新值，而不会看到旧值。也就是说，对于单个数据获取者而言，其读取到的数据一定不会比自己上次写入的值旧。因此，读己之所写也可以看作是一种特殊的因果一致性。</p>
<p><strong>会话一致性</strong>：会话一致性将对系统数据的访问过程框定在了一个会话当中：系统能保证在同一个有效的会话中实现“读己之所写”的一致性，也就是说，执行更新操作之后，客户端能够在同一个会话中始终读取到该数据项的最新值。</p>
<p><strong>单调读一致性</strong>：单调读一致性是指如果一个进程从系统中读取出一个数据项的某个值后，那么系统对于该进程后续的任何数据访问都不应该返回更旧的值。</p>
<p><strong>单调写一致性</strong>：单调写一致性是指，一个系统需要能够保证来自同一个进程的写操作被顺序地执行。</p>
<h6 id="Not-Only-SQL：Hbase"><a href="#Not-Only-SQL：Hbase" class="headerlink" title="Not Only SQL：Hbase"></a>Not Only SQL：Hbase</h6><p>Hbase是基于Hadoop存储的一个Key-Value列式数据库，因为key-value，Hbase响应会很快，即使数据再大；因为列式存储，字段再多，也能分散负载。同样这也导致了Hbase不可能很快，通常说的快是指，在大数据的环境下，相比较其他数据库系统，慢的不明显。</p>
<p><strong>适用场景</strong>：单表数据量超过千万，并且并发也高；数据分析需求弱，不需要那么灵活；</p>
<p><strong>不适用</strong>：报表分析，数据分析要求高（需要Join）；单表数据量不超过千万。</p>
<h3 id="LSM树和B-树"><a href="#LSM树和B-树" class="headerlink" title="LSM树和B+树"></a>LSM树和B+树</h3><p>B+树：传统的机械磁盘具有快速顺序读写、慢速随机读写的特性，这对磁盘的存储结构和算法影响较大。关系型数据库为了改善访问速度，通常会对数据进行排序后存储，加快数据检索速度，因此，这也就需要数据在不断地插入、更新、删除等操作后依然有序，导致在插入的时候有大量的随机I/O。</p>
<p>LSM树：Log-Structured Merge Tree。核心思想是<strong>将对数据的修改增量保持在内存中，达到指定的大小限制后将这些修改操作批量写入磁盘</strong>，这也导致了读取性能会下降，需要先看是否命中内存，否则需要检索各个文件。Hbase则是提供了一个blockCache，然后才是MemStore，最后Hfile（即StoreFile）。</p>
<p>Hbase存储主要设计思想：</p>
<p>SML树原理把一棵大树拆分成N棵小树，它首先写入内存中，随着小树越来越大，内存中的小树（MemStore）会flush到磁盘中，磁盘中的树（HFile）定期可以做merge操作（Compact），合并成一棵大树（HFile），以优化读性能。</p>
<ul>
<li>因为小树先写到内存中，为了防止内存数据丢失，写内存的同时需要暂时持久化到磁盘，对应了HBase的MemStore和HLog</li>
<li>MemStore上的树达到一定大小之后，需要flush到HRegion磁盘中（一般是Hadoop DataNode），这样MemStore就变成了DataNode上的磁盘文件StoreFile，定期HRegionServer对DataNode的数据做merge操作，彻底删除无效空间，多棵小树在这个时机合并成大树，来增强读性能。</li>
</ul>
<h3 id="两种-compaction"><a href="#两种-compaction" class="headerlink" title="两种 compaction"></a>两种 compaction</h3><ul>
<li>minor compaction：选取一些小的、相邻的StoreFile将他们合并成一个更大的StoreFile，在这个过程中不会处理已经Deleted或Expired的Cell。一次Minor Compaction的结果是更少并且更大的StoreFile。</li>
<li>major compaction：将所有的StoreFile合并成一个StoreFile，在这个过程中，标记为Deleted的Cell会被删除，而那些已经Expired的Cell会被丢弃，那些已经超过最多版本数的Cell会被丢弃。一次Major Compaction的结果是一个HStore只有一个StoreFile存在。Major Compaction可以手动或自动触发，然而由于它会引起很多的IO操作而引起性能问题，因而它一般会被安排在周末、凌晨等集群比较闲的时间。</li>
</ul>
<h3 id="Region-定位"><a href="#Region-定位" class="headerlink" title="Region 定位"></a>Region 定位</h3><h6 id="早期的三层查询：-ROOT-–-gt-META-–-gt-Region"><a href="#早期的三层查询：-ROOT-–-gt-META-–-gt-Region" class="headerlink" title="早期的三层查询：-ROOT-  –&gt; .META. –&gt; Region"></a>早期的三层查询：-ROOT-  –&gt; .META. –&gt; Region</h6><ol>
<li>用户通过查找ZK上的 /hbase/root-region-server节点来知道-ROOT-表在哪个regionserver上；</li>
<li>访问该regionserver上的 -ROOT-  表，看看需要的数据在哪个.META. ，.META.在哪个regionserver上；</li>
<li>访问 .META. 表，看需要查询的rowkey在哪个region上以及该region在哪个regionserver上；</li>
<li>访问目标regionserver，读取region中数据；</li>
<li>缓冲部分 .META. 表信息，以备下次查询。</li>
</ol>
<p>三层模型，极大地扩展了region的数量限制。但是，完全不需要那么多region，并且-ROOT-一直以来都是一行数据，形同虚设。同时增加了代码复杂度。</p>
<h6 id="现在的两层模型：-META-–-gt-Region"><a href="#现在的两层模型：-META-–-gt-Region" class="headerlink" title="现在的两层模型： .META. –&gt; Region"></a>现在的两层模型： .META. –&gt; Region</h6><ol>
<li>用户通过查找ZK上的 /hbase/root-region-server节点来知道 .META. 表在哪个regionserver上；</li>
<li>访问该regionserver上的  .META.  表，看看需要的数据在哪个region上以及该region在哪个regionserver上；</li>
<li>访问目标regionserver，读取region中数据；</li>
<li>缓冲 .META. 表信息，下次访问不需要再加载.META. 信息了。</li>
</ol>
]]></content>
      <categories>
        <category>Apache-Hbase</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>一次Hbase-RegionServer连续宕机事故</title>
    <url>/2020/05/05/Apache-Hbase/%E4%B8%80%E6%AC%A1HBaseRS%E8%BF%9E%E7%BB%AD%E5%AE%95%E6%9C%BA%E7%9A%84%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<h5 id="背景："><a href="#背景：" class="headerlink" title="背景："></a>背景：</h5><p>线上通过webui监控发现，几乎所有的 regionserver 都宕机了，而且时间上是连续的。通过查看所有regionserver上的日志发现，有过 gc 时间超长的(20多秒)，也有 zk 连接超时的，还有在 major compaction 时宕机的。到这就很为难了。</p>
<h5 id="措施："><a href="#措施：" class="headerlink" title="措施："></a>措施：</h5><p>查看HBase现在状态以及配置，看看配置是否合理</p>
<p>发现整个集群30台左右rs，每台机器分配了 20G 内存，写缓存比是 0.4，读缓存比是 0.4，region大小是10g，major compact 是 7天自动执行，最大WAL 数未配置，即默认为32（32是一个策略算出来的，未配置就通过默认的策略算）， 整个集群所有region数大概在10000 左右。</p>
<p>按照这种配置，每天集群写缓存只有8G。然后每台机器有大约300个region，活跃在用的region大概有270左右。也就是说每个region的 memstore 大小达到3M左右就需要flush，加上写入并发在十几万，推断是因为flush太过频繁，导致 minor compact 频繁。另外，region 大小只有10G，很容易达到分裂要求，也是对集群稳定性的一大考验。</p>
<h5 id="初步解决方案：修改配置，重启集群。"><a href="#初步解决方案：修改配置，重启集群。" class="headerlink" title="初步解决方案：修改配置，重启集群。"></a>初步解决方案：修改配置，重启集群。</h5><p>​        读写缓存比调整，因为写需求大于读需求，因此读写分别调整为 0.3 和 0.5（两者之和不能超过0.8）；</p>
<p>​        region 大小调整，10G 调整为 100G，降低 region 分裂频率；</p>
<p>​        禁止自动 major compact，设置为0即禁止。major 操作会使RS几乎不可用，禁止自动，需要周期手动</p>
<p>​        修改 maxLogs 大小，因为 region 数太多，WAL数量修改为64或128 （这次好像未改动）</p>
<p>修改完后进行重启。</p>
<p>该配置只是将未来的做了打算，比如 region不会再分裂，不会再自动 major ，等等，但是现有的问题还是存在，region数太多，导致 memStore 经常flush，RS压力还是很大。故还需要先进行region 合并操作，先将region数量减少一般再说。</p>
<p>合并操作代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> hbase;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.HBaseConfiguration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.HRegionInfo;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.TableName;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Collections;</span><br><span class="line"><span class="keyword">import</span> java.util.Comparator;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MergeRegions</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line"></span><br><span class="line">        Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">        configuration.set(<span class="string">"hbase.zookeeper.property.clientPort"</span>, <span class="string">"2181"</span>);</span><br><span class="line">        configuration.set(<span class="string">"hbase.rootdir"</span>, <span class="string">"hdfs://hadoopha:8020/apps/hbase/data"</span>);</span><br><span class="line">        configuration.set(<span class="string">"zookeeper.znode.parent"</span>, <span class="string">"/hbase-unsecure"</span>);</span><br><span class="line">        configuration.set(<span class="string">"hbase.zookeeper.quorum"</span>,<span class="string">"node31,node32,node33"</span>);</span><br><span class="line"></span><br><span class="line">        Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">        Admin admin = connection.getAdmin();</span><br><span class="line"></span><br><span class="line">        List&lt;HRegionInfo&gt; regions = admin.getTableRegions(TableName.valueOf(<span class="string">"oracledb.bdp_hbase_kvbehavior_jydz"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 因为只有相邻 rowKey 的region 才能进行合并，所以需要先根据 每个region的startKey进行排序</span></span><br><span class="line">        Collections.sort(regions, <span class="keyword">new</span> Comparator&lt;HRegionInfo&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(HRegionInfo o1, HRegionInfo o2)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> Bytes.compareTo(o1.getStartKey(),o1.getStartKey());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        HRegionInfo hRegionInfo = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">for</span> (HRegionInfo r:regions)&#123;</span><br><span class="line">            <span class="keyword">int</span> i = regions.indexOf(r);</span><br><span class="line">            <span class="comment">// 每次合并两个相邻的 region</span></span><br><span class="line">            <span class="keyword">if</span> (i % <span class="number">2</span> == <span class="number">0</span>)&#123;</span><br><span class="line">                hRegionInfo = r;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">                System.out.println(<span class="string">"start merge 2 regions... "</span>+i+ <span class="string">": "</span>+hRegionInfo.getEncodedName()+<span class="string">" "</span> +Bytes.toString(hRegionInfo.getStartKey())+<span class="string">"    and    "</span>+ (i+<span class="number">1</span>) +<span class="string">": "</span>+ r.getEncodedName()+<span class="string">" "</span>+Bytes.toString(r.getStartKey()));</span><br><span class="line">                admin.mergeRegions(hRegionInfo.getEncodedNameAsBytes(),r.getEncodedNameAsBytes(),<span class="keyword">false</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            System.out.println(<span class="string">"all regions merge success!"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>合并后检查 region数，降低了一半。</p>
<p>然后重启写入任务，发现了另外一个问题，有一个region A 处于分裂状态，但是有数据需要写到那个region，然后异常了，一直写不进去，异常信息大概是定位到了写入的region，但是region正在分裂，需要等待…，谁知道等待了一天也不行，后面仔细想了想，正常的话，region的split应该是一个一两秒的过程，不至于会出现这个问题，那么这个region肯定不正常。后面去 hbase:meta表查看了一下该region的元数据信息，反正确实一直处于split 状态，并且两个子region的信息也有。然后分别去HDFS上查看父region和两个子region的数据信息，发现其中一个子region目录下还有父region的数据链接文件，另外一个region目录信息正常，但是元数据查不到该子region的信息，万万没想到这个坑了我一下（下线的region的元数据信息不会在meta表）。所以第一个神操作来了，把无元数据信息的region的目录删除。我以为该region已经没用了，所以执行 hbase hbck能检查出问题，没想到的是，问题出来了，不是父region的问题，是这个子region已经分裂，但是它分裂的子region还有数据链接文件(即数据未迁移)，然后就报错了。接着赶紧把 子region 恢复，然后基本判断了。是另外一个子region子出了问题，但是其下又有新的数据文件，所以将 父region下的数据文件全部手动迁移至该region下，并且将对应的数据链接文件进行删除。然后再执行 hbck ，问题解决，写入异常也没了。</p>
<p>接着再看 写入不稳定问题，几乎每个十来分钟，消费写入程序都会进行 rebalance，苦不堪言。然后大概评估了一下原因，还是region太多，memstore不够用。但是再次合并发现，不能再合并了，想了想，合并和分裂应该是相反的操作，也不会立马迁移数据，需要时间。在这期间应该时不能再合并了（好像执行了major 也不管用，也不会立马迁移数据）。那么只能从kafka consumer端解决了。看代码发现，max.poll.interval.ms 这个参数仍然是默认的 300 s。看来就是这个了，因为写HBase会经常卡住一会，然后就把 这个值设置为 1小时了。通常情况下不建议设置这么大，都需要参考处理每批 poll 数据的时间来进行设置，稍长一点即可。但是因为现在经常抖动，也不知道具体的处理时间，因此直接设置为1小时。后果就是再次发生rebanlance时比较难办，因为rebanlance的超时时间其实好像也是这个值，所以，这个值设置千万要慎重。</p>
<p>当改为这一切之后，终于算是正常了。差不多没隔十来分钟左右会停止写入数据，然后程序等待，等待2-5分钟，恢复写入。最起码不会因为rebanlance而导致程序一直 rebanlance 了。</p>
]]></content>
      <categories>
        <category>Apache-Hbase</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>Hbase表设计和高级属性</title>
    <url>/2020/04/11/Apache-Hbase/%E8%A1%A8%E8%AE%BE%E8%AE%A1%E5%92%8C%E9%AB%98%E7%BA%A7%E5%B1%9E%E6%80%A7/</url>
    <content><![CDATA[<h5 id="1、compression"><a href="#1、compression" class="headerlink" title="1、compression"></a>1、compression</h5><p>默认值是 NONE 即不使用压缩， 这个参数意思是该列族是否采用压缩，采用什么压缩算 法</p>
<p>方法: <code>create &#39;table&#39;,{NAME=&gt;&#39;info&#39;,COMPRESSION=&gt;&#39;SNAPPY&#39;}</code>  </p>
<p>建议采用 SNAPPY 压缩算法 ， HBase 中，在 Snappy 发布之前（ Google 2011 年对外发布 Snappy），采用的 LZO 算法，目标是达到尽可能快的压缩和解压速度，同时减少对 CPU 的消耗；</p>
<p>HBase修改压缩格式，需要一个列族一个列族的修改    alter ‘test’, NAME =&gt; ‘f’, COMPRESSION =&gt; ‘snappy’。</p>
<p>而且这个地方要小心，别将列族名字写错，或者大小写错误。因为这个地方任何错误，都会创建一个新的列族，且压缩格式为snappy（修改之前需要先disable，修改完之后需要enable，然后 major_compact  ‘test’）</p>
<h5 id="2、TTL-time-to-live"><a href="#2、TTL-time-to-live" class="headerlink" title="2、TTL  (time to live)"></a>2、TTL  (time to live)</h5><p>设置方法和versions类似</p>
<h5 id="3、disable-all-enable-all-drop-all"><a href="#3、disable-all-enable-all-drop-all" class="headerlink" title="3、disable_all   enable_all    drop_all"></a>3、disable_all   enable_all    drop_all</h5><p>​    支持正则表达式，并列出当前匹配的表，之后给出确认提示。</p>
<h5 id="4、Hbase-预分区"><a href="#4、Hbase-预分区" class="headerlink" title="4、Hbase 预分区"></a>4、Hbase 预分区</h5><p>HBase表在刚刚被创建时，只有1个分区（region），当一个region过大（达到hbase.hregion.max.filesize属性中定义的阈值，默认10GB）时，表将会进行split，分裂为2个分区。表在进行split的时候，会耗费大量的资源，频繁的分区对HBase的性能有巨大的影响。HBase提供了预分区功能，即用户可以在创建表的时候对表按照一定的规则分区。分区是针对表级，不是列族级，因为region是根据rowkey来划分的。</p>
<p><strong>目的：减少由于region split带来的资源消耗。从而提高HBase的性能。</strong></p>
<p>方案1：Hbase shell 创建，16010端口可以查看具体region</p>
<p><img src="F:%5CmyGit%5CDT-Learner%5CHbase%5Cimage-1565608243459.png" alt="img"></p>
<p><img src="F:%5CmyGit%5CDT-Learner%5CHbase%5Cimage.png" alt="img"></p>
<p>方案2：Hbase shell ，通过文件来创建</p>
<p><img src="F:%5CmyGit%5CDT-Learner%5CHbase%5Cimage-1565608208250.png" alt="img"></p>
<p><img src="F:%5CmyGit%5CDT-Learner%5CHbase%5Cimage-1565608229068.png" alt="img"></p>
<h5 id="5、表的设计"><a href="#5、表的设计" class="headerlink" title="5、表的设计"></a>5、表的设计</h5><h6 id="列簇设计："><a href="#列簇设计：" class="headerlink" title="列簇设计："></a>列簇设计：</h6><p>原则：在合理范围内能尽量少的减少列簇就尽量减少列簇，因为列簇是共享region的，每个列簇数据相差太大导致查询效率低下</p>
<p>最优：将所有相关性很强的 key-value 都放在同一个列簇下，这样既能做到查询效率 最高，也能保持尽可能少的访问不同的磁盘文件。以用户信息为例，可以将必须的基本信息存放在一个列族，而一些附加的额外信息可以放在 另一列族 </p>
<h6 id="rowkey设计："><a href="#rowkey设计：" class="headerlink" title="rowkey设计："></a>rowkey设计：</h6><p>长度原则：100字节以内，8的倍数最好，可能的情况下越短越好。因为HFile是按照keyvalue存储的，过长的rowkey会影响存储效率；其次，过长的rowkey在memstore中较大，影响缓冲效果，降低检索效率。最后，操作系统大多为64位，8的倍数，充分利用操作系统的最佳性能。</p>
<p>散列原则：高位散列，低位时间字段。避免热点问题</p>
<p>唯一原则：分利用这个排序的特点，将经常读取的数据存储到一块，将最近可能会被访问 的数据放到一块。</p>
<h5 id="6、数据热点问题"><a href="#6、数据热点问题" class="headerlink" title="6、数据热点问题"></a>6、数据热点问题</h5><h6 id="措施一：加盐"><a href="#措施一：加盐" class="headerlink" title="措施一：加盐"></a>措施一：加盐</h6><p>随机前缀，高位散列，将数据随机分散到region上</p>
<h6 id="措施二：哈希"><a href="#措施二：哈希" class="headerlink" title="措施二：哈希"></a>措施二：哈希</h6><p>哈希前缀，负载均衡，并且可以利用哈希重构rowkey，使用get获取准确数据</p>
<h6 id="措施三：反转"><a href="#措施三：反转" class="headerlink" title="措施三：反转"></a>措施三：反转</h6><p>反转固定长度或者数字格式的rowkey，牺牲了有序性，避免类似手机号固定开头的热点问题</p>
<h6 id="措施四：时间戳反转"><a href="#措施四：时间戳反转" class="headerlink" title="措施四：时间戳反转"></a>措施四：时间戳反转</h6>]]></content>
      <categories>
        <category>Apache-Hbase</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop小文件合并</title>
    <url>/2020/04/11/Apache-Hadoop/HDFS%E5%B0%8F%E6%96%87%E4%BB%B6%E5%90%88%E5%B9%B6/</url>
    <content><![CDATA[<h4 id="HDFS上的小文件问题产生原因及后果"><a href="#HDFS上的小文件问题产生原因及后果" class="headerlink" title="HDFS上的小文件问题产生原因及后果"></a>HDFS上的小文件问题产生原因及后果</h4><h5 id="产生原因："><a href="#产生原因：" class="headerlink" title="产生原因："></a>产生原因：</h5><h5 id="后果："><a href="#后果：" class="headerlink" title="后果："></a>后果：</h5><h4 id="几种文件合并的代码记录"><a href="#几种文件合并的代码记录" class="headerlink" title="几种文件合并的代码记录"></a>几种文件合并的代码记录</h4><h5 id="抽象类："><a href="#抽象类：" class="headerlink" title="抽象类："></a>抽象类：</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileStatus;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.Logger;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.text.SimpleDateFormat;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">CombineSmallFile</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger log = Logger.getLogger(CombineSmallFile<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> String     tablePathStr;</span><br><span class="line">    <span class="keyword">public</span> String     beginPartDate;</span><br><span class="line">    <span class="keyword">public</span> String     endPartDate;</span><br><span class="line">    <span class="keyword">public</span> FileSystem fs;</span><br><span class="line">    <span class="keyword">public</span> String     filePrefix;</span><br><span class="line">    <span class="keyword">public</span> SimpleDateFormat sdf = <span class="keyword">new</span> SimpleDateFormat(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">CombineSmallFile</span><span class="params">()</span></span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取需要合并的分区列表</span></span><br><span class="line">        List&lt;Path&gt; needCombinePartitions = getNeedCombinePartition(tablePathStr);</span><br><span class="line">        log.info(<span class="string">"got need combine partition!"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 合并分区</span></span><br><span class="line">        <span class="keyword">for</span> (Path partition:needCombinePartitions) &#123;</span><br><span class="line">            log.info(<span class="string">"combine partition："</span> + partition.getName());</span><br><span class="line">            ArrayList&lt;Path&gt; fileList;</span><br><span class="line">            <span class="keyword">boolean</span> isGoOn = <span class="keyword">true</span>;</span><br><span class="line">            <span class="keyword">while</span> (isGoOn)&#123;</span><br><span class="line">                fileList = getFileList(partition,filePrefix);</span><br><span class="line">                <span class="keyword">if</span> (fileList.size()&lt;<span class="number">100</span>)&#123;</span><br><span class="line">                    isGoOn = <span class="keyword">false</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span> (fileList.size() &gt; <span class="number">1</span>) &#123;</span><br><span class="line">                    combineFiles(partition, fileList);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    log.info(<span class="string">"small files is less than 1"</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">combineFiles</span><span class="params">(Path partition, ArrayList&lt;Path&gt; fileList)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> List&lt;Path&gt; <span class="title">getNeedCombinePartition</span><span class="params">(String tablePathStr)</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line"></span><br><span class="line">        Path tablePath = <span class="keyword">new</span> Path(HdfsUtil.getBaseURI()+ tablePathStr);</span><br><span class="line">        FileStatus[] partitionFileStatus = fs.listStatus(tablePath);</span><br><span class="line">        ArrayList&lt;Path&gt; partitionPathList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (FileStatus fileStatus:partitionFileStatus)&#123;</span><br><span class="line">            Path partitionPath = fileStatus.getPath();</span><br><span class="line">            String partitionName = partitionPath.getName();</span><br><span class="line">            <span class="keyword">if</span> (fileStatus.isDirectory() &amp;&amp; partitionName.compareTo(beginPartDate)&gt;=<span class="number">0</span>  &amp;&amp; partitionName.compareTo(endPartDate)&lt;=<span class="number">0</span> )&#123;</span><br><span class="line">                partitionPathList.add(partitionPath);</span><br><span class="line">                log.info(partitionName);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> partitionPathList;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> ArrayList&lt;Path&gt; <span class="title">getFileList</span><span class="params">(Path partitionPath,String filePrefix)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        ArrayList&lt;Path&gt; pathArrayList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="comment">// 分区下的所有文件</span></span><br><span class="line">        FileStatus[] fileStatuses = fs.listStatus(partitionPath);</span><br><span class="line">        <span class="keyword">for</span> (FileStatus fileStatus : fileStatuses) &#123;</span><br><span class="line">            Path filePath = fileStatus.getPath();</span><br><span class="line">            String filePathName = filePath.getName();</span><br><span class="line">            <span class="keyword">long</span> accessTime = fileStatus.getModificationTime();</span><br><span class="line">            <span class="keyword">long</span> expiredTime = System.currentTimeMillis() - accessTime;</span><br><span class="line">            <span class="keyword">if</span> (filePathName.startsWith(<span class="string">".complete"</span>) &amp;&amp; expiredTime &gt;=<span class="number">12</span>*<span class="number">3600</span>*<span class="number">1000</span>)&#123;</span><br><span class="line">                log.info(<span class="string">"find a expire .complete file! delete it!\t\t"</span>+filePathName + <span class="string">"\t"</span> + sdf.format(accessTime));</span><br><span class="line">                fs.delete(filePath,<span class="keyword">true</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (filePathName.equals(<span class="string">"..combining"</span>) &amp;&amp; expiredTime &lt;=<span class="number">3</span>*<span class="number">3600</span>*<span class="number">1000</span>)&#123;</span><br><span class="line">                log.info(<span class="string">"this partition is combining, skip it."</span>);</span><br><span class="line">                pathArrayList.clear();</span><br><span class="line">                <span class="keyword">return</span> pathArrayList;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 小于100M 的 complete 开头的文件 || 小于50M的combine开头文件</span></span><br><span class="line">            <span class="keyword">if</span> ((filePathName.startsWith(filePrefix) &amp;&amp; fileStatus.getLen()&lt;=<span class="number">100</span>*<span class="number">1024</span>*<span class="number">1024</span>) || (filePathName.startsWith(<span class="string">"combined"</span>) &amp;&amp; fileStatus.getLen()&lt;=<span class="number">50</span>*<span class="number">1024</span>*<span class="number">1024</span>)) &#123;</span><br><span class="line">                pathArrayList.add(filePath);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 每次合并不多于100个文件</span></span><br><span class="line">            <span class="keyword">if</span> (pathArrayList.size()&gt;=<span class="number">100</span>)&#123;</span><br><span class="line">                <span class="keyword">return</span> pathArrayList;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> pathArrayList;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="合并-parquet-文件"><a href="#合并-parquet-文件" class="headerlink" title="合并 parquet 文件"></a>合并 parquet 文件</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.apache.parquet.column.ParquetProperties;</span><br><span class="line"><span class="keyword">import</span> org.apache.parquet.example.data.Group;</span><br><span class="line"><span class="keyword">import</span> org.apache.parquet.hadoop.ParquetFileReader;</span><br><span class="line"><span class="keyword">import</span> org.apache.parquet.hadoop.ParquetFileWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.parquet.hadoop.ParquetReader;</span><br><span class="line"><span class="keyword">import</span> org.apache.parquet.hadoop.ParquetWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.parquet.hadoop.example.ExampleParquetWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.parquet.hadoop.example.GroupReadSupport;</span><br><span class="line"><span class="keyword">import</span> org.apache.parquet.hadoop.metadata.CompressionCodecName;</span><br><span class="line"><span class="keyword">import</span> org.apache.parquet.hadoop.metadata.ParquetMetadata;</span><br><span class="line"><span class="keyword">import</span> org.apache.parquet.schema.MessageType;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CombineParquetSmallFile</span> <span class="keyword">extends</span> <span class="title">CombineSmallFile</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger log = Logger.getLogger(CombineParquetSmallFile<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tablePathStr String for table path . eg: /apps/hive/warehouse/oracledb.db/t_ext</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> beginPartDate begin partDate . eg: partdate=date20200912</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> endPartDate end partDate . eg: partdate=date20200921</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">CombineParquetSmallFile</span><span class="params">(String tablePathStr, String beginPartDate, String endPartDate, String filePrefix)</span></span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.tablePathStr = tablePathStr;</span><br><span class="line">        <span class="keyword">this</span>.beginPartDate = beginPartDate;</span><br><span class="line">        <span class="keyword">this</span>.endPartDate = endPartDate;</span><br><span class="line">        <span class="keyword">this</span>.fs  = HdfsUtil.getFileSystem();</span><br><span class="line">        <span class="keyword">this</span>.filePrefix = filePrefix;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">combineFiles</span><span class="params">(Path partition, ArrayList&lt;Path&gt; fileList)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="comment">// 获取第一个complete文件的 MessageType</span></span><br><span class="line">        MessageType messageType = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">for</span> (Path file : fileList)&#123;</span><br><span class="line">            <span class="keyword">if</span> (file.getName().startsWith(<span class="string">"complete"</span>))&#123;</span><br><span class="line">                ParquetMetadata parquetMetadata = ParquetFileReader.readFooter(HdfsUtil.getConf(), fileList.get(<span class="number">0</span>));</span><br><span class="line">                messageType = parquetMetadata.getFileMetaData().getSchema();</span><br><span class="line">                log.info(<span class="string">"get MessageType: "</span> + file.getName());</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (messageType == <span class="keyword">null</span>)&#123;</span><br><span class="line">            log.warn(<span class="string">"can't get MessageType"</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 新文件</span></span><br><span class="line">        Path combiningDir = <span class="keyword">new</span> Path(partition.toString() + <span class="string">"/..combining"</span>);</span><br><span class="line">        <span class="keyword">if</span> (fs.exists(combiningDir))&#123;</span><br><span class="line">            log.info(<span class="string">"combining history directory is exists! delete it!+\t"</span>+sdf.format(fs.getFileStatus(combiningDir).getAccessTime()));</span><br><span class="line">            fs.delete(combiningDir, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        fs.mkdirs(combiningDir);</span><br><span class="line">        String combineFileName = <span class="string">"/combined-"</span> + System.currentTimeMillis() + <span class="string">".parquet"</span>;</span><br><span class="line">        <span class="comment">// partition/..combining/combine-1111.parquet</span></span><br><span class="line">        Path combineFilePath = <span class="keyword">new</span> Path(combiningDir.toString() + combineFileName);</span><br><span class="line">        ParquetWriter&lt;Group&gt; writer = newParquetWriter(combineFilePath, messageType);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Path file : fileList) &#123;</span><br><span class="line">            log.info(<span class="string">"combining file: "</span>+file.getName());</span><br><span class="line">            GroupReadSupport readSupport = <span class="keyword">new</span> GroupReadSupport();</span><br><span class="line">            ParquetReader reader = ParquetReader.builder(readSupport, file).build();</span><br><span class="line">            Group group;</span><br><span class="line">            <span class="keyword">while</span> ((group = (Group) reader.read()) != <span class="keyword">null</span>) &#123;</span><br><span class="line">                writer.write(group);</span><br><span class="line">            &#125;</span><br><span class="line">            reader.close();</span><br><span class="line">        &#125;</span><br><span class="line">        writer.close();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// partition/..combining/combine-1111.parquet  to partition/combine-1111.parquet</span></span><br><span class="line">            fs.rename(combineFilePath, <span class="keyword">new</span> Path(partition.toString()+ combineFileName));</span><br><span class="line">            log.info(<span class="string">"partition: "</span> + partition.toString() + <span class="string">" combine completed ! begin to delete complete file.."</span>);</span><br><span class="line">            <span class="keyword">for</span> (Path path : fileList) &#123;</span><br><span class="line">                fs.delete(path, <span class="keyword">true</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            fs.delete(combiningDir,<span class="keyword">true</span>);</span><br><span class="line">            log.info(<span class="string">"delete small file completed!"</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            log.error(<span class="string">""</span>, e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> ParquetWriter&lt;Group&gt; <span class="title">newParquetWriter</span><span class="params">(Path path, MessageType initSchema)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        ParquetWriter&lt;Group&gt; parquetWriter = ExampleParquetWriter</span><br><span class="line">                .builder(path)</span><br><span class="line">                .withWriteMode(ParquetFileWriter.Mode.CREATE)</span><br><span class="line">                .withCompressionCodec(CompressionCodecName.SNAPPY)</span><br><span class="line">                .withWriterVersion(ParquetProperties.WriterVersion.PARQUET_1_0)</span><br><span class="line">                .withConf(HdfsUtil.getConf())</span><br><span class="line">                .withPageSize(<span class="number">1024</span> * <span class="number">8</span>)</span><br><span class="line">                .withType(initSchema)</span><br><span class="line">                .build();</span><br><span class="line">        <span class="keyword">return</span> parquetWriter;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="合并-textfile-小文件"><a href="#合并-textfile-小文件" class="headerlink" title="合并 textfile 小文件"></a>合并 textfile 小文件</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.Logger;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CombineTextSmallFile</span> <span class="keyword">extends</span> <span class="title">CombineSmallFile</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger log = Logger.getLogger(CombineTextSmallFile<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">CombineTextSmallFile</span><span class="params">(String tablePathStr, String beginPartDate, String endPartDate, String filePrefix)</span></span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.tablePathStr = tablePathStr;</span><br><span class="line">        <span class="keyword">this</span>.beginPartDate = beginPartDate;</span><br><span class="line">        <span class="keyword">this</span>.endPartDate = endPartDate;</span><br><span class="line">        <span class="keyword">this</span>.fs  = HdfsUtil.getFileSystem();</span><br><span class="line">        <span class="keyword">this</span>.filePrefix = filePrefix;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">combineFiles</span><span class="params">(Path partition, ArrayList&lt;Path&gt; fileList)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="comment">// 新文件</span></span><br><span class="line">        Path combiningDir = <span class="keyword">new</span> Path(partition.toString() + <span class="string">"/..combining"</span>);</span><br><span class="line">        <span class="keyword">if</span> (fs.exists(combiningDir))&#123;</span><br><span class="line">            log.info(<span class="string">"combining history directory is exists! delete it!\t"</span>+sdf.format(fs.getFileStatus(combiningDir).getModificationTime()));</span><br><span class="line">            fs.delete(combiningDir, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        fs.mkdirs(combiningDir);</span><br><span class="line">        String combineFileName = <span class="string">"/combined-"</span> + System.currentTimeMillis() + <span class="string">".txt"</span>;</span><br><span class="line">        <span class="comment">// partition/..combining/combine-1111.parquet</span></span><br><span class="line">        Path combineFilePath = <span class="keyword">new</span> Path(combiningDir.toString() + combineFileName);</span><br><span class="line"></span><br><span class="line">        FSDataOutputStream fsDataOutputStream = fs.create(combineFilePath);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Path file : fileList) &#123;</span><br><span class="line">            log.info(<span class="string">"combining file: "</span>+file.getName());</span><br><span class="line">            FSDataInputStream fsDataInputStream = fs.open(file);</span><br><span class="line">            IOUtils.copyBytes(fsDataInputStream,fsDataOutputStream,<span class="number">4096</span>,<span class="keyword">false</span>);</span><br><span class="line">            fsDataInputStream.close();</span><br><span class="line">        &#125;</span><br><span class="line">        fsDataOutputStream.close();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// partition/..combining/combine-1111.parquet  to partition/combine-1111.parquet</span></span><br><span class="line">            fs.rename(combineFilePath, <span class="keyword">new</span> Path(partition.toString()+ combineFileName));</span><br><span class="line">            log.info(<span class="string">"partition: "</span> + partition.toString() + <span class="string">" combine completed ! begin to delete complete file.."</span>);</span><br><span class="line">            <span class="keyword">for</span> (Path path : fileList) &#123;</span><br><span class="line">                fs.delete(path, <span class="keyword">true</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            fs.delete(combiningDir,<span class="keyword">true</span>);</span><br><span class="line">            log.info(<span class="string">"delete small file completed!"</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            log.error(<span class="string">""</span>, e);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="合并-ORC-文件"><a href="#合并-ORC-文件" class="headerlink" title="合并 ORC 文件"></a>合并 ORC 文件</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">待定</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Apache-Hadoop</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Hadoop</tag>
        <tag>HDFS</tag>
        <tag>parquet</tag>
        <tag>ORC</tag>
        <tag>textfile</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop平常问题记录</title>
    <url>/2020/03/11/Apache-Hadoop/Hadoop%E5%B9%B3%E5%B8%B8%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<h4 id="50070端口占用"><a href="#50070端口占用" class="headerlink" title="50070端口占用"></a><strong>50070端口占用</strong></h4><p>启动nameNode失败，查看启动日志</p>
<p>/var/log/hadoop/hdfs/hadoop-hdfs-namenode-tmaster.log</p>
<p>显示50070端口被占用</p>
<p>查看端口占用情况：netstat -alnp | grep 50070  被 MySQL占用</p>
<p>查看MySQL端口情况</p>
<p>发现大量 端口被MySQL占用，状态全部为 TIME WAIT</p>
<p>因为此MySQL只用来HDP集群，用到的只有hive和ambari。但是hive已经挂了，怀疑是ambari问题。查看ambari日志，发现是有个SQL没有关闭MySQL连接</p>
<p>21 Feb 2017 19:15:29,519  WARN [com.mchange.v2.async.ThreadPoolAsynchronousRunner$PoolThread-#0] StatementUtils:48 - Statement close FAILED.</p>
<p>com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ‘OPTION SQL_SELECT_LIMIT=DEFAULT’ at line 1</p>
<p>原因：Ambari默认的 mysql jdbc 驱动不支持 5.6以上版本</p>
<p>解决方法：下载新的 mysql jdbc驱动</p>
<p>ln -s mysql-connector-java-5.1.40-bin.jar mysql-connector-java.jar</p>
<p>ambari-server setup –jdbc-db=mysql –jdbc-driver=/usr/share/java/mysql-connector-java.jar</p>
<h4 id="提交jar包运行，报错"><a href="#提交jar包运行，报错" class="headerlink" title="提交jar包运行，报错"></a>提交jar包运行，报错</h4><p>java.io.IOException: No FileSystem for scheme: hdfs</p>
<p>​    at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2644)</p>
<p>​    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2651)</p>
<p>​    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:92)</p>
<p>​    at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2687)</p>
<p>​    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2669)</p>
<p>​    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:371)</p>
<p>​    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)</p>
<p>​    at kafka.MyUtils.processFile(MyUtils.java:34)</p>
<p>​    at kafka.KafkaToParquetAutoOffset.main(KafkaToParquetAutoOffset.java:132)</p>
<p>conf配置有问题：core-site添加 或者代码添加</p>
<table>
<thead>
<tr>
<th>fs.file.impl</th>
<th>org.apache.hadoop.fs.LocalFileSystem</th>
</tr>
</thead>
<tbody><tr>
<td>fs.hdfs.impl</td>
<td>org.apache.hadoop.hdfs.DistributedFileSystem</td>
</tr>
</tbody></table>
<p>Configuration conf = new Configuration();</p>
<p>conf.set(“fs.hdfs.impl”, org.apache.hadoop.hdfs.DistributedFileSystem.class.getName());</p>
<p>conf.set(“fs.file.impl”, org.apache.hadoop.fs.LocalFileSystem.class.getName());</p>
]]></content>
      <categories>
        <category>Apache-Hadoop</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Hadoop</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop块丢失问题记录</title>
    <url>/2020/04/11/Apache-Hadoop/hdfs%E5%9D%97%E4%B8%A2%E5%A4%B1%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<h3 id="HDFS块丢失"><a href="#HDFS块丢失" class="headerlink" title="HDFS块丢失"></a>HDFS块丢失</h3><p>   <img src="F:%5CmyGit%5CDT-Learner%5CApache-Hadoop%5C1565310720312.png" alt="1565310720312"></p>
<h6 id="后台检查详细问题"><a href="#后台检查详细问题" class="headerlink" title="后台检查详细问题"></a>后台检查详细问题</h6><p><code>hdfs fsck -list-corruptfileblocks</code>                                       // 列出所有有问题的块</p>
<p><code>hdfs fsck /apps/hive/**  -list-coruptfileblocks</code>          // 列出 /apps/hive/** 中有问题的块，可以是目录也可以是文件</p>
<p><code>hdfs fsck / -delete</code>                                                                    // 删除根目录下所有有问题块的<strong>文件</strong>，path可更改 </p>
<p><code>hdfs fsck path  -files -blocks -locations</code>                      // 查看 path 文件的块的信息，位置，还可以机架(-racks)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master tmp]# hdfs fsck &#x2F;apps&#x2F;hive&#x2F;warehouse&#x2F;yibuwatch.db&#x2F;t_location&#x2F;partdate&#x3D;date20190808&#x2F;part-m-00000-MYSQL_SERVICE_DATA-mgw_84_v4_191.1565279433 -list-corruptfileblocks</span><br><span class="line">Connecting to namenode via http:&#x2F;&#x2F;master:50070&#x2F;fsck?ugi&#x3D;root&amp;listcorruptfileblocks&#x3D;1&amp;path&#x3D;%2Fapps%2Fhive%2Fwarehouse%2Fyibuwatch.db%2Ft_location%2Fpartdate%3Ddate20190808%2Fpart-m-00000-MYSQL_SERVICE_DATA-mgw_84_v4_191.1565279433</span><br><span class="line">The list of corrupt files under path &#39;&#x2F;apps&#x2F;hive&#x2F;warehouse&#x2F;yibuwatch.db&#x2F;t_location&#x2F;partdate&#x3D;date20190808&#x2F;part-m-00000-MYSQL_SERVICE_DATA-mgw_84_v4_191.1565279433&#39; are:</span><br><span class="line">blk_1536251272	&#x2F;apps&#x2F;hive&#x2F;warehouse&#x2F;yibuwatch.db&#x2F;t_location&#x2F;partdate&#x3D;date20190808&#x2F;part-m-00000-MYSQL_SERVICE_DATA-mgw_84_v4_191.1565279433</span><br><span class="line">The filesystem under path &#39;&#x2F;apps&#x2F;hive&#x2F;warehouse&#x2F;yibuwatch.db&#x2F;t_location&#x2F;partdate&#x3D;date20190808&#x2F;part-m-00000-MYSQL_SERVICE_DATA-mgw_84_v4_191.1565279433&#39; has 1 CORRUPT files</span><br></pre></td></tr></table></figure>

<p>可以看出，确实这个文件有问题。那么看一下丢失的块是哪个</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hdfs fsck &#x2F;apps&#x2F;hive&#x2F;warehouse&#x2F;yibuwatch.db&#x2F;t_location&#x2F;partdate&#x3D;date20190808&#x2F;part-m-00000-MYSQL_SERVICE_DATA-mgw_84_v4_191.1565279433 -files -blocks -locations</span><br><span class="line">...</span><br><span class="line">36. BP-1440124469-192.168.1.1-1466135627881:blk_1536233077_4849844907 len&#x3D;134217728 repl&#x3D;3 [DatanodeInfoWithStorage[172.28.199.50:50010,DS-b05716dd-2233-469c-b05d-e20faf9d9bb0,DISK], DatanodeInfoWithStorage[172.28.199.47:50010,DS-83333601-39b3-4898-a340-532ad12322ce,DISK], DatanodeInfoWithStorage[172.28.199.53:50010,DS-7ddba9ed-ece4-4ddb-93aa-058f91ccc3fa,DISK]]</span><br><span class="line">37. BP-1440124469-192.168.1.1-1466135627881:blk_1536251272_4849869877 len&#x3D;133158089 MISSING!</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p><strong>可知，第37个块 MISSING。</strong></p>
<h6 id="解决方案和思路"><a href="#解决方案和思路" class="headerlink" title="解决方案和思路"></a>解决方案和思路</h6><p>这个MISSING有两种情况</p>
<ul>
<li><p>块以及副本确实全都丢了，这种只能把块(文件)删了以解决问题</p>
</li>
<li><p>块的连接丢了，但是副本信息还在。</p>
</li>
</ul>
<p>无论如何，发生了块MISSING时，应先尝试把对应的文件 get 到本地，如果能成功最好，不成功就准备删块。</p>
<ol>
<li><p>如果成功的话，就将文件保留在本地，然后使用 <code>hdfs fsck filePath -delete</code> 删除该文件，然后将本地的数据文件重新 put 到HDFS。</p>
</li>
<li><p>如果不成功，同时文件不重要或者有备份，直接删除文件，然后重新上传数据。</p>
</li>
<li><p>如果不成功，同时本地无备份，只能选择删除块了。</p>
<ul>
<li><p><code>hdfs fsck path  -files -blocks -locations</code> 能看到丢失块的具体信息，如 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">37. BP-1440124469-192.168.1.1-1466135627881:blk_1536251272_4849869877 len&#x3D;133158089 MISSING!</span><br></pre></td></tr></table></figure>
</li>
<li><p>去对应的机器(192.168.1.1)上查找 <code>BP-1440124469-192.168.1.1-1466135627881</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">find  &#x2F; -name &quot;BP-1440124469-192.168.1.1-1466135627881&quot;</span><br></pre></td></tr></table></figure>
</li>
<li><p>进入查找到的目录，找到 <code>blk_1536251272_4849869877 len=133158089</code> 文件，删除！</p>
</li>
</ul>
</li>
</ol>
<h4 id="修复未正常关闭文件"><a href="#修复未正常关闭文件" class="headerlink" title="修复未正常关闭文件"></a>修复未正常关闭文件</h4><p> <code>hdfs debug recoverLease -path &lt;path&gt; [-retries &lt;num-retries&gt;]</code></p>
]]></content>
      <categories>
        <category>Apache-Hadoop</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Hadoop</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka问题记录</title>
    <url>/2020/07/30/Apache-Kafka/00.Kafka%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<p>日志：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Exception in thread &quot;Thread-0&quot; org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.</span><br><span class="line">	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.sendOffsetCommitRequest(ConsumerCoordinator.java:702)</span><br><span class="line">	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsSync(ConsumerCoordinator.java:581)</span><br><span class="line">	at org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:1090)</span><br><span class="line">	at com.eebbk.da.kafka2HDFS.handle.KafkaToParquetAutoOffset.run(KafkaToParquetAutoOffset.java:287)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure>

<p>查看日志，因为处理poll到数据的时间超过了 <code>max.poll.interval.ms</code>  导致 <code>consumer</code> 发生了 <code>rebalance</code> ，从而无法手动提交 <code>offset</code>。</p>
<p>溯源：影响处理poll数据的参数是 <code>max.poll.records</code>，因为此参数设置过大，导致超  <code>max.poll.interval.ms</code>  时间。（<code>max.poll.records</code>默认500条，<code>max.poll.interval.ms</code> 默认300000）</p>
<p>解决办法：降低吞吐量，即将 <code>max.poll.records</code> 调小，目前环境我设置的为5000，调整为2000</p>
<p>​                 调整超时时间，将 <code>max.poll.interval.ms</code>  调整为 600000</p>
<p>​                 在手动提交失败是，跳过本次提交，继续处理，期待下一次提交。</p>
<h3 id="KAFKA-broker一直起不来，有时候有报错信息，有时候直接崩溃。"><a href="#KAFKA-broker一直起不来，有时候有报错信息，有时候直接崩溃。" class="headerlink" title="KAFKA broker一直起不来，有时候有报错信息，有时候直接崩溃。"></a>KAFKA broker一直起不来，有时候有报错信息，有时候直接崩溃。</h3><p>启动日志：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">java.io.IOException: Map failed</span><br><span class="line">        at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:888)</span><br><span class="line">        at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:111)</span><br><span class="line">        at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:101)</span><br><span class="line">        at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:213)</span><br><span class="line">        at kafka.log.AbstractIndex.resize(AbstractIndex.scala:101)</span><br><span class="line">        at kafka.log.LogSegment.truncateTo(LogSegment.scala:292)</span><br><span class="line">        at kafka.log.Log.truncateTo(Log.scala:893)</span><br><span class="line">        at kafka.log.LogManager$$anonfun$truncateTo$2.apply(LogManager.scala:301)</span><br><span class="line">        at kafka.log.LogManager$$anonfun$truncateTo$2.apply(LogManager.scala:293)</span><br><span class="line">        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)</span><br><span class="line">        at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)</span><br><span class="line">        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)</span><br><span class="line">        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)</span><br><span class="line">        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)</span><br><span class="line">        at kafka.log.LogManager.truncateTo(LogManager.scala:293)</span><br><span class="line">        at kafka.server.ReplicaManager.makeFollowers(ReplicaManager.scala:854)</span><br><span class="line">        at kafka.server.ReplicaManager.becomeLeaderOrFollower(ReplicaManager.scala:700)</span><br><span class="line">        at kafka.server.KafkaApis.handleLeaderAndIsrRequest(KafkaApis.scala:148)</span><br><span class="line">        at kafka.server.KafkaApis.handle(KafkaApis.scala:84)</span><br><span class="line">        at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:62)</span><br><span class="line">        at java.lang.Thread.run(Thread.java:745)</span><br><span class="line">Caused by: java.lang.OutOfMemoryError: Map failed</span><br><span class="line">        at sun.nio.ch.FileChannelImpl.map0(Native Method)</span><br><span class="line">        at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:885)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">其他直接JVM崩溃</span><br></pre></td></tr></table></figure>

<p>JVM崩溃日志：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#</span><br><span class="line"># There is insufficient memory for the Java Runtime Environment to continue.</span><br><span class="line"># Native memory allocation (malloc) failed to allocate 312 bytes for AllocateHeap</span><br><span class="line"># Possible reasons:</span><br><span class="line">#   The system is out of physical RAM or swap space</span><br><span class="line">#   In 32 bit mode, the process size limit was hit</span><br><span class="line"># Possible solutions:</span><br><span class="line">#   Reduce memory load on the system</span><br><span class="line">#   Increase physical memory or swap space</span><br><span class="line">#   Check if swap backing store is full</span><br><span class="line">#   Use 64 bit Java on a 64 bit OS</span><br><span class="line">#   Decrease Java heap size (-Xmx&#x2F;-Xms)</span><br><span class="line">#   Decrease number of Java threads</span><br><span class="line">#   Decrease Java thread stack sizes (-Xss)</span><br><span class="line">#   Set larger code cache with -XX:ReservedCodeCacheSize&#x3D;</span><br><span class="line"># This output file may be truncated or incomplete.</span><br><span class="line">#</span><br><span class="line">#  Out of Memory Error (allocation.inline.hpp:61), pid&#x3D;56865, tid&#x3D;0x00007fb2aa587700</span><br><span class="line">#</span><br><span class="line"># JRE version: Java(TM) SE Runtime Environment (8.0_131-b11) (build 1.8.0_131-b11)</span><br><span class="line"># Java VM: Java HotSpot(TM) 64-Bit Server VM (25.131-b11 mixed mode linux-amd64 compressed oops)</span><br><span class="line"># Core dump written. Default location: &#x2F;data1&#x2F;kafka&#x2F;kafka_2.11-0.10.2.1&#x2F;core or core.56865</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line">---------------  T H R E A D  ---------------</span><br><span class="line"></span><br><span class="line">Current thread (0x00007fcaa1e3f000):  JavaThread &quot;ExpirationReaper-5&quot; [_thread_in_vm, id&#x3D;57248, stack(0x00007fb2aa487000,0x00007fb2aa588000)]</span><br><span class="line"></span><br><span class="line">Stack: [0x00007fb2aa487000,0x00007fb2aa588000],  sp&#x3D;0x00007fb2aa586900,  free space&#x3D;1022k</span><br><span class="line">Native frames: (J&#x3D;compiled Java code, j&#x3D;interpreted, Vv&#x3D;VM code, C&#x3D;native code)</span><br><span class="line">V  [libjvm.so+0xac826a]</span><br><span class="line">V  [libjvm.so+0x4fd4cb]</span><br><span class="line">V  [libjvm.so+0x2e23c8]</span><br><span class="line">V  [libjvm.so+0x2e2444]</span><br><span class="line">V  [libjvm.so+0x70f40a]</span><br><span class="line">V  [libjvm.so+0xa76910]</span><br><span class="line">V  [libjvm.so+0x927568]</span><br><span class="line">C  [libpthread.so.0+0x7aa1]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">---------------  P R O C E S S  ---------------</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">power management:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Memory: 4k page, physical 65860516k(289912k free), swap 20479996k(19336528k free)</span><br><span class="line"></span><br><span class="line">vm_info: Java HotSpot(TM) 64-Bit Server VM (25.131-b11) for linux-amd64 JRE (1.8.0_131-b11), built on Mar 15 2017 01:23:40 by &quot;java_re&quot; with gcc 4.3.0 20080428 (Red Hat 4.3.0-8)</span><br><span class="line"></span><br><span class="line">time: Wed Jun 10 08:48:32 2020</span><br><span class="line">elapsed time: 322 seconds (0d 0h 5m 22s)</span><br></pre></td></tr></table></figure>

<p>搜索  <a href="https://blog.csdn.net/b644ROfP20z37485O35M/article/details/81571710" target="_blank" rel="noopener">https://blog.csdn.net/b644ROfP20z37485O35M/article/details/81571710</a></p>
<h5 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h5><p>从上面分析解决问题的方法有两个</p>
<ul>
<li>增大系统限制<code>/proc/sys/vm/max_map_count</code>  <code>vm.max_map_count=200000直接写到/etc/sysctl.conf中,然后执行sysctl -p</code></li>
<li>kafka的索引文件是否不需要一直有？是否可以限制一下</li>
</ul>
<h5 id="问题总结"><a href="#问题总结" class="headerlink" title="问题总结"></a>问题总结</h5><p>上面的过程是我思考的一个过程，可能过程有些绕，不过我这里可以来个简单的概述，描述下整个问题发生的过程：</p>
<p>kafka做了很多索引文件的内存映射，每个索引文件占用的内存还很大，这些索引文件并且一直占着没有释放，于是随着索引文件数的增多，而慢慢达到了物理内存的一个上限，比如映射到某个索引文件的时候，物理内存还剩1G，但是我们要映射一个超过1G的文件，因此会映射失败，映射失败接着就做了一次System GC，而在System GC过程中因为PermSize和MaxPermSize不一样，从而导致了在Full GC完之后Perm进行扩容，在扩容的时候因为又调用了一次mmap，而在mmap的时候check是否达到了vma的最大上限，也就是<code>/proc/sys/vm/max_map_count</code>里的65530，如果超过了，就直接crash了。</p>
<p>这只是我从此次crash文件里能想像到的一个现场，当然其实可能会有更多的场景，只要是能触发mmap动作的地方都有可能是导致crash的案发现场，比如后面又给了我一个crash文件，是在创建线程栈的时候因为mmap而导致的crash，完全和OOM没有关系，所以根本原因还是因为kafka做了太多的索引文件映射，导致mmap的vma非常多，超过了系统的限制，从而导致了crash。</p>
]]></content>
      <categories>
        <category>Apache-Kafka</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka集群扩容方案研究</title>
    <url>/2020/08/01/Apache-Kafka/01.Kafka%E9%9B%86%E7%BE%A4%E6%89%A9%E5%AE%B9%E6%96%B9%E6%A1%88%E7%A0%94%E7%A9%B6/</url>
    <content><![CDATA[<h6 id="根据官方脚本：kafka-reassign-partitions-sh-，这里先不研究源码，直接看脚本帮助"><a href="#根据官方脚本：kafka-reassign-partitions-sh-，这里先不研究源码，直接看脚本帮助" class="headerlink" title="根据官方脚本：kafka-reassign-partitions.sh ，这里先不研究源码，直接看脚本帮助"></a>根据官方脚本：kafka-reassign-partitions.sh ，这里先不研究源码，直接看脚本帮助</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost bin]# .&#x2F;kafka-reassign-partitions.sh </span><br><span class="line">This command moves topic partitions between replicas.</span><br><span class="line">Option                                Description                           </span><br><span class="line">------                                -----------                           </span><br><span class="line">--broker-list &lt;String: brokerlist&gt;    The list of brokers to which the      </span><br><span class="line">                                        partitions need to be reassigned in </span><br><span class="line">                                        the form &quot;0,1,2&quot;. This is required  </span><br><span class="line">                                        if --topics-to-move-json-file is    </span><br><span class="line">                                        used to generate reassignment       </span><br><span class="line">                                        configuration                       </span><br><span class="line">--disable-rack-aware                  Disable rack aware replica assignment </span><br><span class="line">--execute                             Kick off the reassignment as specified</span><br><span class="line">                                        by the --reassignment-json-file     </span><br><span class="line">                                        option.                             </span><br><span class="line">--generate                            Generate a candidate partition        </span><br><span class="line">                                        reassignment configuration. Note    </span><br><span class="line">                                        that this only generates a candidate</span><br><span class="line">                                        assignment, it does not execute it. </span><br><span class="line">--reassignment-json-file &lt;String:     The JSON file with the partition      </span><br><span class="line">  manual assignment json file path&gt;     reassignment configurationThe format</span><br><span class="line">                                        to use is -                         </span><br><span class="line">                                      &#123;&quot;partitions&quot;:                        </span><br><span class="line">                                      	[&#123;&quot;topic&quot;: &quot;foo&quot;,                    </span><br><span class="line">                                      	  &quot;partition&quot;: 1,                    </span><br><span class="line">                                      	  &quot;replicas&quot;: [1,2,3] &#125;],            </span><br><span class="line">                                      &quot;version&quot;:1                           </span><br><span class="line">                                      &#125;                                     </span><br><span class="line">--throttle &lt;Long: throttle&gt;           The movement of partitions will be    </span><br><span class="line">                                        throttled to this value (bytes&#x2F;sec).</span><br><span class="line">                                        Rerunning with this option, whilst a</span><br><span class="line">                                        rebalance is in progress, will alter</span><br><span class="line">                                        the throttle value. The throttle    </span><br><span class="line">                                        rate should be at least 1 KB&#x2F;s.     </span><br><span class="line">                                        (default: -1)                       </span><br><span class="line">--topics-to-move-json-file &lt;String:   Generate a reassignment configuration </span><br><span class="line">  topics to reassign json file path&gt;    to move the partitions of the       </span><br><span class="line">                                        specified topics to the list of     </span><br><span class="line">                                        brokers specified by the --broker-  </span><br><span class="line">                                        list option. The format to use is - </span><br><span class="line">                                      &#123;&quot;topics&quot;:                            </span><br><span class="line">                                      	[&#123;&quot;topic&quot;: &quot;foo&quot;&#125;,&#123;&quot;topic&quot;: &quot;foo1&quot;&#125;],</span><br><span class="line">                                      &quot;version&quot;:1                           </span><br><span class="line">                                      &#125;                                     </span><br><span class="line">--verify                              Verify if the reassignment completed  </span><br><span class="line">                                        as specified by the --reassignment- </span><br><span class="line">                                        json-file option. If there is a     </span><br><span class="line">                                        throttle engaged for the replicas   </span><br><span class="line">                                        specified, and the rebalance has    </span><br><span class="line">                                        completed, the throttle will be     </span><br><span class="line">                                        removed                             </span><br><span class="line">--zookeeper &lt;String: urls&gt;            REQUIRED: The connection string for   </span><br><span class="line">                                        the zookeeper connection in the form</span><br><span class="line">                                        host:port. Multiple URLS can be     </span><br><span class="line">                                        given to allow fail-over.</span><br></pre></td></tr></table></figure>



<h2 id="几个重要的参数"><a href="#几个重要的参数" class="headerlink" title="几个重要的参数"></a>几个重要的参数</h2><h4 id="三种模式"><a href="#三种模式" class="headerlink" title="三种模式"></a>三种模式</h4><h6 id="–generate"><a href="#–generate" class="headerlink" title="–generate"></a>–generate</h6><p>给定需要重新分配的Topic，自动生成reassign plan，并不会执行</p>
<h6 id="–execute"><a href="#–execute" class="headerlink" title="–execute"></a>–execute</h6><p>根据指定的reassign plan重新分配Partition</p>
<h6 id="–verify"><a href="#–verify" class="headerlink" title="–verify"></a>–verify</h6><p>验证重新分配Partition是否成功</p>
<h4 id="两个文件参数"><a href="#两个文件参数" class="headerlink" title="两个文件参数"></a>两个文件参数</h4><h6 id="–topics-to-move-json-file"><a href="#–topics-to-move-json-file" class="headerlink" title="–topics-to-move-json-file"></a>–topics-to-move-json-file</h6><p>需要进行重新分配的Topic配置文件，格式如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&quot;partitions&quot;:                        </span><br><span class="line">    [&#123;&quot;topic&quot;: &quot;foo&quot;,                    </span><br><span class="line">      &quot;partition&quot;: 1,                    </span><br><span class="line">      &quot;replicas&quot;: [1,2,3] &#125;],            </span><br><span class="line">      &quot;version&quot;:1                           </span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>文件名不重要，保证内容是json格式就行了。</p>
<h6 id="–reassignment-json-file"><a href="#–reassignment-json-file" class="headerlink" title="–reassignment-json-file"></a>–reassignment-json-file</h6><p>根据上面的配置文件生成的reassign plan。格式如下，可以自己修改。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&quot;topics&quot;:                            </span><br><span class="line">  [&#123;&quot;topic&quot;: &quot;foo&quot;&#125;,&#123;&quot;topic&quot;: &quot;foo1&quot;&#125;],</span><br><span class="line">    &quot;version&quot;:1                           </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="两个集群参数"><a href="#两个集群参数" class="headerlink" title="两个集群参数"></a>两个集群参数</h4><h6 id="–zookeeper"><a href="#–zookeeper" class="headerlink" title="–zookeeper"></a>–zookeeper</h6><p>zk1:2181,zk2:2181</p>
<h6 id="–broker-list"><a href="#–broker-list" class="headerlink" title="–broker-list"></a>–broker-list</h6><p>具体数值为各个broker下面的配置ID，”1,2,3,4,5”。该参数为指定新分配到的broker节点，即，假设原始broker是 “1,2,3,4”，扩容一台，则为”1,2,3,4,5”。</p>
<h2 id="官方方案"><a href="#官方方案" class="headerlink" title="官方方案"></a>官方方案</h2><blockquote>
<ul>
<li>写好 topics-to-move-json-file 配置文件 tp-file.json</li>
<li>使用命令生成 reassign plan即 reassignment-json-file<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;.&#x2F;kafka-reassign-partitions.sh --zookeeper zk1:2181 --topics-to-move-json-file tp-file.json --broker-list &quot;1,2,3,4&quot; --generate</span><br></pre></td></tr></table></figure></li>
<li>(可选)根据需求自己定义或修改上面的 reassignment-json-file 文件</li>
<li>使用命令触发分配计划执行<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;.&#x2F;kafka-reassign-partitions.sh --zookeeper zk1:2181 --reassignment-json-file reassignment-json-file.json --execute</span><br></pre></td></tr></table></figure></li>
<li>校验分配结果<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;.&#x2F;kafka-reassign-partitions.sh --zookeeper zk1:2181 --reassignment-json-file reassignment-json-file.json --verify</span><br></pre></td></tr></table></figure>
</li>
</ul>
</blockquote>
<h2 id="扩容原理"><a href="#扩容原理" class="headerlink" title="扩容原理"></a>扩容原理</h2><p>一般来说，我们Kafka集群上的Topic的Partition数是大于broker数的，所以基本上，一个Topic的partition基本是遍布所有broker的。<br>所以我们扩容的根本原理就在于，将原来的Partition的副本进行迁移。整个迁移过程可能会涉及到副本leader和follower的迁移。  </p>
<h6 id="整体原理如下"><a href="#整体原理如下" class="headerlink" title="整体原理如下:"></a>整体原理如下:</h6><blockquote>
<ol>
<li>将副本数增加一份(无论扩容多少台)</li>
<li>新增副本开始从副本leader开始从头开始复制，即从earliest offset开始</li>
<li>等新增副本跟随上最新offset，将新增的副本添加到 ISR 列表</li>
<li>移除需要移除的broker上的副本</li>
</ol>
</blockquote>
<h6 id="这种方案的缺陷"><a href="#这种方案的缺陷" class="headerlink" title="这种方案的缺陷"></a>这种方案的缺陷</h6><blockquote>
<p>在数据量大的时候进行扩容是，因为要从头拷贝数据，会造成大量读原磁盘，消耗大量的I/O，造成producer操作缓慢，容易产生抖动。</p>
</blockquote>
<h2 id="改良方案"><a href="#改良方案" class="headerlink" title="改良方案"></a>改良方案</h2><p>优化点主要在第二步的从earliest offset。假设我们从latest offset开始复制数据，然后等待新副本保持稳定一段时间后，添加到ISR列表，再移除就副本。<br>具体代码可以看官方的issues，链接：<br><a href="https://issues.apache.org/jira/browse/KAFKA-8328" target="_blank" rel="noopener">官方代码：https://issues.apache.org/jira/browse/KAFKA-8328</a></p>
<h2 id="就官方方案进行测试"><a href="#就官方方案进行测试" class="headerlink" title="就官方方案进行测试"></a>就官方方案进行测试</h2><h6 id="查看-topic-信息"><a href="#查看-topic-信息" class="headerlink" title="查看 topic 信息"></a>查看 topic 信息</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost bin]# .&#x2F;kafka-topics.sh --describe --topic FLINK-TOPIC-1  --zookeeper tnode3:2181</span><br><span class="line">Topic:FLINK-TOPIC-1	PartitionCount:4	ReplicationFactor:2	Configs:</span><br><span class="line">	Topic: FLINK-TOPIC-1	Partition: 0	Leader: 0	Replicas: 0,2	Isr: 2,0</span><br><span class="line">	Topic: FLINK-TOPIC-1	Partition: 1	Leader: 1	Replicas: 1,0	Isr: 1,0</span><br><span class="line">	Topic: FLINK-TOPIC-1	Partition: 2	Leader: 2	Replicas: 2,1	Isr: 2,1</span><br><span class="line">	Topic: FLINK-TOPIC-1	Partition: 3	Leader: 0	Replicas: 0,1	Isr: 1,0</span><br></pre></td></tr></table></figure>

<h6 id="编辑-topics-to-move-json-file-需要的json格式文件"><a href="#编辑-topics-to-move-json-file-需要的json格式文件" class="headerlink" title="编辑 topics-to-move-json-file 需要的json格式文件"></a>编辑 topics-to-move-json-file 需要的json格式文件</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@tnode3 kafka_2.11-0.10.2.1]# cat flink-topic-json </span><br><span class="line">&#123;&quot;topics&quot;: [&#123;&quot;topic&quot;: &quot;FLINK-TOPIC-1&quot;&#125;],</span><br><span class="line"> &quot;version&quot;:1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="生成重分配计划-即-reassignment-json-file-所需要的文件"><a href="#生成重分配计划-即-reassignment-json-file-所需要的文件" class="headerlink" title="生成重分配计划 即 reassignment-json-file 所需要的文件"></a>生成重分配计划 即 reassignment-json-file 所需要的文件</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@tnode3 kafka_2.11-0.10.2.1]# bin&#x2F;kafka-reassign-partitions.sh --zookeeper tnode3:2181 --topics-to-move-json-file flink-topic-json --broker-list &quot;0,1,2,3&quot; --generate</span><br><span class="line">Current partition replica assignment</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[0,1]&#125;,                                &#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[2,1]&#125;,                                &#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[1,0]&#125;,                                &#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[0,2]&#125;</span><br><span class="line">                          ]&#125;</span><br><span class="line"></span><br><span class="line">Proposed partition reassignment configuration</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[2,3]&#125;,                                &#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,2]&#125;,                                &#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[0,1]&#125;,                                &#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[3,0]&#125;</span><br><span class="line">                          ]&#125;</span><br></pre></td></tr></table></figure>
<p>将 Proposed partition reassignment configuration 下面的内容保存到 flink-topic-result.json</p>
<h6 id="执行修改计划"><a href="#执行修改计划" class="headerlink" title="执行修改计划"></a>执行修改计划</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@tnode3 kafka_2.11-0.10.2.1]# bin&#x2F;kafka-reassign-partitions.sh --zookeeper tnode3:2181 --reassignment-json-file flink-topic-result.json --execute</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[0,1]&#125;,                                &#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[2,1]&#125;,                                &#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[1,0]&#125;,                                &#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[0,2]&#125;</span><br><span class="line">                          ]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started reassignment of partitions.</span><br></pre></td></tr></table></figure>
<p>上面有提到。可以再讲 当前的分配情况保存成新的计划文件，用来回滚重分配的操作。</p>
<h6 id="校验重分配是否成功"><a href="#校验重分配是否成功" class="headerlink" title="校验重分配是否成功"></a>校验重分配是否成功</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@tnode3 kafka_2.11-0.10.2.1]# bin&#x2F;kafka-reassign-partitions.sh --zookeeper tnode3:2181 --reassignment-json-file flink-topic-result.json --verify</span><br><span class="line">Status of partition reassignment: </span><br><span class="line">Reassignment of partition [FLINK-TOPIC-1,3] completed successfully</span><br><span class="line">Reassignment of partition [FLINK-TOPIC-1,2] completed successfully</span><br><span class="line">Reassignment of partition [FLINK-TOPIC-1,1] completed successfully</span><br><span class="line">Reassignment of partition [FLINK-TOPIC-1,0] completed successfully</span><br></pre></td></tr></table></figure>

<h6 id="修改副本顺序-利用kafka自动生成计划的跳过下面部分，因为自动生成的计划，已经调整了副本顺序"><a href="#修改副本顺序-利用kafka自动生成计划的跳过下面部分，因为自动生成的计划，已经调整了副本顺序" class="headerlink" title="修改副本顺序   (利用kafka自动生成计划的跳过下面部分，因为自动生成的计划，已经调整了副本顺序)"></a>修改副本顺序   (利用kafka自动生成计划的跳过下面部分，因为自动生成的计划，已经调整了副本顺序)</h6><p>此时，topic的副本已经迁移完成，但是所有副本的leader还在原来的broker上，我们需要分配一定的leader到新的broker上，所以还需要进行平衡leader。</p>
<p>手动编辑的计划(也可在编辑的时候就调整顺序，此处为只迁移副本的计划，不含调整副本顺序)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:0, &quot;replicas&quot;:[4,2,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:1, &quot;replicas&quot;:[5,3,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:2, &quot;replicas&quot;:[0,4,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:3, &quot;replicas&quot;:[1,5,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:4, &quot;replicas&quot;:[2,0,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:5, &quot;replicas&quot;:[3,1,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:6, &quot;replicas&quot;:[4,3,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:7, &quot;replicas&quot;:[5,4,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:8, &quot;replicas&quot;:[0,5,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:9, &quot;replicas&quot;:[1,0,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:10,&quot;replicas&quot;:[2,1,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:11,&quot;replicas&quot;:[3,2,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:12,&quot;replicas&quot;:[4,5,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:13,&quot;replicas&quot;:[5,0,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:14,&quot;replicas&quot;:[0,1,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:15,&quot;replicas&quot;:[1,2,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:16,&quot;replicas&quot;:[2,3,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:17,&quot;replicas&quot;:[3,4,8]&#125;</span><br><span class="line">                           ]&#125;</span><br></pre></td></tr></table></figure>

<p>修改副本顺序，因为副本顺序中，kafka默认第一个为首选leader，经过平衡后会首选使用第一个副本作为leader。topicPartition.json 格式，只是调整了副本顺序</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:0, &quot;replicas&quot;:[4,2,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:1, &quot;replicas&quot;:[5,3,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:2, &quot;replicas&quot;:[0,4,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:3, &quot;replicas&quot;:[1,5,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:4, &quot;replicas&quot;:[2,0,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:5, &quot;replicas&quot;:[3,1,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:6, &quot;replicas&quot;:[4,3,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:7, &quot;replicas&quot;:[5,4,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:8, &quot;replicas&quot;:[0,5,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:9, &quot;replicas&quot;:[6,0,1]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:10,&quot;replicas&quot;:[7,1,2]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:11,&quot;replicas&quot;:[8,2,3]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:12,&quot;replicas&quot;:[6,5,4]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:13,&quot;replicas&quot;:[7,0,5]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:14,&quot;replicas&quot;:[8,1,0]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:15,&quot;replicas&quot;:[1,2,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:16,&quot;replicas&quot;:[2,3,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:17,&quot;replicas&quot;:[3,4,8]&#125;</span><br><span class="line">                           ]&#125;</span><br></pre></td></tr></table></figure>

<p>以上两步也可一次性编辑完成。</p>
<h6 id="平衡leader"><a href="#平衡leader" class="headerlink" title="平衡leader"></a>平衡leader</h6><p>一次性平衡所有Topic：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;kafka-preferred-replica-election.sh --zookeeper gs-kafka1:2181</span><br></pre></td></tr></table></figure>
<p>或者 添加配置 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">auto.leader.rebalance.enable&#x3D;true</span><br></pre></td></tr></table></figure>
<p>或者平衡单个Topic</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;kafka-preferred-replica-election.sh --zookeeper gs-kafka1:2181 --path-to-json-file topicPartition.json</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Apache-Kafka</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>kafka集群扩容记录</title>
    <url>/2020/08/01/Apache-Kafka/02.kafka%E9%9B%86%E7%BE%A4%E6%89%A9%E5%AE%B9%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<h1 id="kafka集群扩容记录"><a href="#kafka集群扩容记录" class="headerlink" title="kafka集群扩容记录"></a>kafka集群扩容记录</h1><p>因为我们需要扩容 3 个broker，即broker从 0,1,2,3,4,5 扩容到 0,1,2,3,4,5,6,7,8 ，所以先进行第一部分工作</p>
<h2 id="集群扩容"><a href="#集群扩容" class="headerlink" title="集群扩容"></a>集群扩容</h2><ul>
<li><p>新机器的 6,7,8 的准备：JAVA、文件句柄数、防火墙、主机名以及hosts映射</p>
</li>
<li><p>从 0,1,2,3,4,5 选一台机器，拷贝其上的 kafka 安装目录到 6,7,8 三台机器，删除其中的logs目录，修改 server.properties 文件中的 broker.id 项。</p>
</li>
<li><p>新机器启动kafka：</p>
<p><code>sh kafka-server-start.sh -daemon ../config/server.properties</code></p>
</li>
<li><p>检查新broker是否加入成功，检查zookeeper上的/broker/ids下的node是否包含新增broker.id</p>
</li>
</ul>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><h5 id="查看现有Topic的partition、副本以及leader和follower分布。"><a href="#查看现有Topic的partition、副本以及leader和follower分布。" class="headerlink" title="查看现有Topic的partition、副本以及leader和follower分布。"></a>查看现有Topic的partition、副本以及leader和follower分布。</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@gs-kafka1 kafka_2.11-0.10.2.1]# bin&#x2F;kafka-topics.sh --describe --zookeeper gs-kafka1:2181 --topic WATCH-LOCATION</span><br><span class="line">Topic:WATCH-LOCATION	PartitionCount:18	ReplicationFactor:3	Configs:</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 0	Leader: 4	Replicas: 4,2,3	Isr: 3,4,2</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 1	Leader: 5	Replicas: 5,3,4	Isr: 5,3,4</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 2	Leader: 0	Replicas: 0,4,5	Isr: 5,0,4</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 3	Leader: 1	Replicas: 1,5,0	Isr: 5,0,1</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 4	Leader: 2	Replicas: 2,0,1	Isr: 2,0,1</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 5	Leader: 3	Replicas: 3,1,2	Isr: 3,2,1</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 6	Leader: 4	Replicas: 4,3,5	Isr: 5,3,4</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 7	Leader: 5	Replicas: 5,4,0	Isr: 5,0,4</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 8	Leader: 0	Replicas: 0,5,1	Isr: 5,0,1</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 9	Leader: 1	Replicas: 1,0,2	Isr: 0,1,2</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 10	Leader: 2	Replicas: 2,1,3	Isr: 2,3,1</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 11	Leader: 3	Replicas: 3,2,4	Isr: 3,2,4</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 12	Leader: 4	Replicas: 4,5,0	Isr: 5,0,4</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 13	Leader: 5	Replicas: 5,0,1	Isr: 5,0,1</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 14	Leader: 0	Replicas: 0,1,2	Isr: 0,2,1</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 15	Leader: 1	Replicas: 1,2,3	Isr: 1,3,2</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 16	Leader: 2	Replicas: 2,3,4	Isr: 2,4,3</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 17	Leader: 3	Replicas: 3,4,5	Isr: 5,3,4</span><br></pre></td></tr></table></figure>
<p>因为现有topic的副本及其leader都在 0,1,2,3,4,5 broker，所以我们需要迁移三分之一的副本到新broker，并且将三分之一的leader转到新broker，才能做到网络和磁盘IO的扩容。</p>
<h2 id="迁移"><a href="#迁移" class="headerlink" title="迁移"></a>迁移</h2><h6 id="参考扩容方案中的方法，我们直接编辑-topic-result-json，格式如下"><a href="#参考扩容方案中的方法，我们直接编辑-topic-result-json，格式如下" class="headerlink" title="参考扩容方案中的方法，我们直接编辑 topic-result.json，格式如下"></a>参考扩容方案中的方法，我们直接编辑 topic-result.json，格式如下</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:0, &quot;replicas&quot;:[4,2,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:1, &quot;replicas&quot;:[5,3,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:2, &quot;replicas&quot;:[0,4,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:3, &quot;replicas&quot;:[1,5,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:4, &quot;replicas&quot;:[2,0,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:5, &quot;replicas&quot;:[3,1,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:6, &quot;replicas&quot;:[4,3,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:7, &quot;replicas&quot;:[5,4,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:8, &quot;replicas&quot;:[0,5,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:9, &quot;replicas&quot;:[1,0,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:10,&quot;replicas&quot;:[2,1,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:11,&quot;replicas&quot;:[3,2,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:12,&quot;replicas&quot;:[4,5,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:13,&quot;replicas&quot;:[5,0,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:14,&quot;replicas&quot;:[0,1,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:15,&quot;replicas&quot;:[1,2,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:16,&quot;replicas&quot;:[2,3,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:17,&quot;replicas&quot;:[3,4,8]&#125;</span><br><span class="line">                           ]&#125;</span><br></pre></td></tr></table></figure>
<p>但是上述只考虑到了副本迁移，未考虑到leader迁移，参考扩容方案，我们迁移的时候顺便 修改replicas顺序，修改如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:0, &quot;replicas&quot;:[4,2,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:1, &quot;replicas&quot;:[5,3,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:2, &quot;replicas&quot;:[0,4,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:3, &quot;replicas&quot;:[1,5,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:4, &quot;replicas&quot;:[2,0,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:5, &quot;replicas&quot;:[3,1,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:6, &quot;replicas&quot;:[4,3,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:7, &quot;replicas&quot;:[5,4,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:8, &quot;replicas&quot;:[0,5,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:9, &quot;replicas&quot;:[6,0,1]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:10,&quot;replicas&quot;:[7,1,2]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:11,&quot;replicas&quot;:[8,2,3]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:12,&quot;replicas&quot;:[6,5,4]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:13,&quot;replicas&quot;:[7,0,5]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:14,&quot;replicas&quot;:[8,1,0]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:15,&quot;replicas&quot;:[1,2,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:16,&quot;replicas&quot;:[2,3,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:17,&quot;replicas&quot;:[3,4,8]&#125;</span><br><span class="line">                           ]&#125;</span><br></pre></td></tr></table></figure>
<p>其他 topic 也可参考此方法一起修改副本位置和副本顺序。然后开始迁移</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;kafka-reassign-partitions.sh --zookeeper gs-kafka1:2181 --reassignment-json-file topic-result.json  --execute</span><br></pre></td></tr></table></figure>



<h3 id="等所有topic迁移完成后，开始leader迁移，两个方法"><a href="#等所有topic迁移完成后，开始leader迁移，两个方法" class="headerlink" title="等所有topic迁移完成后，开始leader迁移，两个方法"></a>等所有topic迁移完成后，开始leader迁移，两个方法</h3><ul>
<li>一次性平衡所有topic<br><code>bin/kafka-preferred-replica-election.sh --zookeeper gs-kafka1:2181</code></li>
<li>每次平衡部分topic，编写 topicPartition.json ，格式如下。<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:0&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:1&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:2&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:3&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:4&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:5&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:6&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:7&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:8&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:9&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:10&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:11&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:12&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:13&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:14&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:15&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:16&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:17&#125;</span><br><span class="line">                           ]&#125;</span><br></pre></td></tr></table></figure>
然后平衡 <code>bin/kafka-preferred-replica-election.sh --zookeeper gs-kafka1:2181 --path-to-json-file topicPartition.json</code></li>
</ul>
]]></content>
      <categories>
        <category>Apache-Kafka</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka客户端命令操作</title>
    <url>/2020/07/17/Apache-Kafka/Kafka%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<h4 id="Topic-管理"><a href="#Topic-管理" class="headerlink" title="Topic 管理"></a>Topic 管理</h4><p><code>kafka-topics.sh</code>  的参数</p>
<table>
<thead>
<tr>
<th align="left">参数名称</th>
<th>解释及其作用</th>
</tr>
</thead>
<tbody><tr>
<td align="left">–alter</td>
<td>用于修改主题，包括分区数及Topic的配置</td>
</tr>
<tr>
<td align="left">–config</td>
<td>创建或修改Topic时，用于设置 Topic 级别的配置</td>
</tr>
<tr>
<td align="left">–create</td>
<td>创建 Topic</td>
</tr>
<tr>
<td align="left">–delete</td>
<td>删除 Topic</td>
</tr>
<tr>
<td align="left">–delete-config</td>
<td>删除 Topic 级别被覆盖的配置</td>
</tr>
<tr>
<td align="left">–describe</td>
<td>查看 Topic 的详细信息</td>
</tr>
<tr>
<td align="left">–disable-rack-aware</td>
<td>创建 Topic 时不考虑机架信息</td>
</tr>
<tr>
<td align="left">–help</td>
<td>打印帮助信息文档</td>
</tr>
<tr>
<td align="left">–if-exists</td>
<td>修改或删除 Topic时，只有 Topic 存在时才会执行操作</td>
</tr>
<tr>
<td align="left">–if-not-exists</td>
<td>创建 Topic 时，只有 Topic 不存在才会执行操作</td>
</tr>
<tr>
<td align="left">–list</td>
<td>列出所有可用的 Topic</td>
</tr>
<tr>
<td align="left">–partitions</td>
<td>创建 Topic 或增加分区时 指定分区数</td>
</tr>
<tr>
<td align="left">–replica-assignment</td>
<td>手工制定分区副本的分配方案</td>
</tr>
<tr>
<td align="left">–replication-factor</td>
<td>创建 Topic 时指定副本数</td>
</tr>
<tr>
<td align="left">–topic</td>
<td>指定 Topic 名称</td>
</tr>
<tr>
<td align="left">–topics-with-overrides</td>
<td>使用describe查看 Topic 时，只展示包含覆盖配置的 Topic</td>
</tr>
<tr>
<td align="left">–unavailable-partition</td>
<td>使用describe查看 Topic 时，只展示包含无leader的分区</td>
</tr>
<tr>
<td align="left">–under-replicated-partitions</td>
<td>使用describe查看 Topic 时，只展示包含失效副本的分区</td>
</tr>
<tr>
<td align="left">–zookeeper</td>
<td>指定连接的zookeeper信息（必填）（zk1:2181/kafka）</td>
</tr>
</tbody></table>
<h6 id="创建-Topic"><a href="#创建-Topic" class="headerlink" title="创建 Topic"></a>创建 Topic</h6><p>首先确认一个参数 <code>auto.create.topics.enable=true</code> ，此参数用来自动创建 Topic，也就说当发送消费等操作用到了未创建的 <code>Topic</code> 的时候，此参数会帮忙自动创建一个默认配置的 Topic。<strong>强烈建议在 Kafka 配置关闭这个参数。</strong></p>
<p> 接下来看看怎么在客户端手动创建一个 <code>Topic</code>，使用  </p>
<p>-topics.sh<code>，指定</code>Zookeeper、partition、replica`  完成创建即可。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# pwd</span><br><span class="line">/data/kafka/kafka_2.11-0.10.2.1/bin</span><br><span class="line">[root@tnode1 bin]# ./kafka-topics.sh --create --zookeeper tnode3:2181 --replication-factor 2 --partitions 3 --topic LxmTest</span><br><span class="line">Created topic "LxmTest".</span><br></pre></td></tr></table></figure>

<h6 id="查看-Topic"><a href="#查看-Topic" class="headerlink" title="查看 Topic"></a>查看 Topic</h6><p>查看所有 <code>Topic</code></p>
<p>查看所有 <code>Topic</code> 信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-topics.sh --zookeeper tnode3:2181  --describe</span><br><span class="line">Topic:LxmTest	PartitionCount:3	ReplicationFactor:2	Configs:</span><br><span class="line">	Topic: LxmTest	Partition: 0	Leader: 1	Replicas: 1,0	Isr: 1,0</span><br><span class="line">	Topic: LxmTest	Partition: 1	Leader: 2	Replicas: 2,1	Isr: 2,1</span><br><span class="line">	Topic: LxmTest	Partition: 2	Leader: 0	Replicas: 0,2	Isr: 0,2</span><br><span class="line">... </span><br><span class="line">(其他 topic 信息省略)</span><br></pre></td></tr></table></figure>

<p>查看单个 <code>Topic</code> 信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-topics.sh --zookeeper tnode3:2181  --describe --topic LxmTest</span><br><span class="line">Topic:LxmTest	PartitionCount:3	ReplicationFactor:2	Configs:</span><br><span class="line">	Topic: LxmTest	Partition: 0	Leader: 1	Replicas: 1,0	Isr: 1,0</span><br><span class="line">	Topic: LxmTest	Partition: 1	Leader: 2	Replicas: 2,1	Isr: 2,1</span><br><span class="line">	Topic: LxmTest	Partition: 2	Leader: 0	Replicas: 0,2	Isr: 0,2</span><br></pre></td></tr></table></figure>

<h6 id="删除-Topic"><a href="#删除-Topic" class="headerlink" title="删除 Topic"></a>删除 Topic</h6><p>删除 Topic 分为手动和自动两种方式</p>
<ul>
<li>手动删除 Topic ：需要先删除 Zookeeper 上的相关元数据，然后删除各个 Broker 节点上日志目录的下的此 Topic 的分区日志。</li>
<li>自动删除 Topic ：首先 <code>delete.topic.enable=true</code> 参数必须配置，然后通过 客户端命令删除 Topic，将会自动将元数据和各分区的数据日志删除。如果未配置，将只会删除元数据，不会自动删除日志。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-topics.sh --zookeeper tnode3:2181  --delete --topic LxmTest</span><br><span class="line">Topic LxmTest is marked for deletion.</span><br><span class="line">Note: This will have no impact if delete.topic.enable is not set to true.</span><br></pre></td></tr></table></figure>

<h6 id="增加分区"><a href="#增加分区" class="headerlink" title="增加分区"></a>增加分区</h6><p>一般来说，我们在需要提升较大吞吐的情况下，可以选择增加 partition 。扩展分区命令如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-topics.sh --zookeeper tnode3:2181 --topic LxmTest  --alter --partitions 3</span><br><span class="line">WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affected</span><br><span class="line">Error while executing topic command : The number of partitions for a topic can only be increased</span><br><span class="line">[2019-08-23 08:42:20,176] ERROR kafka.admin.AdminOperationException: The number of partitions for a topic can only be increased</span><br><span class="line">	at kafka.admin.AdminUtils$.addPartitions(AdminUtils.scala:271)</span><br><span class="line">	at kafka.admin.TopicCommand$$anonfun$alterTopic$1.apply(TopicCommand.scala:145)</span><br><span class="line">	at kafka.admin.TopicCommand$$anonfun$alterTopic$1.apply(TopicCommand.scala:122)</span><br><span class="line">	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)</span><br><span class="line">	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)</span><br><span class="line">	at kafka.admin.TopicCommand$.alterTopic(TopicCommand.scala:122)</span><br><span class="line">	at kafka.admin.TopicCommand$.main(TopicCommand.scala:62)</span><br><span class="line">	at kafka.admin.TopicCommand.main(TopicCommand.scala)</span><br><span class="line"> (kafka.admin.TopicCommand$)</span><br><span class="line">[root@tnode1 bin]# ./kafka-topics.sh --zookeeper tnode3:2181 --topic LxmTest  --alter --partitions 4</span><br><span class="line">WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affected</span><br><span class="line">Adding partitions succeeded!</span><br></pre></td></tr></table></figure>

<p>目前分区只能增加，不能减少或者不变的进行修改。</p>
<h4 id="配置管理"><a href="#配置管理" class="headerlink" title="配置管理"></a>配置管理</h4><p>配置管理分为 Topic 级别和 Client 级别两种</p>
<h6 id="Topics-级别"><a href="#Topics-级别" class="headerlink" title="Topics  级别"></a>Topics  级别</h6><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --describe</span><br><span class="line">Configs for topic 'LxmTest' are</span><br></pre></td></tr></table></figure>

<p>这里的结果表示没有做单独配置，均使用的集群默认配置。</p>
<p>增加一个配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --alter --add-config flush.messages=2</span><br><span class="line">Completed Updating config for entity: topic 'LxmTest'.</span><br><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --describe</span><br><span class="line">Configs for topic 'LxmTest' are flush.messages=2</span><br></pre></td></tr></table></figure>

<p>再增加一个配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --alter --add-config retention.ms=259200000</span><br><span class="line">Completed Updating config for entity: topic 'LxmTest'.</span><br><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --describe</span><br><span class="line">Configs for topic 'LxmTest' are retention.ms=259200000,flush.messages=2</span><br></pre></td></tr></table></figure>

<p>一次性添加多个配置，配置用逗号分隔即可</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --alter --add-config retention.ms=259200000,flush.messages=6</span><br><span class="line">Completed Updating config for entity: topic 'LxmTest'.</span><br><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --describe</span><br><span class="line">Configs for topic 'LxmTest' are retention.ms=259200000,flush.messages=6</span><br></pre></td></tr></table></figure>

<p>修改其中一个配置，其实增加没有区别</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --alter --add-config flush.messages=5</span><br><span class="line">Completed Updating config for entity: topic 'LxmTest'.</span><br><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --describe</span><br><span class="line">Configs for topic 'LxmTest' are retention.ms=259200000,flush.messages=5</span><br></pre></td></tr></table></figure>

<p>删除一个配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --alter --delete-config retention.ms</span><br><span class="line">Completed Updating config for entity: topic 'LxmTest'.</span><br><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --describe</span><br><span class="line">Configs for topic 'LxmTest' are flush.messages=5</span><br></pre></td></tr></table></figure>

<h6 id="Clients-级别"><a href="#Clients-级别" class="headerlink" title="Clients 级别"></a>Clients 级别</h6><h4 id="分区管理"><a href="#分区管理" class="headerlink" title="分区管理"></a>分区管理</h4><h6 id="分区平衡"><a href="#分区平衡" class="headerlink" title="分区平衡"></a>分区平衡</h6><p>分区平衡分为自动和手动两种：</p>
<ul>
<li>自动：在 server.properties 中配置参数 <code>auto.leader.rebalance.enable = true</code> ，那么集群将会定时进行分区平衡，将 prefer-replica 副本即AR列表第一个副本，置为 leader 副本。 (<code>leader.imbalance.per.broker.percentage = 10</code> 此参数来决定自动平衡的阈值即失衡率超过10%会开始平衡)</li>
<li>手动：执行平衡命令：<code>./kafka-preferred-replica-election.sh --zookeeper tnode3:2181</code></li>
</ul>
<h6 id="分区迁移"><a href="#分区迁移" class="headerlink" title="分区迁移"></a>分区迁移</h6><p>如果我们需要下线一个 broker 节点，那么就需要将该节点上的相关 Topic 的 partition 迁移到其他 broker 节点上去，因为 Kafka 不会自动进行分区迁移的操作，如果不进行手动迁移，就会发生副本实效和数据丢失的情况。同样，如果是我们新上线一个节点，只有新增 Topic 的分区才会分配到新节点，此时也需要进行分区迁移操作。</p>
<p>这个 Topic 目前三个分区分别在 0、1、2 三个 broker 上。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-topics.sh  --zookeeper tnode3:2181 --describe --topic LxmTest</span><br><span class="line">Topic:LxmTest	PartitionCount:3	ReplicationFactor:2	Configs:retention.ms=259200000,flush.messages=6</span><br><span class="line">	Topic: LxmTest	Partition: 0	Leader: 0	Replicas: 0,2	Isr: 0,2</span><br><span class="line">	Topic: LxmTest	Partition: 1	Leader: 1	Replicas: 1,0	Isr: 1,0</span><br><span class="line">	Topic: LxmTest	Partition: 2	Leader: 2	Replicas: 2,1	Isr: 2,1</span><br></pre></td></tr></table></figure>

<p>现在，我们需要下线 broker2，也就是我们需要将 partition 2 迁移到 0,1 broker 上，步骤如下：</p>
<ol>
<li><p>先创建一个分区迁移的配置文件，这里叫  <code>topic-move.json</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    "topics":</span><br><span class="line">    	[	</span><br><span class="line">    		&#123;</span><br><span class="line">            	"topic":"LxmTest"</span><br><span class="line">        	&#125;</span><br><span class="line">    	],</span><br><span class="line">    "version":1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>然后执行命令生成迁移计划</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-reassign-partitions.sh --zookeeper tnode3:2181 --topics-to-move-json-file topic-move.json --broker-list "0,1" --generate</span><br><span class="line">Current partition replica assignment</span><br><span class="line">&#123;"version":1,"partitions":[&#123;"topic":"LxmTest","partition":2,"replicas":[2,1]&#125;,                            &#123;"topic":"LxmTest","partition":0,"replicas":[0,2]&#125;,                            &#123;"topic":"LxmTest","partition":1,"replicas":[1,0]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Proposed partition reassignment configuration</span><br><span class="line">&#123;"version":1,"partitions":[&#123;"topic":"LxmTest","partition":2,"replicas":[1,0]&#125;,                            &#123;"topic":"LxmTest","partition":0,"replicas":[1,0]&#125;,                            &#123;"topic":"LxmTest","partition":1,"replicas":[0,1]&#125;]&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>然后将上面的 <code>Proposed partition reassignment configuration</code> 下面的内容复制到 <code>topic-repartition.json</code> 文件中</p>
</li>
<li><p>执行迁移计划</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# vi topic-repartition.json</span><br><span class="line">[root@tnode1 bin]# ./kafka-reassign-partitions.sh --zookeeper tnode3:2181 --reassignment-json-file topic-repartition.json --execute</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;"version":1,"partitions":[&#123;"topic":"LxmTest","partition":2,"replicas":[2,1]&#125;,                            &#123;"topic":"LxmTest","partition":0,"replicas":[0,2]&#125;,                            &#123;"topic":"LxmTest","partition":1,"replicas":[1,0]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started reassignment of partitions.</span><br></pre></td></tr></table></figure>
</li>
<li><p>验证迁移是否成功</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-reassign-partitions.sh --zookeeper tnode3:2181 --reassignment-json-file topic-repartition.json --verify</span><br><span class="line">Status of partition reassignment: </span><br><span class="line">Reassignment of partition [LxmTest,2] completed successfully</span><br><span class="line">Reassignment of partition [LxmTest,0] completed successfully</span><br><span class="line">Reassignment of partition [LxmTest,1] completed successfully</span><br></pre></td></tr></table></figure>
</li>
<li><p>再次查看 topic 信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-topics.sh  --zookeeper tnode3:2181 --describe --topic LxmTest</span><br><span class="line">Topic:LxmTest	PartitionCount:3	ReplicationFactor:2	Configs:retention.ms=259200000,flush.messages=6</span><br><span class="line">	Topic: LxmTest	Partition: 0	Leader: 0	Replicas: 1,0	Isr: 0,1</span><br><span class="line">	Topic: LxmTest	Partition: 1	Leader: 1	Replicas: 0,1	Isr: 1,0</span><br><span class="line">	Topic: LxmTest	Partition: 2	Leader: 1	Replicas: 1,0	Isr: 1,0</span><br></pre></td></tr></table></figure>

<p>发现已经没有 broker 2 节点的分区了 </p>
</li>
</ol>
<h6 id="集群扩容"><a href="#集群扩容" class="headerlink" title="集群扩容"></a>集群扩容</h6><p>集群扩容很好理解，就是Kafka磁盘容量不足了，需要增加 broker 节点，然后迁移分区。和上面的 broker下线操作类似，只不是生成迁移计划的时候，增加 broker 而已。具体代码就不贴了。</p>
<p>总得来说，其实集群迁移，无论是 broker 下线 还是 集群扩容 或者是 增加副本，我们都只要手动梳理迁移计划，尽量将各个分区平衡分配，同时将 AR 列表第一个副本 (即默认的 leader 副本) 平均分配，然后进行迁移即可。</p>
<p><strong>！## ！大量 topic 进行迁移时，最好分批进行，避免影响正常业务。另外，可以限制迁移的流量。</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-reassign-partitions.sh --zookeeper tnode3:2181 --reassignment-json-file topic-repartition.json --execute --throttle 1024</span><br></pre></td></tr></table></figure>

<p><strong>！## ！如果做了流量限制，需要使用 –verify 来检查是否完成迁移，在完成时，–verify 参数会解除限流</strong></p>
<h2 id="kafka设置某个topic的数据过期时间"><a href="#kafka设置某个topic的数据过期时间" class="headerlink" title="kafka设置某个topic的数据过期时间"></a>kafka设置某个topic的数据过期时间</h2><h6 id="全局设置"><a href="#全局设置" class="headerlink" title="全局设置"></a>全局设置</h6><p>修改 server.properties</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">log.retention.hours&#x3D;72</span><br><span class="line">log.cleanup.policy&#x3D;delete</span><br></pre></td></tr></table></figure>

<h6 id="单独对某一个topic设置过期时间"><a href="#单独对某一个topic设置过期时间" class="headerlink" title="单独对某一个topic设置过期时间"></a>单独对某一个topic设置过期时间</h6><p>但如果只有某一个topic数据量过大。想单独对这个topic的过期时间设置短点：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;kafka-configs.sh --zookeeper localhost:2181 --alter --entity-name mytopic --entity-type topics --add-config retention.ms&#x3D;86400000</span><br></pre></td></tr></table></figure>

<p>retention.ms = 86400000 为一天，单位是毫秒。</p>
<p>查看设置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;kafka-configs.sh --zookeeper localhost:2181 --describe --entity-name mytopic --entity-type topics</span><br><span class="line"></span><br><span class="line">Configs for mytopic are retention.ms&#x3D;86400000</span><br></pre></td></tr></table></figure>

<h6 id="立即删除某个topic下的数据"><a href="#立即删除某个topic下的数据" class="headerlink" title="立即删除某个topic下的数据"></a>立即删除某个topic下的数据</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;kafka-topics.sh --zookeeper localhost:2181 --alter --topic mytopic --config cleanup.policy&#x3D;delete</span><br></pre></td></tr></table></figure>









]]></content>
      <categories>
        <category>Apache-Kafka</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Maven编译错误</title>
    <url>/2020/08/15/Apache-Maven/%E7%BC%96%E8%AF%91%E9%94%99%E8%AF%AF/</url>
    <content><![CDATA[<p>报错：Failed to transfer file:<strong>*****</strong>. Return code is: 501 , ReasonPhrase:HTTPS Required. 这时候好像发现了什么重点————我们需要将maven配置修改一下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;!-- 中央仓库1 --&gt;</span><br><span class="line">&lt;mirror&gt;</span><br><span class="line">    &lt;id&gt;repo1&lt;&#x2F;id&gt;</span><br><span class="line">    &lt;mirrorOf&gt;central&lt;&#x2F;mirrorOf&gt;</span><br><span class="line">    &lt;name&gt;Human Readable Name for this Mirror.&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;url&gt;https:&#x2F;&#x2F;repo1.maven.org&#x2F;maven2&#x2F;&lt;&#x2F;url&gt;</span><br><span class="line">&lt;&#x2F;mirror&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 中央仓库2 --&gt;</span><br><span class="line">&lt;mirror&gt;</span><br><span class="line">    &lt;id&gt;repo2&lt;&#x2F;id&gt;</span><br><span class="line">    &lt;mirrorOf&gt;central&lt;&#x2F;mirrorOf&gt;</span><br><span class="line">    &lt;name&gt;Human Readable Name for this Mirror.&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;url&gt;https:&#x2F;&#x2F;repo2.maven.org&#x2F;maven2&#x2F;&lt;&#x2F;url&gt;</span><br><span class="line">&lt;&#x2F;mirror&gt;</span><br></pre></td></tr></table></figure>

<p>随后发现可以正常导入依赖了。</p>
<p>所以这个问题的根源就是maven中央仓库现在只支持HTTPS协议，我们改一下就行。</p>
]]></content>
      <categories>
        <category>Apache-Maven</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title>Apache-Kylin初探</title>
    <url>/2020/08/15/Apache-Kylin/Apache-Kylin%E5%88%9D%E6%8E%A2/</url>
    <content><![CDATA[<h3 id="Apache-Kylin-是什么"><a href="#Apache-Kylin-是什么" class="headerlink" title="Apache Kylin 是什么"></a>Apache Kylin 是什么</h3><p>一个采用多维立方体预计算技术的 <code>OLAP</code> <strong>引擎框架</strong>，简单来说就是做大数据查询的。</p>
<p><code>Apache Kylin</code> 的工作原理本质上是 <code>MOLAP</code>（Multidimensional Online Analytical Processing）Cube，也就是多维立方体分析。</p>
<h3 id="Kylin-安装环境要求"><a href="#Kylin-安装环境要求" class="headerlink" title="Kylin 安装环境要求"></a>Kylin 安装环境要求</h3><ul>
<li>KYLIN_HOME </li>
<li>HADOOP_CONF_DIR</li>
<li>HBase Client</li>
<li>Hive Client</li>
<li>Zookeeper Client</li>
<li>Spark Client</li>
</ul>
<p>安装之前需要确认节点上的环境是否满足要求，可以使用  <code>bin/check-env.sh</code>  来确认是否满足环境要求</p>
<h3 id="kylin-properties，初始配置修改"><a href="#kylin-properties，初始配置修改" class="headerlink" title="kylin.properties，初始配置修改"></a>kylin.properties，初始配置修改</h3><p>因为 Kylin 很多默认配置都在根目录下，这里先改几个默认配置 </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Kylin 在 HDFS 上的工作目录，默认是 /kylin</span></span><br><span class="line">kylin.env.hdfs-working-dir=/apps/kylin26</span><br><span class="line"><span class="meta">#</span><span class="bash"> Kylin 在 ZK 上的目录，默认是 /kylin</span></span><br><span class="line">kylin.env.zookeeper-base-path=/apps/kylin26</span><br><span class="line"><span class="meta">#</span><span class="bash"> Kylin 服务启动地址和端口，默认是 localhost:7070</span></span><br><span class="line">kylin.server.cluster-servers=calcnode1:7070</span><br><span class="line"><span class="meta">#</span><span class="bash"> Kylin 在 Hive 中的数据库，默认是 default</span></span><br><span class="line">kylin.source.hive.database-for-flat-table=kylin26</span><br><span class="line"><span class="meta">#</span><span class="bash"> Kylin 在 HBase 中的工作命名空间，默认是             </span></span><br><span class="line">kylin.storage.hbase.namespace=kylin26</span><br></pre></td></tr></table></figure>



<h4 id="满足环境要求之后直接启动就-OK，成功启动的界面如下。"><a href="#满足环境要求之后直接启动就-OK，成功启动的界面如下。" class="headerlink" title="满足环境要求之后直接启动就 OK，成功启动的界面如下。"></a>满足环境要求之后直接启动就 OK，成功启动的界面如下。</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@master apache-kylin-2.5.2-bin-hbase1x]# bin/kylin.sh start</span><br><span class="line">Retrieving hadoop conf dir...</span><br><span class="line">KYLIN_HOME is set to /data4/kylin/apache-kylin-2.5.2-bin-hbase1x</span><br><span class="line">Retrieving hive dependency...</span><br><span class="line">Retrieving hbase dependency...</span><br><span class="line">Retrieving hadoop conf dir...</span><br><span class="line">Retrieving kafka dependency...</span><br><span class="line">Retrieving Spark dependency...</span><br><span class="line">Start to check whether we need to migrate acl tables</span><br><span class="line">Retrieving hadoop conf dir...</span><br><span class="line">KYLIN_HOME is set to /data4/kylin/apache-kylin-2.5.2-bin-hbase1x</span><br><span class="line">Retrieving hive dependency...</span><br><span class="line">Retrieving hbase dependency...</span><br><span class="line">Retrieving hadoop conf dir...</span><br><span class="line">Retrieving kafka dependency...</span><br><span class="line">Retrieving Spark dependency...</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/data4/kylin/apache-kylin-2.5.2-</span><br><span class="line"></span><br><span class="line">-hbase1x/tool/kylin-tool-2.5.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/usr/hdp/2.4.2.0-258/hadoop/lib/slf4j-log4j12-</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">A new Kylin instance is started by root. To stop it, run 'kylin.sh stop'</span><br><span class="line">Check the log at /data4/kylin/apache-kylin-2.5.2-bin-hbase1x/logs/kylin.log</span><br><span class="line">Web UI is at http://&lt;hostname&gt;:7070/kylin</span><br></pre></td></tr></table></figure>

<p>看到 <code>Web UI is at http://&lt;hostname&gt;:7070/kylin</code> 即表示服务已经启动，直接前往浏览器打开登录界面，账号 ADMIN，默认密码 KYLIN。如果浏览器打不开，排查启动日志 $KYLIN_HOME/logs/kylin.log，查看具体错误原因。</p>
<p>例如，启动报错：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Invocation of init method failed; nested exception is java.lang.IllegalStateException: Cannot start job scheduler due to lack of job lock</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>既然是 lack of job ，查看 HBase Zookeeper 的元数据信息，极大可能是元数据信息冲突了，在没做集群的情况下换节点启动服务容易出现这个错误，因为默认的元数据信息都在 HBase Zookeeper 的默认nameSpace 或 根目录下。</p>
<h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><p>如果修改了部分 Kylin 配置，最好进行重启，否则有些配置在 Kylin 的 System 页面重新加载不一定生效。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;kylin.sh stop</span><br><span class="line">bin&#x2F;kylin.sh start</span><br></pre></td></tr></table></figure>

<p>在重启后需要在 System 页面 重新加载配置和元数据信息。</p>
<h4 id="样例-Cube"><a href="#样例-Cube" class="headerlink" title="样例 Cube"></a>样例 Cube</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">KYLIN_HOME/bin/sample.sh</span></span><br></pre></td></tr></table></figure>





<h3 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h3><ul>
<li><p>SQL 接口</p>
<p>标准查询 SQL 支持</p>
</li>
<li><p>支持海量数据集，增量更新</p>
<p>不会随着数据量的增长导致查询时间线性增长，查询时间只跟维度的个数和基数</p>
</li>
<li><p>亚秒级响应</p>
<p>因为预计算，所以查询时间就是一个查询结果的时间</p>
</li>
<li><p>水平扩展</p>
<p>扩展查询性能，预计算性能扩展需要根据底层计算引擎分别扩展</p>
</li>
<li><p>可视化集成</p>
<p>提供了 ODBC/JDBC 接口和 RESTful API ，方便集成</p>
</li>
<li><p><strong>计算引擎可插拔</strong></p>
<p>目前主要的两个引擎是 MR 和 Spark</p>
</li>
<li><p><strong>存储后端可插拔</strong></p>
<p>目前主要是 基于HBase 的查询后端，也可以基于 Druid 等。</p>
</li>
</ul>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li><p>不支持随机和明细查询</p>
<p>因为预计算的都是多维度的聚合结果，因为随机和明细的查询是不支持的</p>
</li>
<li><p>只支持星型模型</p>
<p>雪花模型和星座模型可以转为星型模型</p>
</li>
<li><p>空间换时间</p>
<p>因为预计算的原因，结果数据会因为维度和度量的个数和基数而直线膨胀</p>
</li>
<li><p>数据结构发生变化需要重新 build cube</p>
<p>因为预计算的原因，导致历史结果数据无法更新只能重写覆盖</p>
</li>
</ul>
<h2 id="Kylin-将hive-client的计算引擎换成-SparkSQL"><a href="#Kylin-将hive-client的计算引擎换成-SparkSQL" class="headerlink" title="Kylin 将hive client的计算引擎换成 SparkSQL"></a>Kylin 将hive client的计算引擎换成 SparkSQL</h2><p>正常来说，Hive 默认的任务引擎是 MR，如果需要将 MR 改成 SparkSQL 时，对配置进行一些改变</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 打开 spark-sql</span></span><br><span class="line">kylin.source.hive.enable-sparksql-for-table-ops=true</span><br><span class="line"><span class="meta">#</span><span class="bash"> 必须指定对应 spark 集群的 beeline 路径</span></span><br><span class="line">kylin.source.hive.sparksql-beeline-shell=/data/spark/spark-2.1.1-bin-hadoop2.7/bin/beeline</span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定 beeline 连接参数，注意 hive 集群的参数</span></span><br><span class="line">kylin.source.hive.sparksql-beeline-params=-n root --hiveconf hive.security.authorization.sqlstd.confwhitelist.append='mapreduce.job.*|dfs.*' -u jdbc:hive2://master:10000</span><br></pre></td></tr></table></figure>



<h2 id="关于队列设置"><a href="#关于队列设置" class="headerlink" title="关于队列设置"></a>关于队列设置</h2><p>（1）默认配置文件，通过配置文件($KYLIN_HOME/conf/kylin_job_conf.xml)设置队列：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.job.queuename&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;default&lt;&#x2F;value&gt;</span><br><span class="line">        &lt;description&gt;Job queue&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>

<p>Hive配置使用单独的配置文件kylin_hive_conf.xml，kylin_hive_conf.xml是kylin提交任务到hive的配置文件</p>
<p>（2）cube级别重载</p>
<p>如果某一个任务想要单独配置队列（例如放在一个较大的队列加快Build速度），可以通过Cube的属性重载默认的配置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;kylin.job.mr.config.override.mapreduce.job.queuename&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;kylin&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;description&gt;cube level override queue&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>

<p>如果想要重载hive队列：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;kylin.hive.config.override.mapreduce.job.queuename&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;kylin&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;description&gt;override hive queue&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>



<h2 id="Kylin-增加-Spark-任务引擎"><a href="#Kylin-增加-Spark-任务引擎" class="headerlink" title="Kylin 增加 Spark 任务引擎"></a>Kylin 增加 Spark 任务引擎</h2><p>因为目前spark集群时独立于 hdp 之外的 standalone 模式，所以相关 yarn的配置无所谓 。</p>
<p>进行配置修改：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 指定 hadoop_conf_dir</span></span><br><span class="line">kylin.env.hadoop-conf-dir=/etc/hadoop/conf</span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定 spark 集群</span></span><br><span class="line">kylin.engine.spark-conf.spark.master=spark://hadoopha:7077</span><br><span class="line"><span class="meta">#</span><span class="bash"> spark 任务模式</span></span><br><span class="line">kylin.engine.spark-conf.spark.submit.deployMode=cluster</span><br><span class="line"><span class="meta">#</span><span class="bash"> yarn 队列，因为跟 yarn 无关，这里无所谓</span></span><br><span class="line">kylin.engine.spark-conf.spark.yarn.queue=default</span><br><span class="line">kylin.engine.spark-conf.spark.yarn.executor.memoryOverhead=1024</span><br><span class="line">kylin.engine.spark-conf.spark.hadoop.yarn.timeline-service.enabled=false</span><br><span class="line"><span class="meta">#</span><span class="bash"> spark 常规配置</span></span><br><span class="line">kylin.engine.spark-conf.spark.driver.memory=2G</span><br><span class="line">kylin.engine.spark-conf.spark.executor.memory=4G</span><br><span class="line">kylin.engine.spark-conf.spark.executor.instances=40</span><br><span class="line">kylin.engine.spark-conf.spark.shuffle.service.enabled=true</span><br><span class="line">kylin.engine.spark-conf.spark.eventLog.enabled=true</span><br><span class="line">kylin.engine.spark-conf.spark.eventLog.dir=hdfs\://bbkhd/apps/kylin/spark-history</span><br><span class="line">kylin.engine.spark-conf.spark.history.fs.logDirectory=hdfs\://bbkhd/apps/kylin/spark-history</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">### Spark conf for specific job</span></span></span><br><span class="line">kylin.engine.spark-conf-mergedict.spark.executor.memory=6G</span><br><span class="line">kylin.engine.spark-conf-mergedict.spark.memory.fraction=0.2</span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># manually upload spark-assembly jar to HDFS and then set this property will avoid repeatedly uploading jar at runtime</span></span></span><br><span class="line">kylin.engine.spark-conf.spark.yarn.archive=hdfs://bbkhd/apps/kylin/spark/spark-libs.jar</span><br><span class="line">kylin.engine.spark-conf.spark.io.compression.codec=org.apache.spark.io.SnappyCompressionCodec</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Apache-Kylin</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>实时数据分析</tag>
        <tag>Kylin</tag>
      </tags>
  </entry>
  <entry>
    <title>大数据实时多维分析工具选型</title>
    <url>/2020/08/01/Apache-Kylin/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%A4%9A%E7%BB%B4%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7%E9%80%89%E5%9E%8B/</url>
    <content><![CDATA[<h3 id="Apache-Kylin"><a href="#Apache-Kylin" class="headerlink" title="Apache Kylin"></a>Apache Kylin</h3><p>一个采用多维立方体预计算技术的 <code>OLAP</code> <strong>引擎框架</strong>，简单来说就是做大数据查询的。</p>
<p><code>Apache Kylin</code> 的工作原理本质上是 <code>MOLAP</code>（Multidimensional Online Analytical Processing）Cube，也就是多维立方体分析。</p>
<h6 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h6><ul>
<li><p>SQL 接口</p>
<p>标准查询 SQL 支持</p>
</li>
<li><p>支持海量数据集，增量更新</p>
<p>不会随着数据量的增长导致查询时间线性增长，查询时间只跟维度的个数和基数</p>
</li>
<li><p>亚秒级响应</p>
<p>因为预计算，所以查询时间就是一个查询结果的时间</p>
</li>
<li><p>水平扩展</p>
<p>扩展查询性能，预计算性能扩展需要根据底层计算引擎分别扩展</p>
</li>
<li><p>可视化集成</p>
<p>提供了 ODBC/JDBC 接口和 RESTful API ，方便集成</p>
</li>
<li><p><strong>计算引擎可插拔</strong></p>
<p>目前主要的两个引擎是 MR 和 Spark</p>
</li>
<li><p><strong>存储后端可插拔</strong></p>
<p>目前主要是 基于HBase 的查询后端，也可以基于 Druid 等。</p>
</li>
</ul>
<h6 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h6><ul>
<li><p>不支持随机和明细查询</p>
<p>因为预计算的都是多维度的聚合结果，因为随机和明细的查询是不支持的</p>
</li>
<li><p>只支持星型模型</p>
<p>雪花模型和星座模型可以转为星型模型</p>
</li>
<li><p>空间换时间</p>
<p>因为预计算的原因，结果数据会因为维度和度量的个数和基数而直线膨胀</p>
</li>
<li><p>数据结构发生变化需要重新 build cube</p>
<p>因为预计算的原因，导致历史结果数据无法更新只能重写覆盖</p>
</li>
</ul>
<h3 id="Apache-Druid"><a href="#Apache-Druid" class="headerlink" title="Apache Druid"></a>Apache Druid</h3><p>是一个为在大数据集之上做实时统计分析而设计的开源数据存储。这个系统集合了一个面向列存储的层，一个分布式、shared-nothing的架构，和一个高级的索引结构，来达成在秒级以内对十亿行级别的表进行任意的探索分析。</p>
<h6 id="特性-1"><a href="#特性-1" class="headerlink" title="特性"></a>特性</h6><ul>
<li>亚秒级响应</li>
<li>实时导入-kafka</li>
<li>支持复杂的聚合</li>
<li>不支持大表join</li>
<li>lookup功能可以join维表</li>
<li>预计算-使用 MR 导入数据</li>
</ul>
<h6 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h6><ul>
<li><p>集群复杂</p>
<p>各个节点的配置太多，难以管理</p>
</li>
<li><p>SQL支持不完善</p>
</li>
<li><p>集成 BI 只能 superset 等几个</p>
</li>
<li><p>无法精确计算</p>
</li>
</ul>
<h3 id="Presto"><a href="#Presto" class="headerlink" title="Presto"></a>Presto</h3><h6 id="特性-2"><a href="#特性-2" class="headerlink" title="特性"></a>特性</h6><ul>
<li><p>多数据源</p>
<p>MySQL、Hive、Kafka、PostgreSQL、Cassandra等多种connector，并且支持分库分表。</p>
</li>
<li><p>支持 SQL</p>
<p>完全支持标准 SQL</p>
</li>
<li><p>扩展性强</p>
<p>很容易自定义特定数据源的 connector</p>
</li>
<li><p>混合计算</p>
<p>同一个类型数据源可以配多个 catalog，多个catalog之间能直接进行 join 查询和计算</p>
</li>
<li><p>高性能</p>
<p>平均性能是 Hive 的 10 倍。</p>
</li>
<li><p>BI 集成</p>
<p>ODBC和JDBC进行 BI 集成，无 REST API</p>
</li>
</ul>
<h6 id="缺点-2"><a href="#缺点-2" class="headerlink" title="缺点"></a>缺点</h6><ul>
<li><p>性能不稳定，延迟不稳定</p>
<p>受数据量和查询维度的影响较大，到达一定数量级，性能直线下降。</p>
</li>
<li><p>并发不高</p>
<p>受内存资源影响，查询并发不够高</p>
</li>
</ul>
<h2 id="选型要求"><a href="#选型要求" class="headerlink" title="选型要求"></a>选型要求</h2><p>总体来说，多维分析有两类场景：</p>
<ul>
<li><p>随机查询：临时性的数据分析需求，手写 SQL，形式多变，逻辑也不定，这种场景一般对响应时间没有具体要求，越快越好</p>
</li>
<li><p>固化查询：已经固定的维度分析以及报表分析等场景，对响应时间要求高，聚合查询的 SQL 较多</p>
</li>
</ul>
<h6 id="随机查询选型"><a href="#随机查询选型" class="headerlink" title="随机查询选型"></a>随机查询选型</h6><p>正常情况下，随机查询没有捷径可走。只能通过优化计算来提高查询速度，同时保证数据准确度和用户友好即SQL支持好，因此，此类场景下的选型比较简单，以下选择：</p>
<ul>
<li><p>Hive</p>
<p>原生的Hadoop生态的数据仓库，相对来说，用于存数据比较好，查询用MR计算，比较耗时。</p>
</li>
<li><p>SparkSQL</p>
<p>Spark体系的，完全支持读 Hive数据，内存计算，内存不够磁盘来凑。</p>
</li>
<li><p>Presto</p>
<p>纯内存计算，并发读不够，支持跨数据源 join</p>
</li>
<li><p>Impala</p>
<p>CDH粘合度较高，不支持跨库查询</p>
</li>
</ul>
<h6 id="固化查询选型"><a href="#固化查询选型" class="headerlink" title="固化查询选型"></a>固化查询选型</h6><p>一般来说，适用固化查询的有两类，一个是MPP架构，一个是基于预计算的。在 MPP架构方面，Impala 和 Presto 等性能还不错，但是随着数据量的增加，查询性能直线下降。预计算方面，只要有 Kylin 和 Druid 两个开源组件，各有优劣。通过表格来进行比较。</p>
<table>
<thead>
<tr>
<th align="center">对比项</th>
<th align="center">Presto</th>
<th align="center">Apache Druid</th>
<th align="center">SparkSQL</th>
<th align="center">Apache Kylin</th>
</tr>
</thead>
<tbody><tr>
<td align="center">亚秒级响应</td>
<td align="center">N</td>
<td align="center">Y</td>
<td align="center">N</td>
<td align="center">Y</td>
</tr>
<tr>
<td align="center">高并发</td>
<td align="center">N</td>
<td align="center">Y</td>
<td align="center">N</td>
<td align="center">Y</td>
</tr>
<tr>
<td align="center">百亿数据集</td>
<td align="center">N</td>
<td align="center">Y</td>
<td align="center">Y</td>
<td align="center">Y</td>
</tr>
<tr>
<td align="center">SQL支持</td>
<td align="center">Y</td>
<td align="center">Y (接近完成)</td>
<td align="center">Y</td>
<td align="center">Y</td>
</tr>
<tr>
<td align="center">离线</td>
<td align="center">Y</td>
<td align="center">Y</td>
<td align="center">Y</td>
<td align="center">Y</td>
</tr>
<tr>
<td align="center">实时</td>
<td align="center">N</td>
<td align="center">Y</td>
<td align="center">N</td>
<td align="center">Y (接近完成)</td>
</tr>
<tr>
<td align="center">精确去重</td>
<td align="center">Y</td>
<td align="center">N</td>
<td align="center">Y</td>
<td align="center">Y</td>
</tr>
<tr>
<td align="center">明显查询</td>
<td align="center">Y</td>
<td align="center">N</td>
<td align="center">Y</td>
<td align="center">N</td>
</tr>
<tr>
<td align="center">随机查询</td>
<td align="center">Y</td>
<td align="center">N</td>
<td align="center">Y</td>
<td align="center">N</td>
</tr>
<tr>
<td align="center">模型改变</td>
<td align="center">Y</td>
<td align="center">N</td>
<td align="center">Y</td>
<td align="center">N</td>
</tr>
<tr>
<td align="center">多表 join</td>
<td align="center">Y</td>
<td align="center">N</td>
<td align="center">Y</td>
<td align="center">Y</td>
</tr>
<tr>
<td align="center">ODBC/JDBC 集成</td>
<td align="center">Y</td>
<td align="center">N</td>
<td align="center">Y</td>
<td align="center">Y</td>
</tr>
<tr>
<td align="center">Rest API for BI</td>
<td align="center">N</td>
<td align="center">Y</td>
<td align="center">N</td>
<td align="center">Y</td>
</tr>
</tbody></table>
<p>简单的测试：</p>
<p>Presto：  count 13亿数据 花了 大概半分钟，40G的环境内存没吃满</p>
<p>SparkSQL：同样数据同样SQL情况下，花了大概 十几秒，具体吃的资源忘记看了，但是应该也不大。</p>
<p>Kylin 和 Druid ：时间主要花在预计算上，加上 Druid 接入 Parquet 元数据比较麻烦， 所以就没做这块的对比。业界普遍的测试结果时，查询时 Druid 比 Kylin 快，因为 HBase 的扫描赶不上倒排索引，但是 Druid 摄取数据没有 Kylin 方便。</p>
<h6 id="环境安装和各个组件的样例测试的个人感受："><a href="#环境安装和各个组件的样例测试的个人感受：" class="headerlink" title="环境安装和各个组件的样例测试的个人感受："></a>环境安装和各个组件的样例测试的个人感受：</h6><p>Presto很简单，能连接的数据源也很多，就是响应速度和查询并发上不是很好，相对于 Druid 和 Kylin来说。</p>
<p>Kylin 真正应该说是一个 OLAP引擎框架，因为它的Cube构建引擎、预计算引擎、结果存储后端都可以切换。目前构建引擎支持 Hive，SparkSQL。预计算引擎支持 MR、Spark。存储后端支持 HBase、Druid等。</p>
<p>Druid，内部组件有点多，六七个好像有。每个小组件都有自己不同的职责，分工明确，能提升不少效率，但是也加大了集群管理的难度。加上 SQL 支持在最近的版本在比较好的支持，前面的版本都是用 Rest 来呼叫的…还有摄取数据的方式，如果是Hadoop上的数据，都需要自己写Json脚本，然后提交任务。摄取 hadoop 上的 parquet数据还没搞定。</p>
<p>使用 Kylin 和 Druid 比较难用的就是维度和度量的选择，可能会有一些坑要慢慢填。</p>
]]></content>
      <tags>
        <tag>Apache</tag>
        <tag>实时数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title>git入门</title>
    <url>/2020/05/05/%E6%9D%82%E9%A1%B9/git%E5%85%A5%E9%97%A8/</url>
    <content><![CDATA[<h5 id="初始化仓库"><a href="#初始化仓库" class="headerlink" title="初始化仓库"></a>初始化仓库</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git init</span><br></pre></td></tr></table></figure>

<h5 id="查看修改用户名和用户邮箱"><a href="#查看修改用户名和用户邮箱" class="headerlink" title="查看修改用户名和用户邮箱"></a>查看修改用户名和用户邮箱</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git --global user.name &quot;your name&quot;</span><br><span class="line"></span><br><span class="line">git --global user.email &quot;youe email&quot;</span><br></pre></td></tr></table></figure>

<h5 id="提交文件到仓库"><a href="#提交文件到仓库" class="headerlink" title="提交文件到仓库"></a>提交文件到仓库</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git add file1 file2 file3 </span><br><span class="line"></span><br><span class="line">git commit -m &lt;message&gt;</span><br></pre></td></tr></table></figure>

<h5 id="工作区状态："><a href="#工作区状态：" class="headerlink" title="工作区状态："></a>工作区状态：</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git status</span><br></pre></td></tr></table></figure>

<h5 id="查看修改内容："><a href="#查看修改内容：" class="headerlink" title="查看修改内容："></a>查看修改内容：</h5><p>（文本文件可以知道修改了什么，其他文件只能知道修改了，但是修改了什么不知道）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git diff files1</span><br></pre></td></tr></table></figure>

<h5 id="显示提交日志："><a href="#显示提交日志：" class="headerlink" title="显示提交日志："></a>显示提交日志：</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git log</span><br><span class="line"></span><br><span class="line">git log --pretty&#x3D;oneline</span><br></pre></td></tr></table></figure>

<h5 id="版本回退："><a href="#版本回退：" class="headerlink" title="版本回退："></a>版本回退：</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git reset --hard HEAD~100               数字为上N个版本</span><br><span class="line"></span><br><span class="line">git reset --hard commit_id                直接去版本号所属版本</span><br></pre></td></tr></table></figure>

<h5 id="历史命令："><a href="#历史命令：" class="headerlink" title="历史命令："></a>历史命令：</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git reflog</span><br></pre></td></tr></table></figure>

<h5 id="撤销修改："><a href="#撤销修改：" class="headerlink" title="撤销修改："></a>撤销修改：</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git checkout -- filename            撤销 工作区 修改</span><br><span class="line"></span><br><span class="line">git reset HEAD -- filename        将提交到 暂存区 的修改会退到 工作区</span><br><span class="line"></span><br><span class="line">git reset --hard HEAD~N           将 已提交的修改 回退 到 数字为上N个版本</span><br></pre></td></tr></table></figure>

<h5 id="删除文件"><a href="#删除文件" class="headerlink" title="删除文件"></a>删除文件</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git rm filename</span><br></pre></td></tr></table></figure>

<p>手动 rm filename 之后，git rm filename 和git add filename 效果一样。</p>
<p>最后提交 git commit -m <message></p>
<h5 id="远程仓库-GitHub"><a href="#远程仓库-GitHub" class="headerlink" title="远程仓库 GitHub"></a>远程仓库 GitHub</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git remote add origin git@server-name:path&#x2F;repo-name.git</span><br><span class="line"></span><br><span class="line">git push -u origin master</span><br><span class="line"></span><br><span class="line">git push origin master</span><br></pre></td></tr></table></figure>

<h5 id="克隆远程仓库"><a href="#克隆远程仓库" class="headerlink" title="克隆远程仓库"></a>克隆远程仓库</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone git@github.com:text&#x2F;gitskills.git （快）</span><br><span class="line"></span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;text&#x2F;gitskills.git</span><br></pre></td></tr></table></figure>

<h5 id="查看分支"><a href="#查看分支" class="headerlink" title="查看分支"></a>查看分支</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git branch</span><br></pre></td></tr></table></figure>

<h5 id="创建并切换branch"><a href="#创建并切换branch" class="headerlink" title="创建并切换branch"></a>创建并切换branch</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git checkout -b dev    &lt;&#x3D;&#x3D;&gt;  git branch dev      创建</span><br><span class="line"></span><br><span class="line">git checkout dev                 切换</span><br></pre></td></tr></table></figure>

<h5 id="分支-branch-合并"><a href="#分支-branch-合并" class="headerlink" title="分支(branch)合并"></a>分支(branch)合并</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git merge dev</span><br><span class="line"></span><br><span class="line">git merge --no-ff -m &quot;XXXXXX&quot; dev</span><br><span class="line"></span><br><span class="line">git branch -d dev                 删除分支   </span><br><span class="line"></span><br><span class="line">git log --graph --pretty&#x3D;oneline --abbrev-commit      查看分支合并情况</span><br></pre></td></tr></table></figure>

<h5 id="分支策略"><a href="#分支策略" class="headerlink" title="分支策略"></a><strong>分支策略</strong></h5><p>在实际开发中，我们应该按照几个基本原则进行分支管理：</p>
<p>首先，master分支应该是非常稳定的，也就是仅用来发布新版本，平时不能在上面干活；</p>
<p>那在哪干活呢？干活都在dev分支上，也就是说，dev分支是不稳定的，到某个时候，比如1.0版本发布时，再把dev分支合并到master上，在master分支发布1.0版本；</p>
<p>你和你的小伙伴们每个人都在dev分支上干活，每个人都有自己的分支，时不时地往dev分支上合并就可以了。</p>
<p>所以，团队合作的分支看起来就像这样：</p>
<p><img src="F:%5CmyGit%5CDT-Learner%5C%E6%9D%82%E9%A1%B9%5Cimage-20200623144306682.png" alt="image-20200623144306682"></p>
<h5 id="保存现场，处理其他分支任务"><a href="#保存现场，处理其他分支任务" class="headerlink" title="保存现场，处理其他分支任务"></a>保存现场，处理其他分支任务</h5><p>git stash</p>
<p>git stash list </p>
<p>git stash pop  &lt;==&gt; git stash apply 恢复  git stash drop 删除</p>
<p><strong><em>开发新feature，新建分支，便于后期合并删除</em></strong></p>
<h5 id="强制删除未合并的分区："><a href="#强制删除未合并的分区：" class="headerlink" title="强制删除未合并的分区："></a>强制删除未合并的分区：</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git branch -D branchName</span><br></pre></td></tr></table></figure>

<h5 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h5><ul>
<li>查看远程库信息，使用git remote -v；</li>
<li>本地新建的分支如果不推送到远程，对其他人就是不可见的；</li>
<li>从本地推送分支，使用git push origin branch-name，如果推送失败，先用git pull抓取远程的新提交；</li>
<li>在本地创建和远程分支对应的分支，使用git checkout -b branch-name origin/branch-name，本地和远程分支的名称最好一致；</li>
<li>建立本地分支和远程分支的关联，使用git branch –set-upstream branch-name origin/branch-name；</li>
<li>从远程抓取分支，使用git pull，如果有冲突，要先处理冲突。</li>
</ul>
<h5 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h5><ul>
<li><p>命令git tag <tagname>用于新建一个标签，默认为HEAD，也可以指定一个commit id；</p>
</li>
<li><p>命令git tag -a <tagname> -m “blablabla…”可以指定标签信息；</p>
</li>
<li><p>命令git tag可以查看所有标签。</p>
</li>
<li><p>命令git push origin <tagname>可以推送一个本地标签；</p>
</li>
<li><p>命令git push origin –tags可以推送全部未推送过的本地标签；</p>
</li>
<li><p>命令git tag -d <tagname>可以删除一个本地标签；</p>
</li>
<li><p>命令git push origin :refs/tags/<tagname>可以删除一个远程标签。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>杂项</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title>日常脚本</title>
    <url>/2020/05/01/%E6%9D%82%E9%A1%B9/%E6%97%A5%E5%B8%B8%E8%84%9A%E6%9C%AC/</url>
    <content><![CDATA[<h4 id="redis读取所有key和Value"><a href="#redis读取所有key和Value" class="headerlink" title="redis读取所有key和Value"></a>redis读取所有key和Value</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./redis-cli -h 127.0.0.1 -p 6401 keys "*" | awk '&#123;printf $0 ",";system("/data/redis/redis-cli -h 127.0.0.1 -p 6401 get " $0);&#125;'</span><br></pre></td></tr></table></figure>



<h4 id="测试磁盘是否能正常读写"><a href="#测试磁盘是否能正常读写" class="headerlink" title="测试磁盘是否能正常读写"></a>测试磁盘是否能正常读写</h4><p><code>for i in {1..12};do echo &#39;test&#39; &gt; /data${i}/test ; done;</code></p>
<h4 id="发送配置简单脚本"><a href="#发送配置简单脚本" class="headerlink" title="发送配置简单脚本"></a>发送配置简单脚本</h4><p><code>for i in {1..9}; do scp /etc/hosts root@flink00${i}:/etc/  ; done;</code></p>
<p><code>while read line ; do if [[ $line == node*  ]]; then  scp /data/spark/spark-2.1.1-bin-hadoop2.7/conf/spark-env.sh root@$line:/data/spark/spark-2.1.1-bin-hadoop2.7/conf/;  fi; done &lt; slaves ;</code></p>
<h4 id="根据-hosts-中-ip-建立免密"><a href="#根据-hosts-中-ip-建立免密" class="headerlink" title="根据 hosts 中 ip 建立免密"></a>根据 hosts 中 ip 建立免密</h4><p><code>while read line ;  do if [[ $line == 172*  ]];  then  host=</code>echo ${line} | cut -d ‘ ‘  -f 2<code>; ssh-copy-id root@$host ; fi ;   done  &lt; /etc/hosts ;</code></p>
<h4 id="根据-hosts-中ip-分发-hosts"><a href="#根据-hosts-中ip-分发-hosts" class="headerlink" title="根据 hosts 中ip 分发 hosts"></a>根据 hosts 中ip 分发 hosts</h4><p><code>while read line ; do for var in $line ; do if [[ $var == 172*  ]]; then scp /etc/hosts  root@$var:/etc/ ; fi; done;  done &lt; /etc/hosts ;</code></p>
<h4 id="openssl-升级指南"><a href="#openssl-升级指南" class="headerlink" title="openssl 升级指南"></a>openssl 升级指南</h4><p><a href="https://blog.csdn.net/lw545034502/article/details/102617250" target="_blank" rel="noopener">https://blog.csdn.net/lw545034502/article/details/102617250</a>  </p>
<p>下载：wget https:<em>//github.com/openssl/openssl/archive/OpenSSL_1_1_1-stable.zip</em></p>
<p>解压：unzip OpenSSL_1_1_1-stable.zip</p>
<p>编译：./config –prefix=/usr/local/openssl</p>
<p>安装：make &amp;&amp; make install</p>
<p>备份：</p>
<ul>
<li>mv /usr/bin/openssl /usr/bin/openssl.old</li>
<li>mv /usr/lib64/openssl /usr/lib64/openssl.old</li>
<li>mv /usr/lib64/libssl.so /usr/lib64/libssl.so.old</li>
</ul>
<p>新版：</p>
<ul>
<li>ln -s /usr/local/openssl/bin/openssl /usr/bin/openssl</li>
<li>ln -s /usr/local/openssl/include/openssl /usr/include/openssl</li>
<li>ln -s /usr/local/openssl/lib/libssl.so /usr/lib64/libssl.so</li>
</ul>
<p>更新动态链接库数据：</p>
<p>echo “/usr/local/openssl/lib” &gt;&gt; /etc/ld.so.conf</p>
<p>ldconfig -v</p>
<h4 id="python3-7-安装"><a href="#python3-7-安装" class="headerlink" title="python3.7 安装"></a>python3.7 安装</h4><p><a href="https://blog.51cto.com/jinkcloud/2411644?source=dra" target="_blank" rel="noopener">https://blog.51cto.com/jinkcloud/2411644?source=dra</a></p>
<p>python3支持openssl版本最低为1.0.2，而系统比较老，自带的openssl版本为1.0.2</p>
<h6 id="安装新版openssl"><a href="#安装新版openssl" class="headerlink" title="安装新版openssl"></a>安装新版openssl</h6><ol>
<li><p>下载<br><a href="https://www.openssl.org/source/" target="_blank" rel="noopener">https://www.openssl.org/source/</a></p>
</li>
<li><p>安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir &#x2F;usr&#x2F;local&#x2F;openssl</span><br><span class="line">tar xf openssl-1.1.1c.tar.gz</span><br><span class="line">cd openssl-1.1.1c&#x2F;</span><br><span class="line">.&#x2F;config --prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;openssl shared zlib</span><br><span class="line">make &amp;&amp; make install</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置共享库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo &#39;export LD_LIBRARY_PATH&#x3D;$LD_LIBRARY_PATH:&#x2F;usr&#x2F;local&#x2F;openssl&#x2F;lib&#39; &gt;&gt; ~&#x2F;.bash_profile</span><br><span class="line">   </span><br><span class="line">   ~&#x2F;.bash_profile</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h6 id="安装python3"><a href="#安装python3" class="headerlink" title="安装python3"></a>安装python3</h6><ol>
<li><p>下载<br><a href="https://www.python.org/downloads/source/" target="_blank" rel="noopener">https://www.python.org/downloads/source/</a></p>
</li>
<li><p>安装，指定刚刚安装的opensll</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar xf Python-3.7.3.tar.xz</span><br><span class="line">mkdir &#x2F;usr&#x2F;local&#x2F;python3</span><br><span class="line">cd Python-3.7.3&#x2F;</span><br><span class="line">.&#x2F;configure prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;python3 --with-openssl&#x3D;&#x2F;usr&#x2F;local&#x2F;openssl</span><br><span class="line"></span><br><span class="line">yum install zlib-devel</span><br><span class="line">make &amp;&amp; make install</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置python3环境变量</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi &#x2F;etc&#x2F;profile</span><br><span class="line">#配置python</span><br><span class="line">export PYTHON_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;python3</span><br><span class="line">export PATH&#x3D;$PYTHON_HOME&#x2F;bin:$PATH</span><br><span class="line">source &#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure>

<p>编译的时候能看到ssl成功加载</p>
<p>ln -s /usr/local/python3/bin/python3 /usr/bin/python3</p>
</li>
</ol>
<h6 id="验证ssl模块"><a href="#验证ssl模块" class="headerlink" title="验证ssl模块"></a>验证ssl模块</h6><p>导入模块试试</p>
]]></content>
      <categories>
        <category>杂项</category>
      </categories>
      <tags>
        <tag>Shell</tag>
        <tag>配置</tag>
      </tags>
  </entry>
  <entry>
    <title>atlas-Hook</title>
    <url>/2020/07/02/Apache-Atlas/03.Atlas%20Hook/</url>
    <content><![CDATA[<h3 id="Atlas-Hook"><a href="#Atlas-Hook" class="headerlink" title="Atlas Hook"></a>Atlas Hook</h3><p>Atlas 提供了关于 Hive、HBase、Kafka、Sqoop、Storm 和 Falcon 等组件的 Hook。</p>
<p>在安装包中已经含有 hook 和 hook-bin，不需要像官网所说，需要解压相关hook包，然后拷贝到安装目录。</p>
<h4 id="Hive-Hook"><a href="#Hive-Hook" class="headerlink" title="Hive Hook"></a>Hive Hook</h4><p>分批量导入 和 hook 实时跟踪</p>
<h6 id="批量导入-hive-元数据"><a href="#批量导入-hive-元数据" class="headerlink" title="批量导入 hive 元数据"></a>批量导入 hive 元数据</h6><p>import_hive.sh 脚本在 安装包的 bin 目录下。所以还是需要将编译后的 hook 包进行解压，拷贝此脚本。</p>
<p>直接执行会报错，少依赖包。如下三个，下载后放到 $ATLAS_HOME/hook/hive/atlas-hive-plugin-impl/ 下。</p>
<p>wget <a href="https://repo1.maven.org/maven2/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.9.8/jackson-module-jaxb-annotations-2.9.8.jar" target="_blank" rel="noopener">https://repo1.maven.org/maven2/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.9.8/jackson-module-jaxb-annotations-2.9.8.jar</a></p>
<p>wget <a href="https://repo1.maven.org/maven2/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.9.8/jackson-jaxrs-base-2.9.8.jar" target="_blank" rel="noopener">https://repo1.maven.org/maven2/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.9.8/jackson-jaxrs-base-2.9.8.jar</a></p>
<p>wget <a href="https://repo1.maven.org/maven2/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.9.8/jackson-jaxrs-json-provider-2.9.8.jar" target="_blank" rel="noopener">https://repo1.maven.org/maven2/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.9.8/jackson-jaxrs-json-provider-2.9.8.jar</a></p>
<p>之后再执行，执行日志位于 $ATLAS_HOME/logs/import_hive.log</p>
<h6 id="hook-跟踪"><a href="#hook-跟踪" class="headerlink" title="hook 跟踪"></a>hook 跟踪</h6><p>atlas 提供了对应的 hive atlas hook 来进行跟踪 hive cli 里面的操作来进行元数据的更新。配置 hook 步骤如下：</p>
<ol>
<li><p>在 hive-env.sh 里面增加如下内容：即添加 hive hook 的目录。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export HIVE_AUX_JARS_PATH&#x3D;&#x2F;data&#x2F;atlas&#x2F;apache-atlas-2.0.0&#x2F;hook&#x2F;hive</span><br></pre></td></tr></table></figure>
</li>
<li><p>在 hive-site.xml 内增加配置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;atlas.cluster.name&lt;&#x2F;name&gt;</span><br><span class="line">     &lt;value&gt;primary&lt;&#x2F;value&gt;</span><br><span class="line">   &lt;&#x2F;property&gt;</span><br><span class="line">   </span><br><span class="line">   &lt;property&gt;</span><br><span class="line">     &lt;name&gt;atlas.hook.hive.maxThreads&lt;&#x2F;name&gt;</span><br><span class="line">     &lt;value&gt;1&lt;&#x2F;value&gt;</span><br><span class="line">   &lt;&#x2F;property&gt;</span><br><span class="line">   </span><br><span class="line">   &lt;property&gt;</span><br><span class="line">     &lt;name&gt;atlas.hook.hive.minThreads&lt;&#x2F;name&gt;</span><br><span class="line">     &lt;value&gt;1&lt;&#x2F;value&gt;</span><br><span class="line">   &lt;&#x2F;property&gt;</span><br><span class="line">   </span><br><span class="line">   &lt;property&gt;</span><br><span class="line">     &lt;name&gt;atlas.rest.address&lt;&#x2F;name&gt;</span><br><span class="line">     &lt;value&gt;http:&#x2F;&#x2F;tmaster:21000&lt;&#x2F;value&gt;</span><br><span class="line">   &lt;&#x2F;property&gt;</span><br><span class="line">   </span><br><span class="line">   # 必要配置，上面四项可不配。 </span><br><span class="line">   &lt;property&gt;</span><br><span class="line">     &lt;name&gt;hive.exec.post.hooks&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.atlas.hive.hook.HiveHook&lt;&#x2F;value&gt;</span><br><span class="line">   &lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改 atlas 配置文件 atlas-application.properties。如果不修改value，那么可以不增加这些配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># hive hook 相关</span><br><span class="line">atlas.hook.hive.synchronous&#x3D;false # whether to run the hook synchronously. false recommended to avoid delays in Hive query completion. Default: false</span><br><span class="line">atlas.hook.hive.numRetries&#x3D;3      # number of retries for notification failure. Default: 3</span><br><span class="line">atlas.hook.hive.queueSize&#x3D;10000   # queue size for the threadpool. Default: 10000</span><br><span class="line">atlas.cluster.name&#x3D;primary  # clusterName to use in qualifiedName of entities. Default: primary</span><br><span class="line"></span><br><span class="line"># kafka 相关，所有的kafka相关配置加上 atlas.kafka. 前缀即可生效。</span><br><span class="line">atlas.kafka.zookeeper.connect&#x3D;                    # Zookeeper connect URL for Kafka. Example: localhost:2181</span><br><span class="line">atlas.kafka.zookeeper.connection.timeout.ms&#x3D;30000 # Zookeeper connection timeout. Default: 30000</span><br><span class="line">atlas.kafka.zookeeper.session.timeout.ms&#x3D;60000    # Zookeeper session timeout. Default: 60000</span><br><span class="line">atlas.kafka.zookeeper.sync.time.ms&#x3D;20             # Zookeeper sync time. Default: 20</span><br></pre></td></tr></table></figure>
</li>
<li><p>将 atlas-application.properties 拷贝到 hive conf 目录下，或者直接做个软链接 </p>
</li>
<li><p>其他hive节点配置，需要 <code>atlas-application.properties</code>  和 <code>export HIVE_AUX_JARS_PATH=/data/atlas/apache-atlas-2.0.0/hook/hive</code> </p>
</li>
</ol>
<h4 id="Kafka-Hook"><a href="#Kafka-Hook" class="headerlink" title="Kafka Hook"></a>Kafka Hook</h4><p>将编译后的安装包中的 apache-atlas-2.0.0-kafka-hook 中的 jar 包 目录拷贝到 atlas 安装包的 $ATLAS_HOME/hook/kafka/ 下。</p>
<p>然后 将 atlas 配置文件 打包到  $ATLAS_HOME//hook/kafka/atlas-kafka-plugin-impl/kafka-bridge-2.0.0.jar </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在 atlas-kafka-plugin-impl 目录下打包</span><br><span class="line">zip -u kafka-bridge-2.0.0.jar atlas-application.properties</span><br></pre></td></tr></table></figure>

<p>将准备好的 hook 和 hook-bin分发到 Kafka 节点</p>
<p>hook-bin 下执行 import-kafka.sh</p>
<p><strong>注意点：</strong> </p>
<ol>
<li>kafka 版本会有影响，有个 zkUtils 的包，有个方法一直找不到。换个版本就好了，目测是低版本kafka 集群少了未知的包，</li>
<li>atlas 只会读取与 atlas 集成的 kafka 的 topic 。需要在导入kafka-bridge 包同级目录下增加 atlas-application.properties ，atlas.cluster.name 和 atlas.kafka.zookeeper.connect 两项分别确定 集群名和 目标kafka地址。导入时修改的这两个配置不会影响 atlas 集群中的参数。</li>
</ol>
<h4 id="HBase-Hook"><a href="#HBase-Hook" class="headerlink" title="HBase Hook"></a>HBase Hook</h4><p>提前下载缺失的包：<a href="https://mvnrepository.com/artifact/org.apache.htrace/htrace-core4/4.2.0-incubating" target="_blank" rel="noopener">https://mvnrepository.com/artifact/org.apache.htrace/htrace-core4/4.2.0-incubating</a></p>
<p>HBase-site.xml 增加配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hbase.coprocessor.master.classes&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.atlas.hbase.hook.HBaseAtlasCoprocessor&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>

<p>atlas 配置文件 atlas-application.properties 拷贝到HBase 所有节点的 config 目录。</p>
<p>批量导入时，需要增加上面下载的 jar 包 到hook 下的目录下。</p>
<p>把所有 atlas 下 HBase hook 相关的 jar 包 软链接 到 HBase 下的 lib 目录下。</p>
<p>HBase 版本问题还未解决。</p>
<p><strong>针对于 要求不同 hive 不同 HBase 导入时所对应的 qualifiedName 中 default.testhive@testHive 的 testHive 不一样。需要将不同hive 、HBase 集群 中的 atlas-application.properties 中的 atlas.cluster.name。hive不需要重启。HBase需要重启。切记，同一个集群的，要保持配置一样。</strong> </p>
<h4 id="Spark-Atlas-Connector"><a href="#Spark-Atlas-Connector" class="headerlink" title="Spark-Atlas-Connector"></a>Spark-Atlas-Connector</h4><p><strong>git下载源码</strong>：<a href="https://github.com/hortonworks-spark/spark-atlas-connector/archive/master.zip" target="_blank" rel="noopener">https://github.com/hortonworks-spark/spark-atlas-connector/archive/master.zip</a></p>
<p><strong>编译</strong>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mvn clean</span><br><span class="line">mvn package -DskipTests</span><br></pre></td></tr></table></figure>

<p><strong>创建 atlas 中 spark 相关模块：</strong></p>
<p>拷贝 <code>patch/1100-spark_model.json</code> 到 <code>$ATLAS_HOME/models/1000-Hadoop/</code> 下，然后重启 atlas 服务使之生效。</p>
<p>将 配置 文件 atlas-application.properties 拷贝到 spark conf 下，同时 spark 需要能连接上hive hadoop等。</p>
<p>启动 spark SQL 或者 spark shell 时，带上连接器的 jar 包。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;spark-sql --jars spark-atlas-connector-assembly-0.1.0-SNAPSHOT.jar --conf spark.extraListeners&#x3D;com.hortonworks.spark.atlas.SparkAtlasEventTracker --conf spark.sql.queryExecutionListeners&#x3D;com.hortonworks.spark.atlas.SparkAtlasEventTracker --conf spark.sql.streaming.streamingQueryListeners&#x3D;com.hortonworks.spark.atlas.SparkAtlasStreamingQueryEventTracker</span><br></pre></td></tr></table></figure>

<p>此连接器，只能实现 insert 相关操作，并且生成的 atlas entities 是 spark process，而不是 hive process。所以几乎对追踪 hive 元数据没作用。</p>
<p>所有就有接下来的转换思路，不用spark-sql，而是转为 使用 hive on spark。</p>
<h4 id="Hive-on-Spark"><a href="#Hive-on-Spark" class="headerlink" title="Hive on Spark"></a>Hive on Spark</h4><p>hive on spark 与 spark-sql 操作过程正好相反。spark-sql 是在spark 中连接hive元数据，然后进行计算。而 hive on spark 是，hive 底层执行引擎由 MR 修改为 Spark。</p>
<p>版本依赖：</p>
<table>
<thead>
<tr>
<th align="left">Hive Version</th>
<th align="left">Spark Version</th>
</tr>
</thead>
<tbody><tr>
<td align="left">master</td>
<td align="left">2.3.0</td>
</tr>
<tr>
<td align="left">3.0.x</td>
<td align="left">2.3.0</td>
</tr>
<tr>
<td align="left">2.3.x</td>
<td align="left">2.0.0</td>
</tr>
<tr>
<td align="left">2.2.x</td>
<td align="left">1.6.0</td>
</tr>
<tr>
<td align="left">2.1.x</td>
<td align="left">1.6.0</td>
</tr>
<tr>
<td align="left">2.0.x</td>
<td align="left">1.5.0</td>
</tr>
<tr>
<td align="left">1.2.x</td>
<td align="left">1.3.1</td>
</tr>
<tr>
<td align="left">1.1.x</td>
<td align="left">1.2.0</td>
</tr>
</tbody></table>
<h4 id="spark-sql-监听器实现"><a href="#spark-sql-监听器实现" class="headerlink" title="spark-sql 监听器实现"></a>spark-sql 监听器实现</h4><p>原理：通过listener监听到spark-sql中的每一个sql，然后将sql放入到hive session中进行解析，然后引用atlas的hive hook包 进行 sql 关系的注入，drop 操作除外。</p>
<p><em>（无法实现drop操作是因为，在spark-sql中，执行drop 后，hive metastore 中已经不存在需要drop的数据，当 hive session进行解析时会发现不存在，无法进行解析。）</em></p>
<p>代码打包：（不同的hive环境需要替换掉jar包中的hive-site.xml）</p>
<ul>
<li><code>atlas-bridges-1.0-SNAPSHOT.jar</code> </li>
<li><code>atlas-bridges-1.0-SNAPSHOT-jar-with-dependencies.jar</code> 。</li>
</ul>
<p>使用方法：将jar包放到 $SPARK_HOME/jars 目录下。</p>
<ul>
<li><p>对于 <code>atlas-bridges-1.0-SNAPSHOT.jar</code>  ，还需要在jars目录下创建 atlas-hive-hook 的相关包 的软链接</p>
</li>
<li><p>对于<code>atlas-bridges-1.0-SNAPSHOT-jar-with-dependencies.jar</code> ，只需要放入其包。</p>
<p>然后在启动spark-sql时，加上监听器参数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;spark-sql --jars atlas-bridges-1.0-SNAPSHOT.jar  --conf spark.extraListeners&#x3D;com.eebbk.atlas.bridges.sparksql.AtlasMetadataListener</span><br><span class="line">  	</span><br><span class="line">bin&#x2F;spark-sql --conf spark.extraListeners&#x3D;com.eebbk.atlas.bridges.sparksql.AtlasMetadataListener</span><br></pre></td></tr></table></figure>




</li>
</ul>
<p>listener：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package com.eebbk.atlas.bridges.sparksql;</span><br><span class="line"></span><br><span class="line">import org.apache.log4j.Logger;</span><br><span class="line">import org.apache.spark.scheduler.SparkListener;</span><br><span class="line">import org.apache.spark.scheduler.SparkListenerJobEnd;</span><br><span class="line">import org.apache.spark.scheduler.SparkListenerJobStart;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">public class AtlasMetadataListener extends SparkListener &#123;</span><br><span class="line"></span><br><span class="line">    private static final Logger log &#x3D; Logger.getLogger(AtlasMetadataListener.class);</span><br><span class="line"></span><br><span class="line">    private String command;</span><br><span class="line">    private SparkSqlLineage sparkSqlLineage &#x3D; new SparkSqlLineage();</span><br><span class="line">    private Thread thread &#x3D; new Thread(sparkSqlLineage);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void onJobEnd(SparkListenerJobEnd jobEnd) &#123;</span><br><span class="line">        if (thread.getState().compareTo(Thread.State.NEW)&#x3D;&#x3D;0)&#123;</span><br><span class="line">            thread.start();</span><br><span class="line">        &#125;</span><br><span class="line">        if (!command.toLowerCase().startsWith(&quot;drop&quot;))&#123;</span><br><span class="line">            log.info(&quot;send command to sparkSqlLineage:  &quot;+command);</span><br><span class="line">            sparkSqlLineage.addCommand(command);</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            log.info(&quot;drop operator is not support in spark-sql for apache atlas!&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void onJobStart(SparkListenerJobStart jobStart) &#123;</span><br><span class="line">        Object jobDesc &#x3D; jobStart.properties().get(&quot;spark.job.description&quot;);</span><br><span class="line">        command  &#x3D; jobDesc.toString();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>sqlLineage:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package com.eebbk.atlas.bridges.sparksql;</span><br><span class="line"></span><br><span class="line">import org.apache.atlas.hive.hook.HiveHook;</span><br><span class="line">import org.apache.hadoop.hive.common.ValidTxnList;</span><br><span class="line">import org.apache.hadoop.hive.conf.HiveConf;</span><br><span class="line">import org.apache.hadoop.hive.ql.Context;</span><br><span class="line">import org.apache.hadoop.hive.ql.QueryPlan;</span><br><span class="line">import org.apache.hadoop.hive.ql.hooks.Hook;</span><br><span class="line">import org.apache.hadoop.hive.ql.hooks.HookContext;</span><br><span class="line">import org.apache.hadoop.hive.ql.hooks.HookUtils;</span><br><span class="line">import org.apache.hadoop.hive.ql.lockmgr.LockException;</span><br><span class="line">import org.apache.hadoop.hive.ql.parse.*;</span><br><span class="line">import org.apache.hadoop.hive.ql.session.SessionState;</span><br><span class="line">import org.apache.log4j.Logger;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.util.List;</span><br><span class="line">import java.util.concurrent.LinkedBlockingQueue;</span><br><span class="line"></span><br><span class="line">public class SparkSqlLineage implements Runnable&#123;</span><br><span class="line"></span><br><span class="line">    private static final Logger log &#x3D; Logger.getLogger(SparkSqlLineage.class);</span><br><span class="line"></span><br><span class="line">    private final LinkedBlockingQueue&lt;String&gt; sqlQueue &#x3D; new LinkedBlockingQueue&lt;&gt;(100);</span><br><span class="line"></span><br><span class="line">    private HiveConf     hiveConf &#x3D; null;</span><br><span class="line">    private ParseDriver  pd       &#x3D; null;</span><br><span class="line">    private Context      context  &#x3D; null;</span><br><span class="line">    private SessionState ss       &#x3D; null;</span><br><span class="line">    private String       queryStr &#x3D; null;</span><br><span class="line">    private String       command  &#x3D; null;</span><br><span class="line"></span><br><span class="line">    public void addCommand(String command)&#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            sqlQueue.put(command);</span><br><span class="line">        &#125; catch (InterruptedException e) &#123;</span><br><span class="line">            log.error(&quot;put command to queue fail. &quot;,e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void run() &#123;</span><br><span class="line"></span><br><span class="line">        while (true)&#123;</span><br><span class="line">            try &#123;</span><br><span class="line">                command &#x3D; sqlQueue.take();</span><br><span class="line">            &#125; catch (InterruptedException e) &#123;</span><br><span class="line">                log.error(&quot;get sql command fail.&quot; + e );</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            if (command&#x3D;&#x3D;null||command.equals(&quot;end&quot;))&#123;</span><br><span class="line">                log.info(&quot;process sql thread end !&quot;);</span><br><span class="line">                break;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            if (ss&#x3D;&#x3D;null)&#123;</span><br><span class="line">                initSessionState();</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            String queryId &#x3D; QueryPlan.makeQueryId();</span><br><span class="line"></span><br><span class="line">            command &#x3D; new VariableSubstitution().substitute(hiveConf, command);</span><br><span class="line"></span><br><span class="line">            if (context &#x3D;&#x3D; null)&#123;</span><br><span class="line">                initContext();</span><br><span class="line">            &#125;</span><br><span class="line">            context.setCmd(command);</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; get tree</span><br><span class="line">            ASTNode tree &#x3D; getTree(command);</span><br><span class="line"></span><br><span class="line">            dealSessionState();</span><br><span class="line"></span><br><span class="line">            try &#123;</span><br><span class="line">                queryStr &#x3D; HookUtils.redactLogString(hiveConf, command);</span><br><span class="line">                hiveConf.set(&quot;mapreduce.workflow.name&quot;, queryStr);</span><br><span class="line">            &#125; catch (Exception e) &#123;</span><br><span class="line">                log.error(&quot;get queryStr fail.&quot; + e);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; get sem</span><br><span class="line">            BaseSemanticAnalyzer sem &#x3D; null;</span><br><span class="line">            try &#123;</span><br><span class="line">                sem &#x3D; SemanticAnalyzerFactory.get(hiveConf, tree);</span><br><span class="line">                sem.analyze(tree, context);</span><br><span class="line">                log.info(&quot;Semantic Analysis Completed&quot;);</span><br><span class="line">                sem.validate();</span><br><span class="line">            &#125; catch (SemanticException e) &#123;</span><br><span class="line">                log.error(&quot;init sem fail.&quot;+ e );</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; get query plan</span><br><span class="line">            if (sem &#x3D;&#x3D; null)&#123;</span><br><span class="line">                break;</span><br><span class="line">            &#125;</span><br><span class="line">            QueryPlan plan &#x3D; new QueryPlan(queryStr, sem, System.currentTimeMillis(), queryId, ss.getCommandType());</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; HookContext</span><br><span class="line">            HookContext hookContext &#x3D; null;</span><br><span class="line">            try &#123;</span><br><span class="line">                hookContext &#x3D; new HookContext(plan, hiveConf, context.getPathToCS(), ss.getUserName(), ss.getUserIpAddress(), queryId);</span><br><span class="line">                hookContext.setHookType(HookContext.HookType.POST_EXEC_HOOK);</span><br><span class="line">            &#125; catch (Exception e) &#123;</span><br><span class="line">                log.error(&quot;get hookContext fail.&quot; + e);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            List&lt;Hook&gt; hookList;</span><br><span class="line">            try &#123;</span><br><span class="line">                hookList &#x3D; HookUtils.getHooks(hiveConf, HiveConf.ConfVars.POSTEXECHOOKS, Hook.class);</span><br><span class="line">                for (Hook hook:hookList)&#123;</span><br><span class="line">                    if (hook instanceof HiveHook)&#123;</span><br><span class="line">                        ((HiveHook) hook).run(hookContext);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; catch (Exception e) &#123;</span><br><span class="line">                log.error(&quot;run hook fail.&quot;+ e);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private void initSessionState()&#123;</span><br><span class="line">        SessionState sessionState &#x3D; new SessionState(hiveConf &#x3D;&#x3D; null ? initHiveConf() : hiveConf);</span><br><span class="line">        SessionState.start(sessionState);</span><br><span class="line">        this.ss &#x3D; SessionState.get();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private HiveConf initHiveConf()&#123;</span><br><span class="line">        this.hiveConf &#x3D; new HiveConf();</span><br><span class="line">        return hiveConf;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private void initContext() &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            this.context &#x3D; new Context(hiveConf);</span><br><span class="line">            context.setTryCount(Integer.MAX_VALUE);</span><br><span class="line">            context.setHDFSCleanup(true);</span><br><span class="line">        &#125; catch (IOException e) &#123;</span><br><span class="line">            log.error(&quot;init context fail.&quot; + e );</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private void initParseDriver()&#123;</span><br><span class="line">        this.pd &#x3D; new ParseDriver();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private ASTNode getTree(String command)&#123;</span><br><span class="line">        ASTNode tree &#x3D; null;</span><br><span class="line">        if (pd &#x3D;&#x3D;null)&#123;</span><br><span class="line">            initParseDriver();</span><br><span class="line">        &#125;</span><br><span class="line">        try &#123;</span><br><span class="line">            tree &#x3D; pd.parse(command, context);</span><br><span class="line">            tree &#x3D; ParseUtils.findRootNonNullToken(tree);</span><br><span class="line">            log.info(&quot;parse command to tree success.&quot;);</span><br><span class="line">        &#125; catch (ParseException e) &#123;</span><br><span class="line">            log.error(&quot;parse command to tree fail. &quot; + e);</span><br><span class="line">        &#125;</span><br><span class="line">        return tree;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private void dealSessionState()&#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            ss.initTxnMgr(hiveConf);</span><br><span class="line">            ValidTxnList txns &#x3D; ss.getTxnMgr().getValidTxns();</span><br><span class="line">            String txnStr &#x3D; txns.toString();</span><br><span class="line">            hiveConf.set(ValidTxnList.VALID_TXNS_KEY, txnStr);</span><br><span class="line">        &#125; catch (LockException e) &#123;</span><br><span class="line">            log.error(&quot;deal SessionState fail.&quot; + e );</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Apache-Atlas</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Atlas</tag>
        <tag>Hbase</tag>
        <tag>Kafka</tag>
        <tag>Hive</tag>
        <tag>Spark-sql</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink简单介绍</title>
    <url>/2020/06/15/Apache-Flink/FlinkCoding/01.Flink%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<h3 id="What-Apache-Flink"><a href="#What-Apache-Flink" class="headerlink" title="What Apache Flink"></a>What Apache Flink</h3><p><em>Apache Flink 是一个<strong>分布式大数据处理引擎</strong>，可对<strong>有限数据流</strong>和<strong>无限数据流</strong>进行<strong>有状态计算</strong>。可部署在各种<strong>集群环境</strong>，对各种大小的数据规模进行快速计算。</em></p>
<h6 id="分布式大数据处理引擎"><a href="#分布式大数据处理引擎" class="headerlink" title="分布式大数据处理引擎"></a>分布式大数据处理引擎</h6><ul>
<li>是一个分布式的、高可用的用于大数据处理的计算引擎<h6 id="有限流和无限流"><a href="#有限流和无限流" class="headerlink" title="有限流和无限流"></a>有限流和无限流</h6></li>
<li>有限流：有始有终的数据流。即传统意义上的批数据，进行批处理</li>
<li>无限流：有始无终的数据流。即现实生活中的流数据，进行流处理<h6 id="有状态计算"><a href="#有状态计算" class="headerlink" title="有状态计算"></a>有状态计算</h6></li>
<li>良好的状态机制，进行较好的容错处理和任务恢复。同时实现 Exactly-Once 语义。<h6 id="各种集群环境"><a href="#各种集群环境" class="headerlink" title="各种集群环境"></a>各种集群环境</h6></li>
<li>可部署standalone、Flink on yarn、Flink on Mesos、Flink on k8s等等</li>
<li></li>
</ul>
<h3 id="Flink-Application"><a href="#Flink-Application" class="headerlink" title="Flink Application"></a>Flink Application</h3><h5 id="Streams"><a href="#Streams" class="headerlink" title="Streams"></a>Streams</h5><blockquote>
<p>数据在真实世界中是不停产生不停发出的，所以数据处理也应该还原真实，做到真正的流处理。而批处理则是流处理的特殊情况</p>
<ul>
<li>即上面说的有限流和无限流，贴官网图说明。<br><img src="https://flink.apache.org/img/bounded-unbounded.png" alt="image"></li>
</ul>
</blockquote>
<h5 id="State"><a href="#State" class="headerlink" title="State"></a>State</h5><blockquote>
<p>在流计算场景中，其实所有流计算本质上都是增量计算（Incremental Processing）。<br>例如，计算前几个小时或者一直以来的某个指标（PV、UV等），计算完一条数据之后需要保存其计算结果即状态，以便和下一条计算结果合并。<br>另外，保留计算状态，进行 CheckPoint 可以很好地实现流计算的容错和任务恢复，也可以实现Exactly Once处理语义</p>
</blockquote>
<h5 id="Time"><a href="#Time" class="headerlink" title="Time"></a>Time</h5><blockquote>
<p>三类时间：</p>
<ul>
<li>Event Time：事件真实产生的时间</li>
<li>Processing Time：事件被 Flink 程序处理的时间</li>
<li>Ingestion Time：事件进入到 Flink 程序的时间</li>
</ul>
</blockquote>
<h5 id="API"><a href="#API" class="headerlink" title="API"></a>API</h5><blockquote>
<p>API分三层，越接近SQL层，越抽象，灵活性越低，但更简单易用。</p>
<ul>
<li>SQL/Table层：直接使用SQL进行数据处理</li>
<li>DataStream/DataSet API：最核心的API，对流数据进行处理，可在其上实现自定义的WaterMark、Windows、State等操作</li>
<li>ProcessFunction：也叫RunTime层，最底层的API，带状态的事件驱动。<br><img src="https://flink.apache.org/img/api-stack.png" alt="image"></li>
</ul>
</blockquote>
<p>​    </p>
<h3 id="Flink-Architecture"><a href="#Flink-Architecture" class="headerlink" title="Flink Architecture"></a>Flink Architecture</h3><h5 id="Data-Pipeline-Applications"><a href="#Data-Pipeline-Applications" class="headerlink" title="Data Pipeline Applications"></a>Data Pipeline Applications</h5><blockquote>
<p>即 real-time Stream ETL：流式ETL拆分。<br>通常，ETL都是通过定时任务调度SQL文件或者MR任务来执行的。在实时ETL场景中，将批量ETL逻辑写到流处理中，分散计算压力和提高计算结果的实时性。<br>多用于实时数仓、实时搜索引擎等<br><img src="https://flink.apache.org/img/usecases-datapipelines.png" alt="image"></p>
</blockquote>
<h5 id="Data-Analytics-Applications"><a href="#Data-Analytics-Applications" class="headerlink" title="Data Analytics Applications"></a>Data Analytics Applications</h5><blockquote>
<p>即数据分析，包括流式数据分析和批量数据分析。例如实时报表、实时大屏。<br><img src="https://flink.apache.org/img/usecases-analytics.png" alt="image"></p>
</blockquote>
<h5 id="Event-driven-Applications"><a href="#Event-driven-Applications" class="headerlink" title="Event-driven Applications"></a>Event-driven Applications</h5><blockquote>
<p>即事件驱动应用，在一个有状态的计算过程中，通常情况下都是将状态保存在第三方系统（如Hbase Redis等）中。<br>    而在Flink中，状态是保存在内部程序中，减少了状态存取的不必要的I/O开销，更大吞吐量和更低延时。<br><img src="https://flink.apache.org/img/usecases-eventdrivenapps.png" alt="image"></p>
</blockquote>
<h3 id="第一个-Flink-程序"><a href="#第一个-Flink-程序" class="headerlink" title="第一个 Flink 程序"></a>第一个 Flink 程序</h3><h5 id="开发环境要求"><a href="#开发环境要求" class="headerlink" title="开发环境要求"></a>开发环境要求</h5><blockquote>
<p>主要是Java环境和Maven环境。Java要求JDK1.8，Maven要求3.0以上，开发工具推荐使用 ItelliJ IDEA，社区说法：Eclipse在Java和Scala混合编程下有问题，故不推荐。</p>
</blockquote>
<h5 id="代码示例："><a href="#代码示例：" class="headerlink" title="代码示例："></a>代码示例：</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> source.streamDataSource;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.TimeCharacteristic;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SocketWindowWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(args.length!=<span class="number">2</span>)&#123;</span><br><span class="line">            System.err.println(<span class="string">"Usage:\nSocketWindowWordCount hostname port"</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取程序参数</span></span><br><span class="line">        String hostname = args[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">int</span> port = Integer.parseInt(args[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 入口类，用于设置环境和参数等</span></span><br><span class="line">        StreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 设置 Time 类型</span></span><br><span class="line">        see.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 从指定 IP 端口 读取流数据，返回一个 DataStreamSource</span></span><br><span class="line">        DataStreamSource&lt;String&gt; text = see.socketTextStream(hostname, port, <span class="string">"\n"</span>, <span class="number">5</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 在 DataStreamSource 上做操作即 transformation </span></span><br><span class="line">        DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; windowCount = text</span><br><span class="line">                <span class="comment">// flatMap , FlatMap接口的实现：将获取到的数据分割，并每个元素组合成 (word, count)形式</span></span><br><span class="line">                .flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">for</span> (String word : value.split(<span class="string">"\\s"</span>)) &#123;</span><br><span class="line">                    collector.collect(Tuple2.of(word, <span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">                <span class="comment">// 按位置指定key，进行聚合操作</span></span><br><span class="line">                .keyBy(<span class="number">0</span>)</span><br><span class="line">                <span class="comment">// 指定窗口大小</span></span><br><span class="line">                .timeWindow(Time.seconds(<span class="number">5</span>))</span><br><span class="line">                <span class="comment">// 在每个key上做sum</span></span><br><span class="line">                <span class="comment">// reduce 和 sum 的实现</span></span><br><span class="line"><span class="comment">//                .reduce(new ReduceFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span></span><br><span class="line"><span class="comment">//                    @Override</span></span><br><span class="line"><span class="comment">//                    public Tuple2&lt;String, Integer&gt; reduce(Tuple2&lt;String, Integer&gt; stringIntegerTuple2, Tuple2&lt;String, Integer&gt; t1) throws Exception &#123;</span></span><br><span class="line"><span class="comment">//                        return Tuple2.of(stringIntegerTuple2.f0, stringIntegerTuple2.f1+t1.f1);</span></span><br><span class="line"><span class="comment">//                    &#125;</span></span><br><span class="line"><span class="comment">//                &#125;);</span></span><br><span class="line">                .sum(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 一个线程执行</span></span><br><span class="line">        windowCount.print().setParallelism(<span class="number">1</span>);</span><br><span class="line">        see.execute(<span class="string">"Socket Window WordCount"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 其他 transformation 操作示例</span></span><br><span class="line"><span class="comment">//        windowCount</span></span><br><span class="line"><span class="comment">//                .map(new MapFunction&lt;Tuple2&lt;String,Integer&gt;, String&gt;() &#123;</span></span><br><span class="line"><span class="comment">//                    @Override</span></span><br><span class="line"><span class="comment">//                    public String map(Tuple2&lt;String, Integer&gt; stringIntegerTuple2) throws Exception &#123;</span></span><br><span class="line"><span class="comment">//                        return stringIntegerTuple2.f0;</span></span><br><span class="line"><span class="comment">//                    &#125;</span></span><br><span class="line"><span class="comment">//                &#125;)</span></span><br><span class="line"><span class="comment">//                .print();</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//        text.filter(new FilterFunction&lt;String&gt;() &#123;</span></span><br><span class="line"><span class="comment">//            @Override</span></span><br><span class="line"><span class="comment">//            public boolean filter(String s) throws Exception &#123;</span></span><br><span class="line"><span class="comment">//                return s.contains("h");</span></span><br><span class="line"><span class="comment">//            &#125;</span></span><br><span class="line"><span class="comment">//        &#125;)</span></span><br><span class="line"><span class="comment">//                .print();</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//        SplitStream&lt;String&gt; split = text.split(new OutputSelector&lt;String&gt;() &#123;</span></span><br><span class="line"><span class="comment">//            @Override</span></span><br><span class="line"><span class="comment">//            public Iterable&lt;String&gt; select(String value) &#123;</span></span><br><span class="line"><span class="comment">//                ArrayList&lt;String&gt; strings = new ArrayList&lt;&gt;();</span></span><br><span class="line"><span class="comment">//                if (value.contains("h"))</span></span><br><span class="line"><span class="comment">//                    strings.add("hadoop");</span></span><br><span class="line"><span class="comment">//                else</span></span><br><span class="line"><span class="comment">//                    strings.add("noHadoop");</span></span><br><span class="line"><span class="comment">//                return strings;</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//            &#125;</span></span><br><span class="line"><span class="comment">//        &#125;);</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//        split.select("hadoop").print();</span></span><br><span class="line"><span class="comment">//        split.select("noHadoop").map(new MapFunction&lt;String, String&gt;() &#123;</span></span><br><span class="line"><span class="comment">//            @Override</span></span><br><span class="line"><span class="comment">//            public String map(String s) throws Exception &#123;</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//                return s.toUpperCase();</span></span><br><span class="line"><span class="comment">//            &#125;</span></span><br><span class="line"><span class="comment">//        &#125;).print();</span></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Apache-Flink</category>
        <category>FlinkCoding</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink DataStream API介绍和示例</title>
    <url>/2020/06/15/Apache-Flink/FlinkCoding/02.DataStream%20API%E4%BB%8B%E7%BB%8D%E5%92%8C%E7%A4%BA%E4%BE%8B/</url>
    <content><![CDATA[<h1 id="DataStream-API介绍和示例"><a href="#DataStream-API介绍和示例" class="headerlink" title="DataStream API介绍和示例"></a>DataStream API介绍和示例</h1><h4 id="Flink程序运行流程"><a href="#Flink程序运行流程" class="headerlink" title="Flink程序运行流程"></a>Flink程序运行流程</h4><h6 id="1-获取执行环境"><a href="#1-获取执行环境" class="headerlink" title="1. 获取执行环境"></a>1. 获取执行环境</h6><blockquote>
<p>getExecutionEnvironment()  </p>
<p>createLocalEnvironment()  </p>
<p>createRemoteEnvironment(String host, int port, String… jarFiles) </p>
</blockquote>
<h6 id="2-加载创建初始化数据"><a href="#2-加载创建初始化数据" class="headerlink" title="2. 加载创建初始化数据"></a>2. 加载创建初始化数据</h6><blockquote>
<p>readTextFile()  </p>
<p>addSource  </p>
<p>..</p>
</blockquote>
<h6 id="3-对数据在transformation-operator"><a href="#3-对数据在transformation-operator" class="headerlink" title="3. 对数据在transformation operator"></a>3. 对数据在transformation operator</h6><blockquote>
<p>map  </p>
<p>flatMap  </p>
<p>filter  </p>
<p>..</p>
</blockquote>
<h6 id="4-指定计算结果的输出位置-sink"><a href="#4-指定计算结果的输出位置-sink" class="headerlink" title="4. 指定计算结果的输出位置 sink"></a>4. 指定计算结果的输出位置 sink</h6><blockquote>
<p>print()  </p>
<p>writeAdText(String path)  </p>
<p>addSink  </p>
<p>..</p>
</blockquote>
<h6 id="5-触发程序执行-execute"><a href="#5-触发程序执行-execute" class="headerlink" title="5. 触发程序执行 execute"></a>5. 触发程序执行 execute</h6><blockquote>
<p>env.execute()  </p>
<p>在sink是print时，不需要显示execute，否则会报错。因为在print方法里已经默认调用了execute。</p>
</blockquote>
<h2 id="StreamExecutionEnvironment"><a href="#StreamExecutionEnvironment" class="headerlink" title="StreamExecutionEnvironment"></a>StreamExecutionEnvironment</h2><p>StreamExecutionEnvironment 作为程序入口context，有两类：LocalStreamEnvironment(本地环境) 和RemoteStreamEnvironment(远程环境)。<br>ExecutionConfig、CheckpointConfig等配置均在这里初始化。另外，这里也能设置线程数，检查点周期，以及检查点模式。还有状态后端序列化类型以及注册Type等。  </p>
<ul>
<li>如果集群是standalone模式，则StreamExecutionEnvironment.getExecutionEnvironment() 相当于StreamExecutionEnvironment.createLocalEnvironment()</li>
</ul>
<h2 id="DataStream-Source"><a href="#DataStream-Source" class="headerlink" title="DataStream Source"></a>DataStream Source</h2><h6 id="基于文件的"><a href="#基于文件的" class="headerlink" title="基于文件的"></a>基于文件的</h6><blockquote>
<ul>
<li>readTextFile(String path)    charsetName 默认用 UTF-8  </li>
<li>readTextFile(String path, String charsetName)：文本文件，格式为 TextInputFormat，返回   BasicTypeInfo.STRING_TYPE_INFO  ，TextInputFormat对象调用 setCharsetName(charsetName) 设置字符  ，然后底层再调用 readFile 方法。  </li>
<li>readFile(FileInputFormat<OUT> inputFormat,   String filePath,   FileProcessingMode watchType,   long interval,   TypeInformation<OUT> typeInformation)：根据给定格式和路径读取文件，根据watchType（FileProcessingMode.PROCESS_ONCE：处理一次路径文件后退出，FileProcessingMode.PROCESS_CONTINUOUSLY：检测给定路径的新数据，此时若旧文件发生修改也会重读，不符合exactly-once），interval 扫描路径的周期。调用 createFileInput ，createFileInput 调用 addSource 。</li>
</ul>
</blockquote>
<h6 id="Socket流"><a href="#Socket流" class="headerlink" title="Socket流"></a>Socket流</h6><blockquote>
<ul>
<li>socketTextStream(hostname , port)  // 主机，端口号，字段分隔符 delimiter 默认为 \n</li>
<li>socketTextStream(hostname , port , delimiter)   // maxRetry 默认为零</li>
<li>socketTextStream(hostname, port, delimiter, maxRetry )<br>maxRetry: 当socket端挂掉是，程序等待的最大重试时间。每秒都会重试连接，为0即停止程序…。利用 SocketTextStreamFunction 生成 sourceFunction对象，调用 addSource 生成DataStreamSource</li>
</ul>
</blockquote>
<h6 id="基于数据集的"><a href="#基于数据集的" class="headerlink" title="基于数据集的"></a>基于数据集的</h6><blockquote>
<p>都是通过本身的SourceFunction对象调用addSource</p>
<ul>
<li>fromCollection(Iterator, class)  </li>
<li>fromCollection(Iterator, TypeInformation)    </li>
<li>fromElements(T…)  </li>
<li>fromParallelCollection</li>
</ul>
</blockquote>
<h6 id="Customer-Source-自定义source"><a href="#Customer-Source-自定义source" class="headerlink" title="Customer Source 自定义source"></a>Customer Source 自定义source</h6><blockquote>
<ul>
<li>addSource(sourceFunction )    // sourceName 为默认值</li>
<li>addSource(sourceFunction ,  sourceName) // typeInfo 为 null</li>
<li>addSource(sourceFunction ,  typeInformation) // SourceName 有默认值</li>
<li>addSource(sourceFunction ,  sourceName ,  typeInformation)  </li>
</ul>
</blockquote>
<p><strong><em>自定义source代码示例</em></strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> source;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.RichSourceFunction;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * RichSourceFunction 实现了SourceFunction接口，只做了序列化</span></span><br><span class="line"><span class="comment"> * 实现接口SourceFunction或者继承 RichSourceFunction 需要申明返回的数据类型，不然会报错：</span></span><br><span class="line"><span class="comment"> * Caused by: org.apache.flink.api.common.functions.InvalidTypesException:</span></span><br><span class="line"><span class="comment"> *      The types of the interface org.apache.flink.streaming.api.functions.source.SourceFunction could not be inferred.</span></span><br><span class="line"><span class="comment"> *      Support for synthetic interfaces, lambdas, and generic or raw types is limited at this point</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyDataSource</span> <span class="keyword">extends</span> <span class="title">RichSourceFunction</span>&lt;<span class="title">Integer</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOG = LoggerFactory.getLogger(MyDataSource<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">boolean</span> isRunning = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Integer&gt; ctx)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (isRunning)&#123;</span><br><span class="line">            Thread.sleep(<span class="number">300</span>);</span><br><span class="line">            <span class="keyword">int</span> rnd = (<span class="keyword">int</span>) (Math.random() * <span class="number">10</span>);</span><br><span class="line"></span><br><span class="line">            LOG.info(<span class="string">"emit data:"</span>+rnd);</span><br><span class="line">            ctx.collect( rnd );</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        isRunning = <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="DataStream-Transformations"><a href="#DataStream-Transformations" class="headerlink" title="DataStream Transformations"></a>DataStream Transformations</h2><h6 id="Map-DataStream-gt-DataStream"><a href="#Map-DataStream-gt-DataStream" class="headerlink" title="Map [DataStream -&gt; DataStream]"></a>Map [DataStream -&gt; DataStream]</h6><blockquote>
<p>对数据集内每条数据都进行相同的规则处理，常用来做清洗和转换数据格式等</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; windowCount</span><br><span class="line">        .map(<span class="keyword">new</span> MapFunction&lt;Tuple2&lt;String,Integer&gt;, String&gt;() &#123;</span><br><span class="line">             <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(Tuple2&lt;String, Integer&gt; stringIntegerTuple2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> stringIntegerTuple2.f0;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br></pre></td></tr></table></figure>
<h6 id="FlatMap-DataStream-gt-DataStream"><a href="#FlatMap-DataStream-gt-DataStream" class="headerlink" title="FlatMap [DataStream -&gt; DataStream]"></a>FlatMap [DataStream -&gt; DataStream]</h6><blockquote>
<p>将数据集进行打平，即按照逻辑合并在一个或多个数据集里面</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; windowCount = text</span><br><span class="line">        .flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">for</span> (String word : value.split(<span class="string">"\\s"</span>)) &#123;</span><br><span class="line">                    collector.collect(Tuple2.of(word, <span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br></pre></td></tr></table></figure>
<h6 id="Filter-DataStream-gt-DataStream"><a href="#Filter-DataStream-gt-DataStream" class="headerlink" title="Filter [DataStream -&gt; DataStream]"></a>Filter [DataStream -&gt; DataStream]</h6><blockquote>
<p>过滤数据，符合要求的数据返回 true，不符合要求的返回 false</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">text</span><br><span class="line">        .filter(<span class="keyword">new</span> FilterFunction&lt;String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> s.contains(<span class="string">"h"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br></pre></td></tr></table></figure>
<h6 id="KeyBy-DataStream-gt-KeyedStream"><a href="#KeyBy-DataStream-gt-KeyedStream" class="headerlink" title="KeyBy [DataStream -&gt; KeyedStream]"></a>KeyBy [DataStream -&gt; KeyedStream]</h6><blockquote>
<p>在数据集中进行 Partition操作，将相同的key值的数据放到相同的分区中，返回 keyedStream。<br>指定key时可以按位置指定，也可以按名称指定（此时需要pojo类、case class等明确了字段位置的）<br>注意以下类型不能成为key：</p>
<ul>
<li>POJO类型但是不覆盖 hashCode() 方法并依赖于Object.hashCode() 实现</li>
<li>任何类型的数组</li>
</ul>
</blockquote>
<h6 id="Reduce-KeyedStream-gt-DataStream"><a href="#Reduce-KeyedStream-gt-DataStream" class="headerlink" title="Reduce [KeyedStream -&gt; DataStream]"></a>Reduce [KeyedStream -&gt; DataStream]</h6><blockquote>
<p> 定义聚合逻辑，对数据进行聚合处理，其聚合逻辑要求满足运算结合律和交换律。当前元素的值和最后一个Reduce的值进行组合并返回出新的Reduce的值。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">.reduce(<span class="keyword">new</span> ReduceFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">reduce</span><span class="params">(Tuple2&lt;String, Integer&gt; stringIntegerTuple2, Tuple2&lt;String, Integer&gt; t1)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> Tuple2.of(stringIntegerTuple2.f0, stringIntegerTuple2.f1+t1.f1);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<h6 id="Aggregate-keyedStream-gt-DataStream"><a href="#Aggregate-keyedStream-gt-DataStream" class="headerlink" title="Aggregate [keyedStream -&gt; DataStream]"></a>Aggregate [keyedStream -&gt; DataStream]</h6><blockquote>
<p>聚合算子，将Reduce算子中的函数进行了封装。封装的操作包括 sum、min、minBy、max、maxBy等</p>
</blockquote>
<h6 id="Fold-keyedStream-gt-DataStream"><a href="#Fold-keyedStream-gt-DataStream" class="headerlink" title="Fold [keyedStream -&gt; DataStream]"></a>Fold [keyedStream -&gt; DataStream]</h6><blockquote>
<p>将数据进行滚动折叠，可指定开始值。未来将取消，全部用Aggregate替代</p>
</blockquote>
<h6 id="Union"><a href="#Union" class="headerlink" title="Union"></a>Union</h6><blockquote>
<p>合并操作，要求两个 DataStream 数据格式一样</p>
</blockquote>
<h6 id="Connect"><a href="#Connect" class="headerlink" title="Connect"></a>Connect</h6><blockquote>
<p>不要求格式一样，类似拼接格式操作，返回 ConnectedStreams。  </p>
<p>比如 (String，Int) connect (Int)  结果: ((String, Int), Int)   </p>
<p>ConnectedStreams不能直接print，需要使用CoMapFunction 或CoFlatMapFunction分别处理DataStrea，处理后返回的数据类型必须保持一致。</p>
</blockquote>
<h6 id="Split-DataStream-gt-SplitStream"><a href="#Split-DataStream-gt-SplitStream" class="headerlink" title="Split [DataStream -&gt; SplitStream]"></a>Split [DataStream -&gt; SplitStream]</h6><blockquote>
<p>Union算子的逆向实现</p>
</blockquote>
<h6 id="Select-SplitStream-gt-DataStream"><a href="#Select-SplitStream-gt-DataStream" class="headerlink" title="Select [SplitStream -&gt; DataStream]"></a>Select [SplitStream -&gt; DataStream]</h6><blockquote>
<p>Select是splitStream的方法，split 只是进行标记，并未进行切分。select切分数据集。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SplitStream&lt;String&gt; split = text.split(<span class="keyword">new</span> OutputSelector&lt;String&gt;() &#123;</span><br><span class="line"><span class="comment">// 切分数据的时候给每部分数据打上标记</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">select</span><span class="params">(String value)</span> </span>&#123;</span><br><span class="line">        ArrayList&lt;String&gt; strings = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span> (value.contains(<span class="string">"h"</span>))</span><br><span class="line">            strings.add(<span class="string">"hadoop"</span>);</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            strings.add(<span class="string">"noHadoop"</span>);</span><br><span class="line">        <span class="keyword">return</span> strings;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 打印有 hadoop 标签的数据</span></span><br><span class="line">split.select(<span class="string">"hadoop"</span>).print();</span><br><span class="line"><span class="comment">// 打印有 noHadoop 标签的数据</span></span><br><span class="line">split.select(<span class="string">"noHadoop"</span>)</span><br><span class="line">    .map(<span class="keyword">new</span> MapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> s.toUpperCase();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">    .print();</span><br></pre></td></tr></table></figure>

<h6 id="Partition类-transformation"><a href="#Partition类-transformation" class="headerlink" title="Partition类 transformation"></a>Partition类 transformation</h6><blockquote>
<ul>
<li>shuffle: 随机分配，分区相对均衡，容易失去原有数据分区结构</li>
<li>rebalance: 分布式轮询下游节点，轮询分配。尽可能保证每个分区的数据平衡，多用于数据倾斜  </li>
<li>rescale: 本地轮流分配。只轮询本节点上的下游操作实例。</li>
<li>partitionCustom：自定义，只能返回一个操作示例，单播。</li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Apache-Flink</category>
        <category>FlinkCoding</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>HiveTezUnion问题</title>
    <url>/2020/04/01/Apache-Hive/%E5%B9%B3%E5%B8%B8%E8%AE%B0%E5%BD%95/HiveTezUnion%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p>在 hive on tez 中，使用类似  insert overwrite A select * from B union select * from C 操作时，会导致插入的表或分区目录下有子目录，引起除hive on tez之外的引擎不可查数据。</p>
<h4 id="问题原因"><a href="#问题原因" class="headerlink" title="问题原因"></a>问题原因</h4><p>tez 对于 insert union 操作做了优化，通过并行加快速度，同时防止表目录或分区目录下有相同文件，所以在输出的时候各自生成了子目录。但是在用户查询时，只有 hive tez 能正常查询。其他引擎因为子目录问题无法查询。</p>
<h4 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h4><ol>
<li><p>所有 hive 均使用 tez 引擎</p>
</li>
<li><p>使用 sparkSQL 进行 inert union 操作</p>
</li>
<li><p>使用 tez 进行 insert union 操作时 加上 order by 操作</p>
</li>
<li><p>开启 MR 递归查询模式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set hive.mapred.supports.subdirectories&#x3D;true;</span><br><span class="line">set mapreduce.input.fileinputformat.input.dir.recursive&#x3D;true;</span><br></pre></td></tr></table></figure>
</li>
<li><p>spark 查询时：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set mapreduce.input.fileinputformat.input.dir.recursive&#x3D;true</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>后面两种方案需要考虑是否会对其他任务造成影响，建议前面两种。</p>
]]></content>
      <categories>
        <category>Apache-Hive</category>
        <category>平常记录</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive中文乱码问题</title>
    <url>/2020/03/28/Apache-Hive/%E5%B9%B3%E5%B8%B8%E8%AE%B0%E5%BD%95/Hive%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h4 id="修改-hive-site-xml-（非必须）"><a href="#修改-hive-site-xml-（非必须）" class="headerlink" title="修改 hive-site.xml （非必须）"></a>修改 hive-site.xml （非必须）</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;jdbc:mysql:&#x2F;&#x2F;IP:3306&#x2F;db_name?createDatabaseIfNotExist&#x3D;true&amp;useUnicode&#x3D;true&amp;characterEncoding&#x3D;UTF-8&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;description&gt;JDBC connect string for a JDBC metastore&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>

<h4 id="进入元数据库修改MySQL编码："><a href="#进入元数据库修改MySQL编码：" class="headerlink" title="进入元数据库修改MySQL编码："></a>进入元数据库修改MySQL编码：</h4><h6 id="修改表字段注解和表注释"><a href="#修改表字段注解和表注释" class="headerlink" title="修改表字段注解和表注释"></a>修改表字段注解和表注释</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8;</span><br><span class="line">alter table TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;</span><br></pre></td></tr></table></figure>

<h6 id="修改分区字段注释"><a href="#修改分区字段注释" class="headerlink" title="修改分区字段注释"></a>修改分区字段注释</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">alter table PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;</span><br><span class="line">alter table PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set utf8;</span><br></pre></td></tr></table></figure>

<h6 id="修改索引注释"><a href="#修改索引注释" class="headerlink" title="修改索引注释"></a>修改索引注释</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">alter table INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;</span><br></pre></td></tr></table></figure>

<h4 id="重启-hive-（非必须）"><a href="#重启-hive-（非必须）" class="headerlink" title="重启 hive （非必须）"></a>重启 hive （非必须）</h4>]]></content>
      <categories>
        <category>Apache-Hive</category>
        <category>平常记录</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive额外知识</title>
    <url>/2020/03/21/Apache-Hive/%E5%B9%B3%E5%B8%B8%E8%AE%B0%E5%BD%95/Hive%E9%A2%9D%E5%A4%96%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<h4 id="hive四种存储格式介绍与分析比较"><a href="#hive四种存储格式介绍与分析比较" class="headerlink" title="hive四种存储格式介绍与分析比较"></a><strong>hive四种存储格式介绍与分析比较</strong></h4><p>1、TestFile</p>
<p> TextFile文件不支持块压缩，默认格式，数据不做压缩，磁盘开销大，数据解析开销大。这边不做深入介绍。</p>
<p>2、RCFile</p>
<p> Record Columnar的缩写。是Hadoop中第一个列文件格式。能够很好的压缩和快速的查询性能，但是不支持模式演进。通常写操作比较慢，比非列形式的文件格式需要更多的内存空间和计算量。</p>
<p>RCFile是一种行列存储相结合的存储方式。首先，其将数据按行分块，保证同一个record在一个块上，避免读一个记录需要读取多个block。其次，块数据列式存储，有利于数据压缩和快速的列存取。</p>
<p>3、ORCFile</p>
<p>存储方式：数据按行分块每块按照列存储，压缩快，快速列存取，效率比rcfile高，是 rcfile 的改良版本，相比RC能够更好的压缩，能够更快的查询，但还是不支持模式演进。</p>
<p>4、Parquet</p>
<p>Parquet能够很好的压缩，有很好的查询性能，支持有限的模式演进。但是写速度通常比较慢。这中文件格式主要是用在Cloudera Impala上面的。</p>
<h4 id="压缩算法比较"><a href="#压缩算法比较" class="headerlink" title="压缩算法比较"></a><strong>压缩算法比较</strong></h4><p>1、gzip压缩</p>
<p>优点：压缩率比较高，而且压缩/解压速度也比较快；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；有hadoop native库；大部分linux系统都自带gzip命令，使用方便。</p>
<p>缺点：不支持split。</p>
<p>应用场景：当每个文件压缩之后在130M以内的（1个块大小内），都可以考虑用gzip压缩格式。譬如说一天或者一个小时的日志压缩成一个gzip文件，运行mapreduce程序的时候通过多个gzip文件达到并发。hive程序，streaming程序，和java写的mapreduce程序完全和文本处理一样，压缩之后原来的程序不需要做任何修改。</p>
<p>2、lzo压缩</p>
<p>优点：压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；支持hadoop native库；可以在linux系统下安装lzop命令，使用方便。</p>
<p>缺点：压缩率比gzip要低一些；hadoop本身不支持，需要安装；在应用中对lzo格式的文件需要做一些特殊处理（为了支持split需要建索引，还需要指定inputformat为lzo格式）。</p>
<p>应用场景：一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，lzo优点越越明显。</p>
<p>3、snappy压缩</p>
<p>优点：高速压缩速度和合理的压缩率；支持hadoop native库。</p>
<p>缺点：不支持split；压缩率比gzip要低；hadoop本身不支持，需要安装；linux系统下没有对应的命令。</p>
<p>应用场景：当mapreduce作业的map输出的数据比较大的时候，作为map到reduce的中间数据的压缩格式；或者作为一个mapreduce作业的输出和另外一个mapreduce作业的输入。</p>
<p>4、bzip2压缩</p>
<p>优点：支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便。</p>
<p>缺点：压缩/解压速度慢；不支持native。</p>
<p>应用场景：适合对速度要求不高，但需要较高的压缩率的时候，可以作为mapreduce作业的输出格式；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比较少的情况；或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持split，而且兼容之前的应用程序（即应用程序不需要修改）的情况。</p>
<p>最后用一个表格比较上述4种压缩格式的特征（优缺点）：</p>
<p><img src="F:%5CmyGit%5CDT-Learner%5CApache-Hive%5C%E5%B9%B3%E5%B8%B8%E8%AE%B0%E5%BD%95%5Cimage-20200623154537087.png" alt="image-20200623154537087"></p>
<p>注意：以上几种压缩算法都是在压缩普通文本的前提下来说的是否支持split，如果是RCFile、Sequence Files等，本身就支持split，经过压缩之后一样是支持split的。</p>
<p>综上，我们hadoop2.4集群要求RCFile+gzip是有一定道理的，首先RCFile格式的文件支持按列存储，同时支持split，而gzip的压缩率比较高，而且压缩/解压速度也比较快，所以RCFile格式的文件经过gzip压缩后既能保证文件能split，还能保证很高压缩/解压速度和压缩比。</p>
<h4 id="Hive和RDBMS的区别"><a href="#Hive和RDBMS的区别" class="headerlink" title="Hive和RDBMS的区别"></a><strong>Hive和RDBMS的区别</strong></h4><p>1.查询语言：HSQL为类SQL，可以进行一般的查询。没有SQL灵活好用</p>
<p>2.Hive数据在HDFS，而RDBMS数据在块设备或者本地文件系统</p>
<p>3.Hive通常不能进行update和delete（高版本可以开启ACID，需要ORC等等条件），RDBMS可以</p>
<p>4.Hive是MR计算，RDBMS有自己的计算引擎</p>
<p>5.Hive执行延迟高，适合大批量离线处理；RDBMS延迟低，大批量数据处理性能下降明显</p>
<p>6.Hive底层是HDFS，几千上万台；RDBMS受限，几百台</p>
<h4 id="order-by，sort-by，clustered-by，distribute-by"><a href="#order-by，sort-by，clustered-by，distribute-by" class="headerlink" title="order by，sort by，clustered by，distribute by"></a><strong>order by，sort by，clustered by，distribute by</strong></h4><p>order by：全局排序，一个reduce</p>
<p>sort by  ：每个reduce端排序，局部有序；优点：做完sort by 再做全局排序能提高效率；</p>
<p>distribute by：经常和sort by 搭配。先distribute by，相同的数据到相同的reduce（partition）再做sort by;</p>
<p>cluster by：等价于distribute by和sort by一起使用，但是cluster by只能降序</p>
<h4 id="Hive-Join分类"><a href="#Hive-Join分类" class="headerlink" title="Hive Join分类"></a><strong>Hive Join分类</strong></h4><p>reduce join：inner join，left join，right join，full join，cross join（笛卡尔积）</p>
<p>map join：小表对大表，hive.mapjoin.smalltable.filesize=25000000</p>
<p>sort-merge-bucket join：大表 join 大表，后台会创建两张参与join的表，并分区分桶排序。    clustered by  sort by </p>
<h4 id="hive小文件问题"><a href="#hive小文件问题" class="headerlink" title="hive小文件问题"></a><strong>hive小文件问题</strong></h4><p>影响：NameNode 性能，MapReduce任务缓慢(小文件会增加MapTask)；无数据倾斜，小表关联任务太慢</p>
<p>解决：合理的文件格式(sequence file 比text file好)；</p>
<p>​    控制Reduce数量；</p>
<p>​    少用动态分区，用时记得distribute by分区；</p>
<p>hive.merg.mapfiles=true：合并map输出</p>
<p>hive.merge.mapredfiles=false：合并reduce输出</p>
<p>hive.merge.size.per.task=256<em>1000</em>1000：合并文件的大小</p>
<p>hive.mergejob.maponly=true：如果支持CombineHiveInputFormat则生成只有Map的任务执行merge</p>
<p>hive.merge.smallfiles.avgsize=16000000：文件的平均大小小于该值时，会启动一个MR任务执行merge。</p>
<p>已有的小文件：HAR归档；重建表，设置合理的reduce数；</p>
<h4 id="数据倾斜"><a href="#数据倾斜" class="headerlink" title="数据倾斜"></a><strong>数据倾斜</strong></h4><p>表现：任务长时间卡在99%，某些任务时间远大于其他任务平均时间</p>
<p>原因：1.key分布不均匀，导致join时部分key集中在几个reduce上</p>
<p>​           2.count(distinct )  特殊值过多</p>
<p>​           3.数据业务本身特点</p>
<p>​           4.Hive SQL存在数据倾斜</p>
<p>解决：1.空值产生的数据倾斜 – &gt;  1️⃣ 空值数据单独管理，之后union   2️⃣ 空值赋值 特定值+随机数</p>
<p>​           2.设置hive.map.aggr = true map端部分聚合，相当于Combiner</p>
<p>​           3.count(distinct )   特殊值类型不多，可以单独计算，再union；或者先group by，再做sum</p>
<p>​           4.不同的数据类型关联，需要先转换为相同的数据类型</p>
<p>​           5.map join</p>
<p>​           6.大表对大表，将倾斜的key通过采样得出，将倾斜的key的数据单独拿出来做mapjoin，最后union</p>
<p>​    \7. hive.groupby.skewindata=true控制生成两个MR Job  第一个MRJob 中，Map的输出结果集合会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的GroupBy Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MRJob再根据预处理的数据结果按照GroupBy Key分布到Reduce中（这个过程可以保证相同的GroupBy Key被分布到同一个Reduce中），最后完成最终的聚合操作。</p>
<h4 id="hiveSQL转化为MapReduce任务的过程"><a href="#hiveSQL转化为MapReduce任务的过程" class="headerlink" title="hiveSQL转化为MapReduce任务的过程"></a><strong>hiveSQL转化为MapReduce任务的过程</strong></h4><ul>
<li>Antlr定义SQL的语法规则，完成SQL词法，语法解析，将SQL转化为抽象语法树AST Tree</li>
<li>遍历AST Tree，抽象出查询的基本组成单元QueryBlock</li>
<li>遍历QueryBlock，翻译为执行操作树OperatorTree</li>
<li>逻辑层优化器进行OperatorTree变换，合并不必要的ReduceSinkOperator，减少shuffle数据量</li>
<li>遍历OperatorTree，翻译为MapReduce任务</li>
<li>物理层优化器进行MapReduce任务的变换，生成最终的执行计划</li>
</ul>
<h4 id="Hive目录下的-开头的数据不会被读取。"><a href="#Hive目录下的-开头的数据不会被读取。" class="headerlink" title="Hive目录下的.开头的数据不会被读取。"></a><strong>Hive目录下的.开头的数据不会被读取。</strong></h4><p>Java 写 parquet 文件时注意小文件问题，以及未关闭的文件无法被读取。需要定期合并小文件，并且在任务重启或者消费者rebanlance时需要删除未正常关闭的文件。</p>
]]></content>
      <categories>
        <category>Apache-Hive</category>
        <category>平常记录</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>消息发送</title>
    <url>/2020/06/01/Apache-Kafka/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Kafka%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/00.%E6%B6%88%E6%81%AF%E5%8F%91%E9%80%81-Producer/</url>
    <content><![CDATA[<h3 id="消息发送"><a href="#消息发送" class="headerlink" title="消息发送"></a>消息发送</h3><p>一个完整的正常的生产者逻辑有以下几个步骤：</p>
<ol>
<li>配置生产者实例：包括生产者参数配置和生产者实例创建</li>
<li>构建需要发送的消息 <code>message</code> 发送消息</li>
<li>关闭生产者实例</li>
</ol>
<p>先看一下消息发送的完整代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> kafka;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringSerializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.TimeUnit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyProducer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line"></span><br><span class="line">        String brokerList = <span class="string">"kafka1:9092"</span>;</span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 增加 Producer 参数时，最好不要手写参数，使用类里面定义好的最好，不用担心写错。</span></span><br><span class="line">        <span class="comment">// 另外，序列化的参数值也可以通过获取 class name 的方式。</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList); <span class="comment">// broker list</span></span><br><span class="line">        properties.put(ProducerConfig.RETRIES_CONFIG, <span class="number">10</span>);                   <span class="comment">// 重试次数</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">        properties.put(ProducerConfig.CLIENT_ID_CONFIG, <span class="string">"producer.client.id.myId"</span>);</span><br><span class="line"></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 构建消息 有几个构造方法，按需选择。topic, value 是必选项。最好加上 key，通过 key 来 hash 分 partition</span></span><br><span class="line">        ProducerRecord&lt;String, String&gt; producerRecord = <span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"topic"</span>, <span class="string">"1"</span>, <span class="string">"value"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 第一种 发后即忘 fire-and-forget。正常情况下不会有问题，但是发送失败，数据丢失。</span></span><br><span class="line">        kafkaProducer.send(producerRecord);</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        <span class="comment">// 第二种 同步发送，如果没有 get 方法，则为 发过即忘 ，即发送失败会重试，重试失败丢数据。</span></span><br><span class="line">        <span class="comment">// 有了 get 之后，发送失败会重试，重试不行抛异常，可以在异常里面处理下一步</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            kafkaProducer.send(producerRecord).get();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (ExecutionException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 第三种 异步发送 自定义 Callback，在 Callback 里面处理消息发送失败的逻辑。</span></span><br><span class="line">        kafkaProducer.send(producerRecord, <span class="keyword">new</span> Callback() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata recordMetadata, Exception e)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">if</span> (e != <span class="keyword">null</span>)&#123;</span><br><span class="line">                    System.out.println(<span class="string">"中止程序"</span>);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span> &#123;</span><br><span class="line">                    System.out.println(</span><br><span class="line">                            recordMetadata.topic() + <span class="string">":"</span> +</span><br><span class="line">                            recordMetadata.partition() + <span class="string">":"</span> +</span><br><span class="line">                            recordMetadata.offset());</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 关闭 Producer，实际应用中，通常选择无参的 close。</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">        kafkaProducer.close(<span class="number">10000</span>, TimeUnit.MILLISECONDS) ;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h6 id="配置-kafkaProducer-实例"><a href="#配置-kafkaProducer-实例" class="headerlink" title="配置 kafkaProducer 实例"></a>配置 kafkaProducer 实例</h6><p>如代码注释所说，我们在配置参数时，手写参数可能会出错，因此最好使用 ProducerConfig 类里面定义好的参数，比手写快、方便、准确。同时，对于 <code>key</code>  <code>value</code> 的序列化类还可以通过 获取对应类的 类名来实现，不容易写错，毕竟这里是需要全路径的。配置完参数后就可以通过 KafkaProducer 来实例化一个 Producer 了。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String brokerList = <span class="string">"kafka1:9092"</span>;</span><br><span class="line"></span><br><span class="line">Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 增加 Producer 参数时，最好不要手写参数，使用类里面定义好的最好，不用担心写错。</span></span><br><span class="line"><span class="comment">// 另外，序列化的参数值也可以通过获取 class name 的方式。</span></span><br><span class="line">properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList); <span class="comment">// broker list</span></span><br><span class="line">properties.put(ProducerConfig.RETRIES_CONFIG, <span class="number">10</span>);                   <span class="comment">// 重试次数</span></span><br><span class="line">properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">properties.put(ProducerConfig.CLIENT_ID_CONFIG, <span class="string">"producer.client.id.myId"</span>);</span><br><span class="line"></span><br><span class="line">KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br></pre></td></tr></table></figure>

<p>其中 ，<code>KafkaProducer&lt;String, String&gt;</code>  的结构 <code>&lt;String, String&gt;</code> 指的是 <code>key</code> value 的类型。<code>BOOTSTRAP_SERVERS_CONFIG</code> ：给定两个及以上就OK，<code>producer</code> 会从给定的broker找到其他broker。但是不建议给一个，防止宕机无法联系上kafka集群。<code>CLIENT_ID_CONFIG</code> ：<code>producer</code> 的自有 <code>ID</code>，默认为空，不设置<code>producer</code> 会自动生成一个 <code>producer</code> 开头的字符串。</p>
<p>此外，<code>KafkaProducer</code> 是线程安全的，可以在多个线程中共享单个 <code>KafkaProducer</code> 实例，也可以将实例进行池化。</p>
<h6 id="构建消息-message"><a href="#构建消息-message" class="headerlink" title="构建消息 message"></a>构建消息 message</h6><p>首先，看一下 <code>message</code> 的结构，即 <code>ProducerRecord</code> 的结构 。包括五个值，<code>topic</code>、<code>partition</code>、<code>key</code>、<code>value</code>、<code>timestamp</code>。也就是说，我们构建  <code>ProducerRecord</code> 的时候，需要在构造方法中这几个值。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProducerRecord</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String topic;          <span class="comment">// 记录topic </span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Integer partition;     <span class="comment">// 记录partition</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> K key;                 <span class="comment">// 记录key</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> V value;               <span class="comment">// 记录数据值</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Long timestamp;        <span class="comment">// 记录时间戳</span></span><br><span class="line">    </span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>构建 <code>message</code> 的构造方法有几个，如下代码所示。可以看出，必要的两个参数是 <code>topic</code> 和 <code>value</code> ，通常情况下指定这两个也够用。对于 key，可以根据分区或业务来指定不同的 key，以便 <code>kafka</code> 根据 <code>key</code> 来做 <code>hash</code> 将 <code>message</code> 均衡发送到不同的 <code>partition</code>，另外，<strong>有key的数据还可以支持日志压缩的功能</strong>。对于 <code>partition</code>，如果指定了，那么该条 <code>message</code> 只能发送到对应的 <code>partition</code>，而不会发送到其他 <code>partition</code> 。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ProducerRecord(String topic, V value)</span><br><span class="line">ProducerRecord(String topic, K key, V value)</span><br><span class="line">ProducerRecord(String topic, Integer partition, K key, V value)</span><br><span class="line">ProducerRecord(String topic, Integer partition, Long timestamp, K key, V value)</span><br></pre></td></tr></table></figure>

<p>这边我们只指定 <code>topic</code>、<code>key</code>、<code>value</code>。当其他属性不设置时，默认为 <code>null</code> 。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ProducerRecord&lt;String, String&gt; producerRecord = <span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"topic"</span>, <span class="string">"1"</span>, <span class="string">"value"</span>);</span><br></pre></td></tr></table></figure>



<h6 id="发送消息-message"><a href="#发送消息-message" class="headerlink" title="发送消息 message"></a>发送消息 message</h6><p>发送消息分为两类，一类是同步发送，一类是异步发送。</p>
<ul>
<li>发后即忘：<code>Producer</code> 管发不管结果，发送成功与否不负责，存在数据丢失的风险，特别是 <code>ack = -1</code> 时。性能最高，可靠性最差。</li>
<li>同步发送：<code>kafkaProducer.send(producerRecord)</code> 的返回值不是 <code>void</code> 类型，可以直接通过其返回值 <code>recordMetadata</code> 的 <code>get</code> 方法可以阻塞等待 <code>kafka</code> 响应，或者接受 <code>send</code> 方法的返回值<code>recordMetadata</code> 后，再调用其 <code>get</code>方法。如果发送失败，会进行重试 (假设 <code>producer</code> 的重试次数有设置的话) ，重试失败则直接抛异常。然后在异常里面处理 异常逻辑，<code>recordMetadata</code> 中记录了<code>topic</code>、<code>partition</code>、<code>offset</code>等信息。</li>
<li>异步发送：使用自定义 <code>Callback</code>，<code>kafka</code>有响应时就会回调，要么成功，要么抛异常。异常 Exception 和 <code>RecordMetadata</code> 总有一个为 <code>null</code>。对于同一个分区来说，<code>Callback</code>也是有序的。</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 第一种 发后即忘 fire-and-forget。正常情况下不会有问题，但是发送失败，数据丢失。</span></span><br><span class="line">kafkaProducer.send(producerRecord);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第二种 同步发送，如果没有 get 方法，则为 发过即忘 ，即发送失败会重试，重试失败丢数据。</span></span><br><span class="line"><span class="comment">// 有了 get 之后，发送失败会重试，重试不行抛异常，可以在异常里面处理下一步</span></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    kafkaProducer.send(producerRecord).get();</span><br><span class="line">&#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">    e.printStackTrace();</span><br><span class="line">&#125; <span class="keyword">catch</span> (ExecutionException e) &#123;</span><br><span class="line">    e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第三种 异步发送 自定义 Callback，在 Callback 里面处理消息发送失败的逻辑。</span></span><br><span class="line">kafkaProducer.send(producerRecord, <span class="keyword">new</span> Callback() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata recordMetadata, Exception e)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (e != <span class="keyword">null</span>)&#123;</span><br><span class="line">            System.out.println(<span class="string">"中止程序"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            System.out.println(</span><br><span class="line">                    recordMetadata.topic() + <span class="string">":"</span> +</span><br><span class="line">                    recordMetadata.partition() + <span class="string">":"</span> +</span><br><span class="line">                    recordMetadata.offset());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>



<h6 id="关闭-Producer"><a href="#关闭-Producer" class="headerlink" title="关闭 Producer"></a>关闭 Producer</h6><p>关闭有两个重载方法，无参的重载方法 <code>close()</code> 会等待所有的消息发送完之后进行 <code>Producer</code> 的关闭。有超时时间参数的方法 <code>close()</code>  超时后会强行退出。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 关闭 Producer，实际应用中，通常选择无参的 close。</span></span><br><span class="line">kafkaProducer.close();</span><br><span class="line">kafkaProducer.close(<span class="number">10000</span>, TimeUnit.MILLISECONDS);</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Apache-Kafka</category>
        <category>深入理解Kafka读书笔记</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Kafka</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>生产者拦截器</title>
    <url>/2020/06/02/Apache-Kafka/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Kafka%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/01.%E7%94%9F%E4%BA%A7%E8%80%85%E6%8B%A6%E6%88%AA%E5%99%A8/</url>
    <content><![CDATA[<h3 id="拦截器-Interceptor"><a href="#拦截器-Interceptor" class="headerlink" title="拦截器 Interceptor"></a>拦截器 Interceptor</h3><p>目前，在 <code>kafka</code> 中有两种拦截器，一个是生产者拦截器，也就是马上要看的。另一个是对应的消费者拦截器，后面再研究。生产者拦截器作用发生在消息序列化之前，可以提前做一些准备工作，比如按照规则过滤或修改消息值（不建议修改其他部分，如 <code>key</code>，<code>partition</code>等），也可以做类似消息统计这样的工作。</p>
<p>生产者拦截器主</p>
<p>要是实现 <code>org.apache.kafka.clients.producer.ProducerInterceptor</code> 接口。</p>
<h6 id="ProducerInterceptor-接口"><a href="#ProducerInterceptor-接口" class="headerlink" title="ProducerInterceptor  接口"></a>ProducerInterceptor  接口</h6><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ProducerInterceptor</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Configurable</span> </span>&#123;</span><br><span class="line">	<span class="comment">// 在这个方法中对消息进行相应的定制修改操作。</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ProducerRecord&lt;K, V&gt; <span class="title">onSend</span><span class="params">(ProducerRecord&lt;K, V&gt; record)</span></span>;</span><br><span class="line">	<span class="comment">// 这个方法会在会在消息被应答前或者发送失败时调用</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onAcknowledgement</span><span class="params">(RecordMetadata metadata, Exception exception)</span></span>;</span><br><span class="line">	<span class="comment">// 关闭拦截器时做资源清理工作</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>KafkaProducer</code>在将消息序列化和计算分区之前，会先调用生产者拦截器的 <code>onSend()</code> 方法。另外，<code>KafkaProducer</code> 会在消息发送成功或者失败时调用 <code>onAcknowledgement()</code> 方法，可以做一些简单的统计类操作，因为这个方法是在 I/O 线程中，所以逻辑越简单越好，不然会影响消息发送速度。</p>
<p>另外接口也继承了 <code>Configurable</code> 接口，因此如果要实现该接口，还需实现 <code>Configurable</code> 接口中的 configure方法。</p>
<h6 id="自定义拦截器-MyProducerInterceptor"><a href="#自定义拦截器-MyProducerInterceptor" class="headerlink" title="自定义拦截器 MyProducerInterceptor"></a>自定义拦截器 MyProducerInterceptor</h6><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> kafka.interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerInterceptor;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.RecordMetadata;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyProducerInterceptor</span> <span class="keyword">implements</span> <span class="title">ProducerInterceptor</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOG = LoggerFactory.getLogger(MyProducerInterceptor<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 定义两个全局变量，来统计成功和失败消息数</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> successSend = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> failureSend = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 空方法</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 在这里对消息进行规则定制化</span></span><br><span class="line">    <span class="comment">// 暂时只对消息的值即value加上 first 前缀。</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ProducerRecord <span class="title">onSend</span><span class="params">(ProducerRecord record)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ProducerRecord&lt;&gt;(record.topic(),</span><br><span class="line">                record.partition(),</span><br><span class="line">                record.timestamp(),</span><br><span class="line">                record.key(),</span><br><span class="line">                <span class="string">"first-"</span>+record.value());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 在这里进行消息发送状态进行统计，成功失败都累加到对应的累加器上</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onAcknowledgement</span><span class="params">(RecordMetadata metadata, Exception exception)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (exception == <span class="keyword">null</span>)</span><br><span class="line">            successSend ++;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            failureSend ++;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        LOG.info(<span class="string">"成功发送："</span>+successSend +<span class="string">"条，发送失败："</span>+ failureSend + <span class="string">"条，成功率："</span>+</span><br><span class="line">                String.format(<span class="string">"%f"</span>,(<span class="keyword">double</span>)(successSend/(successSend+failureSend))*<span class="number">100</span>)+<span class="string">"%"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>拦截器的 <code>close()</code> 方法会在 <code>KafkaProducer</code> 关闭时调用。</p>
<h6 id="自定义拦截器的使用"><a href="#自定义拦截器的使用" class="headerlink" title="自定义拦截器的使用"></a>自定义拦截器的使用</h6><p>在<code>KafkaProducer</code>的配置属性中，加上拦截器的配置参数即可。如果有多个拦截器，注意拦截器的顺序。</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">properties.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, MyProducerInterceptor<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line"></span><br><span class="line">properties.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, MyProducerInterceptor.class.getName() + "," + MyProducerInterceptor1.class.getName());</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Apache-Kafka</category>
        <category>深入理解Kafka读书笔记</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Kafka</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>生产者序列化</title>
    <url>/2020/06/03/Apache-Kafka/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Kafka%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/02.%E7%94%9F%E4%BA%A7%E8%80%85%E5%BA%8F%E5%88%97%E5%8C%96%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h3 id="序列化-Serializer"><a href="#序列化-Serializer" class="headerlink" title="序列化 Serializer"></a>序列化 Serializer</h3><p>在 <code>Kafka</code> 中，生产者 <code>Producer</code> 发送消息给 <code>Broker</code> 之前，需要先将消息转换成字节数组才能进行发送操作。同样，消费者 <code>Consumer</code> 消费消息时，也需要将来自 <code>Broker</code> 的字节数组转换成具体消息。这就需要序列化和反序列化了。目前来说，<code>Kafka</code> 中用到的序列化接口只有一个：<code>org.apache.kafka.common.serialization.Serializer</code>。<code>Serializer</code> 的实现类有以下几种：</p>
<ul>
<li><p><code>StringSerializer</code> </p>
</li>
<li><p><code>ByteArraySerializer</code></p>
</li>
<li><p><code>ByteBufferSerializer</code></p>
</li>
<li><p><code>BytesSerializer</code></p>
</li>
<li><p><code>DoubleSerializer</code></p>
</li>
<li><p><code>IntegerSerializer</code></p>
</li>
<li><p><code>LongSerializer</code></p>
</li>
</ul>
<p>先看 <code>Serializer</code> 接口</p>
<h6 id="Serializer-接口"><a href="#Serializer-接口" class="headerlink" title="Serializer 接口"></a>Serializer 接口</h6><p>该接口只有三个方法。<code>configure()</code> 方法用来配置当前类，<code>serializer()</code> 方法用来执行序列化操作，<code>close()</code> 方法则用来关闭当前序列化器。在源码注释有提到，如果实现了 <code>close()</code> 方法，则必须确保该方法的幂等性，因为此方法可能会被 <code>KafkaProducer</code> 调用多次。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Serializer</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">Closeable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs, <span class="keyword">boolean</span> isKey)</span></span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">byte</span>[] serialize(String topic, T data);</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>接着看一下常用的 <code>StringSerializer</code> 类。</p>
<h6 id="实现类：StringSerializer"><a href="#实现类：StringSerializer" class="headerlink" title="实现类：StringSerializer"></a>实现类：StringSerializer</h6><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StringSerializer</span> <span class="keyword">implements</span> <span class="title">Serializer</span>&lt;<span class="title">String</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">// 默认的编码 UTF-8</span></span><br><span class="line">    <span class="keyword">private</span> String encoding = <span class="string">"UTF8"</span>;</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// configure方法，用来确定序列化的编码格式。</span></span><br><span class="line">    <span class="comment">// 但是由于 key.serializer.encoding 和 value.serializer.encoding 以及 serializer.encoding 在 KafkaProducer 基本不会配置。所以基本编码都是该实现类的默认编码。</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs, <span class="keyword">boolean</span> isKey)</span> </span>&#123;</span><br><span class="line">        String propertyName = isKey ? <span class="string">"key.serializer.encoding"</span> : <span class="string">"value.serializer.encoding"</span>;</span><br><span class="line">        Object encodingValue = configs.get(propertyName);</span><br><span class="line">        <span class="keyword">if</span> (encodingValue == <span class="keyword">null</span>)</span><br><span class="line">            encodingValue = configs.get(<span class="string">"serializer.encoding"</span>);</span><br><span class="line">        <span class="keyword">if</span> (encodingValue != <span class="keyword">null</span> &amp;&amp; encodingValue <span class="keyword">instanceof</span> String)</span><br><span class="line">            encoding = (String) encodingValue;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// serializer 方法。将String类型数据转为 byte[] 类型</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">byte</span>[] serialize(String topic, String data) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (data == <span class="keyword">null</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                <span class="keyword">return</span> data.getBytes(encoding);      <span class="comment">// 将String类型数据根据编码类型转换成byte[]</span></span><br><span class="line">        &#125; <span class="keyword">catch</span> (UnsupportedEncodingException e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> SerializationException(<span class="string">"Error when serializing string to byte[] due to unsupported encoding "</span> + encoding);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// nothing to do  基本close方法都不需要做什么，如果要实现，必须实现幂等性。</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>自定义实现 <code>Serializer</code> 类：<code>PersonSerializer</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> kafka.serializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> kafka.utils.Person;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.Serializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.ByteArrayOutputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.ObjectOutputStream;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * created by lxm on 2019-08-15</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定泛型为 Person</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PersonSerializer</span> <span class="keyword">implements</span> <span class="title">Serializer</span>&lt;<span class="title">Person</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String encoding = <span class="string">"UTF-8"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map configs, <span class="keyword">boolean</span> isKey)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 不做什么操作，可以仿照 StringSerializer 来实现</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">byte</span>[] serialize(String topic, Person person) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果数据为空直接返回空</span></span><br><span class="line">        <span class="keyword">if</span> (person == <span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 外部定义好需要返回的结果类型</span></span><br><span class="line">        <span class="keyword">byte</span>[] byteArray = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line"></span><br><span class="line">            ByteArrayOutputStream byteArrayOutputStream = <span class="keyword">new</span> ByteArrayOutputStream();</span><br><span class="line">            ObjectOutputStream objectOutputStream = <span class="keyword">new</span> ObjectOutputStream(byteArrayOutputStream);</span><br><span class="line">            <span class="comment">// 序列化对象 Person （被序列化的对象列需要实现 java.io.Serializer 接口）</span></span><br><span class="line">            objectOutputStream.writeObject(person);</span><br><span class="line">            byteArray = byteArrayOutputStream.toByteArray();</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e)&#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> byteArray;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 照常什么都不做。</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h6 id="是用自定义的序列化器"><a href="#是用自定义的序列化器" class="headerlink" title="是用自定义的序列化器"></a>是用自定义的序列化器</h6><p>使用自定义的序列化器也很简单，只需要将 生产者 <code>Producer</code> 的 <code>value</code> 序列化参数修改为 自定义的序列化类。另外，初始化生产者 <code>Producer</code> 和 ~时，注意泛型的变动。核心代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 实体类</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> id;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span> String address;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">(<span class="keyword">int</span> id, String name, String address)</span></span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.id = id;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">        <span class="keyword">this</span>.address = address;</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, PersonSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line"></span><br><span class="line">KafkaProducer&lt;String, Person&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">ProducerRecord&lt;String, Person&gt; producerRecord = <span class="keyword">new</span> ProducerRecord&lt;String, </span><br><span class="line"></span><br><span class="line">Person&gt;(topic, person);</span><br><span class="line"></span><br><span class="line">Future&lt;RecordMetadata&gt; future = kafkaProducer.send(producerRecord);</span><br><span class="line">RecordMetadata recordMetadata = future.get();</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Apache-Kafka</category>
        <category>深入理解Kafka读书笔记</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Kafka</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>生产者分区器</title>
    <url>/2020/06/04/Apache-Kafka/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Kafka%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/03.%E7%94%9F%E4%BA%A7%E8%80%85%E5%88%86%E5%8C%BA%E5%99%A8/</url>
    <content><![CDATA[<h3 id="分区器-Partitioner"><a href="#分区器-Partitioner" class="headerlink" title="分区器 Partitioner"></a>分区器 Partitioner</h3><p>在 <code>kafka</code> 里，消息经过序列化之后就需要发往分区器<code>Partitioner</code>确定发送的分区，当然这是指在<code>ProducerRecord</code>未指定<code>partition</code>的情况下。如果没有指定分区，就需要按照其中的<code>key</code>，通过<code>Partitioner</code>来进行分区。</p>
<p><strong>此时，如果 <code>key</code> 为 <code>null</code>，那么将轮询发往各个分区；如果 <code>key</code> 不为 <code>null</code> ，则按照<code>Partitioner</code> 规则进行分区。</strong></p>
<p><code>Kafka</code> 中常用的默认分区器 <code>Partitioner</code> 是 <code>org.apache.kafka.clients.producer.internals.DefaultPartitioner</code> ，其实现的是 <code>org.apache.kafka.clients.producer.Partitioner</code>  接口。</p>
<h6 id="Partitioner-接口"><a href="#Partitioner-接口" class="headerlink" title="Partitioner 接口"></a>Partitioner 接口</h6><p><code>Partitioner</code> 接口主要有两个方法，<code>partition()</code> 方法用来计算分区，参数主要是 <code>topic</code>、<code>key</code>、序列化后的<code>key</code>、<code>value</code>、序列化后的<code>value</code>、集群元数据；<code>close()</code> 关闭分区器，同时回收一些资源。此外，该接口也继承了 <code>Configurable</code> 接口，其中主要的方法是 <code>configure()</code>，主要用来配置参数</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Partitioner</span> <span class="keyword">extends</span> <span class="title">Configurable</span>, <span class="title">Closeable</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span></span>;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h6 id="默认分区器-DefaultPartitioner"><a href="#默认分区器-DefaultPartitioner" class="headerlink" title="默认分区器 DefaultPartitioner"></a>默认分区器 DefaultPartitioner</h6><p>直接进代码看具体实现</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DefaultPartitioner</span> <span class="keyword">implements</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line">	</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ConcurrentMap&lt;String, AtomicInteger&gt; topicCounterMap = <span class="keyword">new</span> ConcurrentHashMap&lt;&gt;();</span><br><span class="line">	</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> </span>&#123;&#125;</span><br><span class="line">	<span class="comment">// partition 方法。参数主要是topic、key、序列化后的key、value、序列化后的value、集群元数据信息</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 从集群元数据中获取 topic对应的分区信息。其中 PartitionInfo 属性包含 topic、partition、leader、replicas、ISR列表等待。</span></span><br><span class="line">        List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);</span><br><span class="line">        <span class="comment">// 分区数</span></span><br><span class="line">        <span class="keyword">int</span> numPartitions = partitions.size();</span><br><span class="line">        <span class="comment">// 根据 key 来做分区。key为 null 和 不为 null 分开处理。</span></span><br><span class="line">        <span class="comment">// 如果 key 为空</span></span><br><span class="line">        <span class="keyword">if</span> (keyBytes == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="comment">// 获取此条数据需要对应的随机数</span></span><br><span class="line">            <span class="keyword">int</span> nextValue = nextValue(topic);</span><br><span class="line">            <span class="comment">// 获取集群可用分区信息列表</span></span><br><span class="line">            <span class="comment">// 可用分区，即 分区 leader 不为 null 的分区。</span></span><br><span class="line">            List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic);</span><br><span class="line">            <span class="keyword">if</span> (availablePartitions.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="comment">// 如果集群可用分区数大于0，取 正值，然后除可用分区数取余，即 hash。余数只会是 0 到 分区数-1 之间的数</span></span><br><span class="line">                <span class="keyword">int</span> part = Utils.toPositive(nextValue) % availablePartitions.size();</span><br><span class="line">                <span class="comment">// 拿计算得到的余数去可用分区信息列表获取到对应的分区号</span></span><br><span class="line">                <span class="keyword">return</span> availablePartitions.get(part).partition();</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// 如果可用分区数不大于0，则直接取绝对值 对总分区数取余 作为分区号 返回</span></span><br><span class="line">                <span class="keyword">return</span> Utils.toPositive(nextValue) % numPartitions;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 如果key不为null，则先hash一个值，然后对 总分区数取余 作为分区号 返回</span></span><br><span class="line">            <span class="keyword">return</span> Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">nextValue</span><span class="params">(String topic)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 从Map获取topic对应的 AtomicInteger，第一次必为 null</span></span><br><span class="line">        AtomicInteger counter = topicCounterMap.get(topic);</span><br><span class="line">        <span class="comment">// 如果是第一次，counter 必为 null</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">null</span> == counter) &#123;</span><br><span class="line">            <span class="comment">// 随机一个数，初始化 AtomicInteger</span></span><br><span class="line">            counter = <span class="keyword">new</span> AtomicInteger(<span class="keyword">new</span> Random().nextInt());</span><br><span class="line">            <span class="comment">// 往Map里插入初始化好的 topic AtomicInteger ，然后 get 出来</span></span><br><span class="line">            AtomicInteger currentCounter = topicCounterMap.putIfAbsent(topic, counter);</span><br><span class="line">            <span class="keyword">if</span> (currentCounter != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="comment">// 当 上面 get 到的值不为空时，替换开头的 counter</span></span><br><span class="line">                counter = currentCounter;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 返回 counter（topic对应的AtomicInteger）+1 的值</span></span><br><span class="line">        <span class="keyword">return</span> counter.getAndIncrement();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>也就是说，在默认 <code>Partitioner</code> 规则下，当 <code>key</code> 为空时，只会选择可用分区来做轮询（当可用分区为0时才会对总分区取余获取分区号，可用都为0，也就没意义）。当 <code>key</code> 不为空，才会选择所有分区来 <code>hash</code>，然后取分区号，相同的 <code>key</code> 会得到相同的分区号。</p>
<h6 id="自定义分区器-MyPartitioner"><a href="#自定义分区器-MyPartitioner" class="headerlink" title="自定义分区器 MyPartitioner"></a>自定义分区器 MyPartitioner</h6><p>既然如此，先实现一个简单的分区器 <code>Partitioner</code>：当 <code>key</code> 为 <code>null</code> 时，选择所有分区来做轮询，不为 <code>null</code> 时，<code>hash</code> 值然后获取分区号。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> kafka.partitioner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.Partitioner;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.Cluster;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.PartitionInfo;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.utils.Utils;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> java.util.Random;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ConcurrentHashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.atomic.AtomicInteger;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyPartitioner</span> <span class="keyword">implements</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span>  ConcurrentHashMap&lt;String,AtomicInteger&gt; tpCounterMap =  <span class="keyword">new</span> ConcurrentHashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 配置信息</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        List&lt;PartitionInfo&gt; partitionInfos = cluster.availablePartitionsForTopic(topic);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> numPartitions = partitionInfos.size();</span><br><span class="line">        <span class="keyword">int</span> nextvalue = nextvalue(topic);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果 key 为空，则轮询所有分区。不为空，hash获取到所有分区中的对应的分区号</span></span><br><span class="line">        <span class="keyword">if</span> (keyBytes == <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">return</span> Utils.toPositive(nextvalue) % numPartitions;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> Utils.toPositive(Utils.murmur2(keyBytes)) &amp; numPartitions;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">nextvalue</span><span class="params">(String topic)</span></span>&#123;</span><br><span class="line">        </span><br><span class="line">        AtomicInteger atomicInteger = tpCounterMap.get(topic);</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (atomicInteger == <span class="keyword">null</span>)&#123;</span><br><span class="line">            atomicInteger = <span class="keyword">new</span> AtomicInteger(<span class="keyword">new</span> Random().nextInt());</span><br><span class="line">            AtomicInteger currentCount = tpCounterMap.putIfAbsent(topic, atomicInteger);</span><br><span class="line">            <span class="keyword">if</span> (currentCount != <span class="keyword">null</span>)&#123;</span><br><span class="line">                atomicInteger = currentCount;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> atomicInteger.getAndIncrement();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 空方法</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h6 id="是用自定义分区器"><a href="#是用自定义分区器" class="headerlink" title="是用自定义分区器"></a>是用自定义分区器</h6><p>如果需要只用到自定义的分区器，只需要在 <code>Producer</code> 的参数配置中加上对应的分区器配置就OK了。如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">properties.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, MyPartitioner.class.getName());</span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>Apache-Kafka</category>
        <category>深入理解Kafka读书笔记</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Kafka</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>生产者原理分析</title>
    <url>/2020/06/05/Apache-Kafka/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Kafka%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/04.%E7%94%9F%E4%BA%A7%E8%80%85%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h3 id="生产者数据链路原理分析"><a href="#生产者数据链路原理分析" class="headerlink" title="生产者数据链路原理分析"></a>生产者数据链路原理分析</h3><p>在消息真正发往 <code>Kafka</code> 之前，有可能会经历的有拦截器、序列化器和分区器等一系列操作。那么在这之后，发送的时候又会发生什么呢。</p>
<p>整个生产者由两个线程协调运行。第一个是线程，另一个是 <code>Sender</code> 线程。</p>
<h6 id="主线程"><a href="#主线程" class="headerlink" title="主线程"></a>主线程</h6><p>主线程中，<code>KafkaProducer</code> 创建消息，通过拦截器、序列化器和分区器的作用之后，会将消息缓存到消息累加器 <code>RecordAccumulator</code> 中。<code>RecordAccumulator</code> 主要用来缓存消息，以便 <code>Sender</code> 线程可以批量发送消息，进而减少网络开销从而提升性能。<code>RecordAccumulator</code> 缓存的大小可以通过配置 <code>buffer.memory</code> 来配置（默认值是 33554432 即 32MB），同时配合的参数有 <code>max.block.ms</code> 即在这个时间内，如果 <code>RecordAccumulator</code> 内的数据没被发送，则会阻塞 <code>KafkaProducer</code>，超时则会抛异常。</p>
<p>在 <code>RecordAccumulator</code> 缓存中，针对 <strong>每个分区</strong> 都维护了一个双端队列，队列中的内容就是 <code>RecordBatch</code> 即 <code>Deque&lt;RecordBatch&gt;</code>。当消息数据 <code>ProducerRecord</code> 进入到 <code>RecordAccumulator</code> 时，会先根据 <code>ProducerRecord</code> 对应分区的双端队列 <code>Deque</code> 是否存在，不存在则新建一个对应的 <code>Deque</code>，有则在 <code>Deque</code> 尾部获取一个 <code>RecordBatch</code>（没有则新建），然后查看该 <code>RecordBatch</code> 是否还能写入此条数据，如果能则写入，如果不能则创建一个新的 <code>RecordBatch</code> （新建时会判断该条消息是否超出 <code>RecordBatch</code> 大小即 <code>batch.size</code> 大小，如果不超过，就以 <code>batch.size</code> 大小来创建 <code>RecordBatch</code>（此 <code>RecordBatch</code> 会被下一个消息复用），否则就以消息大小来创建 <code>RecordBatch</code> ，此 <code>RecordBatch</code> 不会被下一个消息复用）。  此外，在 <code>RecordAccumulator</code>  中，还有一个 <code>BufferPool</code>，用来管理特定大小的 <code>ByteBuffer</code> 即 <code>RecordBatch</code> 。</p>
<p><code>Sender</code>线程从 <code>RecordAccumulator</code>  缓存中的双端队列头部获取 <code>RecordBatch</code>。</p>
<h6 id="Sender线程"><a href="#Sender线程" class="headerlink" title="Sender线程"></a>Sender线程</h6><p>Sender 线程获取到 <code>RecordBatch</code> 之后，消息会被保存成不同的形式，即 从 <code>RecordAccumulator</code> 缓存中的 <code>&lt;分区，Deque&lt;ProducerBatch&gt;&gt;</code>  =&gt; <code>&lt;Node，List&lt;ProducerBatch&gt;&gt;</code>。其中 <code>node</code> 就是 <code>broker</code> 节点，因为一般来说，我们分区数都是节点的倍数。所以一个节点会有多个 <code>partition</code>，但是客户端发送消息只需要负责发到broker，而不关心具体分区。 </p>
<p>在这之后，会再次转换成 <code>&lt;Node, Request&gt;</code> 形式，然后将 <code>Request</code> 发送到具体的 <code>Node</code>。在发送之前，会先将请求放入另一个缓存区 <code>InFlightRequest</code> ，其中的形式为 <code>Map&lt;NodeId，Deque&lt;Request&gt;&gt;</code> ，作用是缓存了已经发送了但是还没收到响应的请求。此缓存还有一些其他配置。</p>
<h3 id="Kafka-元数据更新"><a href="#Kafka-元数据更新" class="headerlink" title="Kafka 元数据更新"></a>Kafka 元数据更新</h3>]]></content>
      <categories>
        <category>Apache-Kafka</category>
        <category>深入理解Kafka读书笔记</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Kafka</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>重要的生产者参数</title>
    <url>/2020/06/06/Apache-Kafka/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Kafka%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/05.%E9%87%8D%E8%A6%81%E7%9A%84%E7%94%9F%E4%BA%A7%E8%80%85%E5%8F%82%E6%95%B0/</url>
    <content><![CDATA[<h2 id="重要的生产者参数"><a href="#重要的生产者参数" class="headerlink" title="重要的生产者参数"></a>重要的生产者参数</h2><p>直入主题</p>
<h2 id="1-acks"><a href="#1-acks" class="headerlink" title="1. acks"></a>1. acks</h2><ul>
<li><code>acks = &quot;1&quot;</code>。同时也是默认值。生产者发送消息之后，只要分区 leader 副本成功写入消息，就会收到来自服务端的成功响应；如果消息无法写入 <code>leader</code> 副本，会进行重试（如果设置了重试次数和间隔），不重试则会丢失消息；如果消息成功写入 <code>leader</code> 副本，也收到成功响应，但是在 <code>follower</code> 从 <code>leader</code> 拉取副本之前 <code>leader</code> 副本崩溃，那么数据丢失。设置为 此值，是吞吐量和消息可靠性之间的折中方案。</li>
<li><code>acks = &quot;0&quot;</code>。吞吐量最高的方案。在生产者发送消息之后，不需要等待服务端的任何响应。如果消息在被发送之后发生了意外，导致 Kafka 未收到此消息，那么数据丢失；写入之后的情况和 <code>acks = 1</code> 时相同。</li>
</ul>
<h2 id="2-max-request-size"><a href="#2-max-request-size" class="headerlink" title="2. max.request.size"></a>2. max.request.size</h2><p>此参数用来限制生产者能发送的消息的最大值，默认为 <code>1048576B</code>，即 1 MB。通常这个值不需要配置，如果非要配置，还请联合 <code>broker</code> 端和 <code>consumer</code> 端的一些配置进行联合配置，以免产生意外。</p>
<h2 id="3-retries-和-retry-backoff-ms"><a href="#3-retries-和-retry-backoff-ms" class="headerlink" title="3. retries 和 retry.backoff.ms"></a>3. retries 和 retry.backoff.ms</h2><ul>
<li><code>retries</code>  默认值为0，即生产者在发送发生异常（leader 重选，网络抖动等）时不进行重试。具体值为具体的重试次数</li>
<li><code>retry.backoff.ms</code>  默认为 100。两次重试之间的时间间隔，避免无效的频繁的重试。可以在写代码时预估一个处理异常的时间，避免让 Producer 过早的重试完放弃发送。</li>
</ul>
<h2 id="4-compression-type"><a href="#4-compression-type" class="headerlink" title="4. compression.type"></a>4. compression.type</h2><p>消息的压缩方式，默认为 <code>&quot;none&quot;</code>。其他选项：”gzip”、”snappy”、”lz4”。消息压缩是一种时间换空间的优化，有一定的时延。</p>
<h2 id="5-connections-max-idle-ms"><a href="#5-connections-max-idle-ms" class="headerlink" title="5. connections.max.idle.ms"></a>5. connections.max.idle.ms</h2><p>此参数用来指定 多久之后关闭限制的连接。默认值是 540000 ms，即 9 分钟。</p>
<h2 id="6-linger-ms"><a href="#6-linger-ms" class="headerlink" title="6. linger.ms"></a>6. linger.ms</h2><p>此参数用来指定生产者发送 <code>RecordBatch</code> 之前 等待更多消息 <code>ProducerRecord</code> 加入的时间，默认值是 0。<code>Producer</code> 会在 <code>RecordBatch</code> 填满或者超过 <code>linger.ms</code> 时，发送 <code>RecordBatch</code>。时延换性能的提升方案。</p>
<h2 id="7-receive-buffer-bytes"><a href="#7-receive-buffer-bytes" class="headerlink" title="7. receive.buffer.bytes"></a>7. receive.buffer.bytes</h2><p>此参数用来设置 <code>Socket</code> 接受消息缓冲区（<code>SO_RECBUF</code>）的大小，默认值 32768 B，即 32 KB。如果为 -1 ，则为操作系统的默认值。如果<code>Producer</code>和<code>Kafka</code>在不同的机房，可以适当调大此参数值。</p>
<h2 id="8-send-buffer-bytes"><a href="#8-send-buffer-bytes" class="headerlink" title="8. send.buffer.bytes"></a>8. send.buffer.bytes</h2><p>此参数用来设置 Socket 发送消息缓冲区（<code>SO_SNDBUF</code>）的大小，默认值 131072 B，即 128KB。如果为 -1，则为操作系统默认值。</p>
<h2 id="9-request-timeout-ms"><a href="#9-request-timeout-ms" class="headerlink" title="9. request.timeout.ms"></a>9. request.timeout.ms</h2><p>此参数用来配置 <code>Producer</code> 等待请求响应的最长时间，默认值为 30000 ms，即 5 分钟。请求超时之后可以选择重试。此参数最好比 broker 端参数 <code>replica.lag.time.max.ms</code> 的值要大，减小 <code>Producer</code> 重试引起的消息重复的概率。</p>
<p>​    </p>
]]></content>
      <categories>
        <category>Apache-Kafka</category>
        <category>深入理解Kafka读书笔记</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Kafka</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>消费者和消费组</title>
    <url>/2020/06/07/Apache-Kafka/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Kafka%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/10.%E6%B6%88%E8%B4%B9%E8%80%85%E5%92%8C%E6%B6%88%E8%B4%B9%E7%BB%84%E5%8F%8A%E5%BC%80%E5%8F%91/</url>
    <content><![CDATA[<h3 id="消费"><a href="#消费" class="headerlink" title="消费"></a>消费</h3><h6 id="消费者和消费组"><a href="#消费者和消费组" class="headerlink" title="消费者和消费组"></a>消费者和消费组</h6><p>对于每一个消费者 <code>Consumer</code> 而言，都有其对应的消费组 <code>Consumer Group</code>。每条消息都只会发送到每个消费组中的一个消费者。消费者数量不应该大于 <code>Topic</code> 分区数量，否则会有消费者消费不到消息的情况。</p>
<h6 id="客户端开发"><a href="#客户端开发" class="headerlink" title="客户端开发"></a>客户端开发</h6><p>和生产者类似，一个完整的消费逻辑如下：</p>
<ol>
<li>配置消费者 <code>Consumer</code> 参数以及创建相应的消费者实例</li>
<li>消费者订阅 <code>Topic</code></li>
<li>消费者拉取消息进行消费</li>
<li>消费者提交 <code>offset</code></li>
<li>关闭消费者实例</li>
</ol>
<p>先来看一个简单的 <code>Consumer demo</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> kafka.consumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyConsumer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOG = LoggerFactory.getLogger(MyConsumer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 参数初始化</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Properties <span class="title">initConfig</span><span class="params">(String brokerList, String groupID, String clientID)</span></span>&#123;</span><br><span class="line"></span><br><span class="line">        Properties prop = <span class="keyword">new</span> Properties();</span><br><span class="line">        prop.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;    <span class="comment">// key 反序列化</span></span><br><span class="line">        prop.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;  <span class="comment">// value 反序列化</span></span><br><span class="line">        prop.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,brokerList);   <span class="comment">// Kafka 服务</span></span><br><span class="line">        prop.put(ConsumerConfig.GROUP_ID_CONFIG, groupID);    <span class="comment">// Consumer Group</span></span><br><span class="line">        prop.put(ConsumerConfig.CLIENT_ID_CONFIG, clientID);    <span class="comment">// client id</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> prop;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// consumer基本信息</span></span><br><span class="line">        String topic = <span class="string">"TEST-TOPIC"</span>;</span><br><span class="line">        String brokerList = <span class="string">"tnode1:9092"</span>;</span><br><span class="line">        String groupID = <span class="string">"group.demo"</span>;</span><br><span class="line">        String clientID = <span class="string">"consumer.client.id.demo"</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 初始化配置</span></span><br><span class="line">        Properties prop = initConfig(brokerList, groupID, clientID);</span><br><span class="line">        <span class="comment">// 生产 Consumer 实例</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(prop);</span><br><span class="line">        <span class="comment">// 订阅指定的 TOPIC，可以多个，组装成 List 就行。</span></span><br><span class="line">        kafkaConsumer.subscribe(Arrays.asList(topic));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 持续不断消费数据</span></span><br><span class="line">            <span class="keyword">while</span> (<span class="keyword">true</span>)&#123;</span><br><span class="line">                <span class="comment">// 每隔 1 秒拉取一批数据</span></span><br><span class="line">                ConsumerRecords&lt;String, String&gt; records = kafkaConsumer.poll(<span class="number">1000</span>);</span><br><span class="line">                <span class="comment">// 遍历拉取的消息集 </span></span><br><span class="line">                <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record:records)&#123;</span><br><span class="line">                    <span class="comment">// 打印每条消息的topic、partition、offset、key、value</span></span><br><span class="line">                    LOG.info(record.topic()+<span class="string">":"</span>+record.partition()+<span class="string">":"</span>+record.offset()+<span class="string">" ::: "</span>+record.key()+<span class="string">":"</span>+record.value());</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">            LOG.error(<span class="string">""</span>, e);</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="comment">// 关闭消费者</span></span><br><span class="line">            kafkaConsumer.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h6 id="配置消费者-Consumer-参数以及创建相应的消费者实例"><a href="#配置消费者-Consumer-参数以及创建相应的消费者实例" class="headerlink" title="配置消费者 Consumer 参数以及创建相应的消费者实例"></a>配置消费者 Consumer 参数以及创建相应的消费者实例</h6><p>参考上面代码，类比 生产者 <code>Producer</code> 的配置，这边配置也类似，只不过多了一个 <code>group id</code> 即 <code>Consumer Group</code> 的配置，其他配置都类似，不再赘述。</p>
<p>额外提一点，<code>KafkaProducer</code> 和 <code>KafkaConsumer</code> 都有两个构造方法，第一个都是直接加载 参数的，即我们代码中使用的；第二个是 加载参数和 序列化反序列化器的。因为我们一般都在参数中设置序列化和反序列化器，所以常用的构造方法都是第一种，第二种不怎么用。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">KafkaConsumer</span><span class="params">(Map&lt;String, Object&gt; configs)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">KafkaConsumer</span><span class="params">(Map&lt;String, Object&gt; configs, Deserializer&lt;K&gt; keyDeserializer, Deserializer&lt;V&gt; valueDeserializer)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">KafkaConsumer</span><span class="params">(Properties properties)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">KafkaConsumer</span><span class="params">(Properties properties, Deserializer&lt;K&gt; keyDeserializer, Deserializer&lt;V&gt; valueDeserializer)</span></span></span><br><span class="line"><span class="function"><span class="comment">// 最终的调用</span></span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="title">KafkaConsumer</span><span class="params">(ConsumerConfig config, Deserializer&lt;K&gt; keyDeserializer, Deserializer&lt;V&gt; valueDeserializer)</span></span></span><br></pre></td></tr></table></figure>



<h6 id="消费者订阅-Topic"><a href="#消费者订阅-Topic" class="headerlink" title="消费者订阅 Topic"></a>消费者订阅 Topic</h6><p><code>kafkaConsumer</code> 订阅消息有两种类型：</p>
<ul>
<li><p><code>subscribe</code> 直接订阅 Topic，可直接指定 <code>Topic</code> 集合，也可使用正则表达式。如果不指定 <code>ReBalanceListener</code> ，则使用默认的 <code>ReBalanceListener</code>。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">subscribe</span><span class="params">(Pattern pattern, ConsumerRebalanceListener listener)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">subscribe</span><span class="params">(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener)</span></span></span><br></pre></td></tr></table></figure>

<ol>
<li><p>对于使用集合方式订阅 <code>topic</code> 的，”后发制人”，即最后订阅有效，前面的不生效。如 </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">kafkaConsumer.subscribe(Arrays.asList(<span class="string">"topic1"</span>));</span><br><span class="line">kafkaConsumer.subscribe(Arrays.asList(<span class="string">"topic2"</span>));</span><br></pre></td></tr></table></figure>

<p>最终消费订阅的是 <code>topic2</code></p>
</li>
<li><p>对于使用 正则表达式的 方式订阅 <code>topic</code>的，符合正则表达式的所有 <code>topic</code> 都会被消费，无论此 topic是在消费程序启动之前创建还是消费程序启动之后创建的。前期的版本，使用正则时，必须显示指定<code>ReBalanceListener</code> 。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">kafkaConsumer.subscribe(Pattern.compile(<span class="string">"topic-.*"</span>));</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p><code>assign</code> 订阅指定分区，参数为 <code>TopicPartition</code> 的集合。另外，<strong>这种消费方式不能自动 rebanlance</strong>。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">assign</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span></span></span><br></pre></td></tr></table></figure>

<p>先看以下 <code>TopicPartition</code> 类。主要属性就两个，一个 <code>topic</code>，一个 <code>partition</code>。作用就是映射 <code>topic-partition</code>。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">TopicPartition</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> hash = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> partition;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String topic;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">TopicPartition</span><span class="params">(String topic, <span class="keyword">int</span> partition)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.partition = partition;</span><br><span class="line">        <span class="keyword">this</span>.topic = topic;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> partition;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">topic</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> topic;</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">｝</span><br></pre></td></tr></table></figure>

<p>例子如下：<code>kafkaConsumer.assign(Arrays.asList(new TopicPartition(&quot;topic&quot;,0)));</code></p>
</li>
</ul>
<p><code>kafkaConsumer</code>  取消订阅</p>
<ol>
<li><p>几种取消的方式，效果一样。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kafkaConsumer.unsubscribe();</span><br><span class="line">kafkaConsumer.subscribe(Arrays.asList());</span><br><span class="line">kafkaConsumer.assign(new ArrayList&lt;TopicPartition&gt;());</span><br></pre></td></tr></table></figure>

<p>如果取消了订阅，那么程序会抛异常 ：</p>
<p><code>java.lang.IllegalStateException: Consumer is not subscribed to any topics or assigned any partitions</code></p>
</li>
</ol>
<p>此外，三种订阅方式（基于集合、基于正则和指定分区的）都会使消费者具有不同的状态，且互斥。因此同一个消费者只能使用其中的一种。</p>
<h6 id="消费者拉取消息进行消费"><a href="#消费者拉取消息进行消费" class="headerlink" title="消费者拉取消息进行消费"></a>消费者拉取消息进行消费</h6><p>一般消息队列的消费有两种模式：推模式和拉模式。见名知意，推模式就是服务端主动将消息推送给消费者，拉模式就是消费者主动向服务端发起拉取消息的请求。而 <code>Kafka</code> 就是基于拉模式的，通过上面的 <code>demo</code> 代码可见，<code>kafka</code> 中消费者消费是一个持续循环的 <code>poll</code> 的过程，<code>poll()</code> 方法返回的就是所订阅的主题 (分区) 的上一组消息，具体为 <code>ConsumerRecords</code>。然后轮询 <code>ConsumerRecords</code> 就可以对每一条消息 <code>ConsumerRecord</code> 进行处理。 </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>)&#123;</span><br><span class="line">	<span class="comment">// 每隔 1 秒拉取一批数据</span></span><br><span class="line">	ConsumerRecords&lt;String, String&gt; records = kafkaConsumer.poll(<span class="number">1000</span>);</span><br><span class="line">	<span class="comment">// 遍历拉取的消息集</span></span><br><span class="line">	<span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record:records)&#123;</span><br><span class="line">		<span class="comment">// 打印每条消息的topic、partition、offset、key、value</span></span><br><span class="line">		LOG.info(record.topic() + <span class="string">":"</span> + record.partition() + <span class="string">":"</span> + record.offset() + <span class="string">" ::: "</span> + record.key() + <span class="string">":"</span> + record.value());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>ConsumerRecord</code> 的属性有很多</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ConsumerRecord</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> NO_TIMESTAMP = Record.NO_TIMESTAMP;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> NULL_SIZE = -<span class="number">1</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> NULL_CHECKSUM = -<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String topic;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> partition;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> offset;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> timestamp;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> TimestampType timestampType;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> checksum;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> serializedKeySize;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> serializedValueSize;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> K key;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> V value;</span><br><span class="line">    ... <span class="comment">// 省略其他方法</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h6 id="消费者提交-offset"><a href="#消费者提交-offset" class="headerlink" title="消费者提交 offset"></a>消费者提交 offset</h6><p>在上面消费消息时，每次 poll 的数据都是未被消费过的数据，要做到标记未被消费过，就需要每次记录上一次消费后的消费的 <code>offset</code>，并且这个记录需要持久化保存，因为存在缓存或者内存可能会丢失。另外，在 <code>Consumer Group</code> 成员变动时，对于同一分区，可能在发生 <code>rebanlance</code> 之后，被不同消费者消费了，也需要持久化记录消费的 <code>offset</code>。</p>
<p><strong><em>注意！！提交的 offset 是下次拉取的 offset ，而不是现在消费完的 offset 。</em></strong></p>
<p>当然，在 Kafka 中也有不同的 offset 提交方式，有默认的自动提交，也有显示的手动提交。手动提交又分同步和异步两种：</p>
<ul>
<li><p>自动提交 <code>offset</code> ：<code>Kafka</code> 周期性的自动提交 offset。<code>enable.auto.commit = true</code></p>
</li>
<li><p>同步提交 <code>offset</code> ：<code>enable.auto.commit = false</code> 每次提交 offset 时，会阻塞线程直到 offset 提交成功才能进行下一次 <code>poll</code> 消息。无参的 <code>commitSync()</code> 是每次 <code>poll</code> 的批量提交，带参数 (TP 和 <code>offset</code> 的 Map) 的是针对每条或每个分区的消息的 <code>offset</code> 进行提交，如果是每条消息进行提交，那么性能将最低；如果是每个分区提交，那么可以类似这样子做：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="comment">// 持续不断消费数据</span></span><br><span class="line">          <span class="keyword">while</span> (<span class="keyword">true</span>)&#123;</span><br><span class="line">              <span class="comment">// 每隔 1 秒拉取一批数据</span></span><br><span class="line">              ConsumerRecords&lt;String, String&gt; records = kafkaConsumer.poll(<span class="number">1000</span>);</span><br><span class="line">              <span class="comment">// 遍历分区</span></span><br><span class="line">              <span class="keyword">for</span> (TopicPartition tp:records.partitions())&#123;</span><br><span class="line">                  <span class="comment">// 获取每个分区的消息</span></span><br><span class="line">                  List&lt;ConsumerRecord&lt;String, String&gt;&gt; tpRecords = records.records(tp);</span><br><span class="line">                  <span class="comment">// 遍历每个分区的消息集合</span></span><br><span class="line">                  <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record:tpRecords)&#123;</span><br><span class="line">                      <span class="comment">// 业务逻辑处理</span></span><br><span class="line">                      LOG.info(record.topic()+<span class="string">":"</span>+record.partition()+<span class="string">":"</span>+record.offset()+<span class="string">" ::: "</span>+record.key()+<span class="string">":"</span>+record.value());</span><br><span class="line">                  &#125;</span><br><span class="line">                  <span class="comment">// 同步提交每个分区的 offset </span></span><br><span class="line">                  <span class="comment">// tpRecords.get(tpRecords.size()-1).offset() 最后一条消息的offset</span></span><br><span class="line">                  kafkaConsumer.commitSync(Collections.singletonMap(tp, <span class="keyword">new</span> OffsetAndMetadata( tpRecords.get(tpRecords.size()-<span class="number">1</span>).offset() + <span class="number">1</span>)));</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">          LOG.error(<span class="string">""</span>, e);</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">          <span class="comment">// 关闭消费者</span></span><br><span class="line">          kafkaConsumer.close();</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure>



</li>
</ul>
<ul>
<li><p>异步提交 <code>offset</code> ：<code>enable.auto.commit = false</code> 和同步的相反，异步提交 <code>offset</code> 时，不会阻塞消费，可能还没提交成功就已经拉取完下一批数据了。异步提交可以一定程度的提高性能。异步提交有三个重载方法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commitAsync</span><span class="params">()</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commitAsync</span><span class="params">(OffsetCommitCallback callback)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commitAsync</span><span class="params">(<span class="keyword">final</span> Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, OffsetCommitCallback callback)</span></span></span><br></pre></td></tr></table></figure>

<p>第一个和第三个中的 <code>offset</code> 和同步提交的一样，不一样的就是第二个和第三个的 <code>OffsetCommitCallback</code>。异步和同步最大的不同就是，有可能前一批 <code>offset</code> 提交失败，但是后一批提交成功，这样子容易造成重复消费。如下代码，<code>OffsetCommitCallback</code> 类，在提交完成之后会回调其中的 <code>onComplete()</code> 方法，我们可以在这个方法里面进行判断成功或者失败，然后进行相应的处理。假设提交失败的话，如果需要进行重试，那么要注意判断此次提交的  offset 和已经提交成功的 offset 的大小，然后才能知道是否需要提交 <code>offset</code> （如果后一批提交成功，那么已经提交成功的 offset 就肯定会比前一批提交失败的 <code>offset</code> 大）。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>)&#123;</span><br><span class="line">           <span class="comment">// 每隔 1 秒拉取一批数据</span></span><br><span class="line">           ConsumerRecords&lt;String, String&gt; records = kafkaConsumer.poll(<span class="number">1000</span>);</span><br><span class="line">           <span class="comment">// 遍历拉取的消息集</span></span><br><span class="line">           <span class="keyword">for</span> (TopicPartition tp:records.partitions())&#123;</span><br><span class="line">               List&lt;ConsumerRecord&lt;String, String&gt;&gt; tpRecords = records.records(tp);</span><br><span class="line">               <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record:tpRecords)&#123;</span><br><span class="line">                   LOG.info(record.topic()+<span class="string">":"</span>+record.partition()+<span class="string">":"</span>+record.offset()+<span class="string">" ::: "</span>+record.key()+<span class="string">":"</span>+record.value());</span><br><span class="line">               &#125;</span><br><span class="line">               <span class="comment">// 异步提交每个分区的 offset </span></span><br><span class="line">               <span class="comment">// tpRecords.get(tpRecords.size()-1).offset() 最后一条消息的offset</span></span><br><span class="line">               kafkaConsumer.commitAsync( Collections.singletonMap(tp, <span class="keyword">new</span> OffsetAndMetadata(tpRecords.get(tpRecords.size() - <span class="number">1</span>).offset() + <span class="number">1</span>)),</span><br><span class="line">                       <span class="keyword">new</span> OffsetCommitCallback() &#123;</span><br><span class="line">                   <span class="meta">@Override</span></span><br><span class="line">                   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onComplete</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, Exception exception)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">if</span> (exception == <span class="keyword">null</span>) &#123;</span><br><span class="line">                            LOG.info(<span class="string">"offset 提交成功："</span> + offsets);</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            LOG.error(<span class="string">"kafka commit offset failure!!"</span>);</span><br><span class="line">                        &#125;</span><br><span class="line">                   &#125;</span><br><span class="line">               &#125;);</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br></pre></td></tr></table></figure>



</li>
</ul>
<h6 id="关闭消费者实例"><a href="#关闭消费者实例" class="headerlink" title="关闭消费者实例"></a>关闭消费者实例</h6><p>正常情况下，消费者是一直都在运行的，但是如果我们需要暂定或者关闭消费者，就需要考虑怎么退出消费的循环了。如果是暂定，则需要考虑如何恢复。</p>
<p>先看简单的暂定和恢复，<code>KafkaConsumer</code> 提供了 <code>pause()</code> 和 <code>resume()</code> 来暂停和恢复 <code>kafkaConsumer</code> 对某些分区的消费。还有一个无参的 <code>paused()</code> 的方法来返回已经暂定的方法。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">pause</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span>   <span class="comment">// 暂定某些分区的消费</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">resume</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span>  <span class="comment">// 恢复某些已经暂定的分区的消费</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Set&lt;TopicPartition&gt; <span class="title">paused</span><span class="params">()</span>	                       <span class="comment">// 返回已经暂定的分区列表</span></span></span><br></pre></td></tr></table></figure>

<p>然后，如何优雅的退出 <code>poll</code> 的那层循环呢。一般来说有两种方式：</p>
<ul>
<li>开始 <code>poll</code> 的那个循环不用试固定的 <code>true</code>，而是做成一个能开关的全局变量，这样子就能在其他地方也可以直接中断循环</li>
<li>直接调用 <code>Kafkaconsumer</code> 的 <code>wakeup()</code> 方法。该方法会退出 <code>poll</code> 的循环并抛出一个 <code>wakeup</code> 的异常</li>
</ul>
<p>在退出循环之后，需要显示的关闭 <code>KafkaConsumer</code>，以释放资源。关闭实例有三个重载方法：  </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">kafkaConsumer.close();     <span class="comment">// 默认 30 秒，也是调用的后面的方法</span></span><br><span class="line">kafkaConsumer.close(<span class="number">30000</span>, TimeUnit.MILLISECONDS);   <span class="comment">// 因为第三种方式的出现，现已经取消。</span></span><br><span class="line">kafkaConsumer.close(Duration.ofMillis(<span class="number">30000</span>));</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Apache-Kafka</category>
        <category>深入理解Kafka读书笔记</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Kafka</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>反序列化</title>
    <url>/2020/06/08/Apache-Kafka/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Kafka%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/11.%E6%B6%88%E8%B4%B9%E8%80%85%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96/</url>
    <content><![CDATA[<h3 id="反序列化"><a href="#反序列化" class="headerlink" title="反序列化"></a>反序列化</h3><p>和生产者端的序列化器类似，消费者端对应的是反序列化器。Kafka 中反序列化器实现的都是 <code>Deserializer</code> 接口，默认实现有：</p>
<ul>
<li><code>ByteBufferDeserializer</code> </li>
<li><code>BytesDeserializer</code> </li>
<li><code>DoubleDeserializer</code> </li>
<li><code>FloatDeserializer</code> </li>
<li><code>IntegerDeserializer</code> </li>
<li><code>LongDeserializer</code> </li>
<li><code>ShortDeserializer</code> </li>
<li><code>StringDeserializer</code> </li>
</ul>
<p>反序列化 Deserializer 接口也和 Serializer 接口一样，需要实现三个方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Deserializer</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">Closeable</span> </span>&#123;</span><br><span class="line">	<span class="comment">// 用来配置当前类</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs, <span class="keyword">boolean</span> isKey)</span></span>;</span><br><span class="line">    <span class="comment">// 用来反序列化，如果 data 为 null，需要返回 null，而不是抛异常</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> T <span class="title">deserialize</span><span class="params">(String topic, <span class="keyword">byte</span>[] data)</span></span>;</span><br><span class="line">	<span class="comment">// 关闭序列化器，一般是空方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>常用的 <code>StringDeserializer</code>  </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StringDeserializer</span> <span class="keyword">implements</span> <span class="title">Deserializer</span>&lt;<span class="title">String</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">// 默认字符编码</span></span><br><span class="line">    <span class="keyword">private</span> String encoding = <span class="string">"UTF8"</span>;</span><br><span class="line">	</span><br><span class="line">    <span class="comment">// 获取配置中设置的字符编码，如果设置了的话 </span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs, <span class="keyword">boolean</span> isKey)</span> </span>&#123;</span><br><span class="line">        String propertyName = isKey ? <span class="string">"key.deserializer.encoding"</span> : <span class="string">"value.deserializer.encoding"</span>;</span><br><span class="line">        Object encodingValue = configs.get(propertyName);</span><br><span class="line">        <span class="keyword">if</span> (encodingValue == <span class="keyword">null</span>)</span><br><span class="line">            encodingValue = configs.get(<span class="string">"deserializer.encoding"</span>);</span><br><span class="line">        <span class="keyword">if</span> (encodingValue != <span class="keyword">null</span> &amp;&amp; encodingValue <span class="keyword">instanceof</span> String)</span><br><span class="line">            encoding = (String) encodingValue;</span><br><span class="line">    &#125;</span><br><span class="line">	</span><br><span class="line">    <span class="comment">// 如果数据是 null，返回 null。否则进行反序列化</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">deserialize</span><span class="params">(String topic, <span class="keyword">byte</span>[] data)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (data == <span class="keyword">null</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> String(data, encoding);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (UnsupportedEncodingException e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> SerializationException(<span class="string">"Error when deserializing byte[] to string due to unsupported encoding "</span> + encoding);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// nothing to do</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>自定义反序列化器 PersonDeserializer ：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> kafka.serializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> kafka.utils.Person;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang.SerializationException;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.Deserializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.ByteArrayInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.ObjectInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.UnsupportedEncodingException;</span><br><span class="line"><span class="keyword">import</span> java.nio.ByteBuffer;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PersonDeserializer</span> <span class="keyword">implements</span> <span class="title">Deserializer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 默认字符编码</span></span><br><span class="line">    <span class="keyword">private</span> String encoding = <span class="string">"UTF-8"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map configs, <span class="keyword">boolean</span> isKey)</span> </span>&#123;</span><br><span class="line">        String propertyName = isKey ? <span class="string">"key.deserializer.encoding"</span> : <span class="string">"value.deserializer.encoding"</span>;</span><br><span class="line">        Object encodingValue = configs.get(propertyName);</span><br><span class="line">        <span class="keyword">if</span> (encodingValue == <span class="keyword">null</span>)&#123;</span><br><span class="line">            encodingValue = configs.get(<span class="string">"deserializer.encoding"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (encodingValue != <span class="keyword">null</span> &amp;&amp; encodingValue <span class="keyword">instanceof</span> String)&#123;</span><br><span class="line">            encoding = (String) encodingValue;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">deserialize</span><span class="params">(String topic, <span class="keyword">byte</span>[] data)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 数据为空直接返回</span></span><br><span class="line">        <span class="keyword">if</span> (data == <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        <span class="comment">// 定义返回类型</span></span><br><span class="line">        Person person = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            ByteArrayInputStream byteArrayInputStream = <span class="keyword">new</span> ByteArrayInputStream(data);</span><br><span class="line">            ObjectInputStream objectInputStream = <span class="keyword">new</span> ObjectInputStream(byteArrayInputStream);</span><br><span class="line">            <span class="comment">// 反序列化对象，此对象需要实现 java.io.Serializer 接口</span></span><br><span class="line">            person = (Person)objectInputStream.readObject();</span><br><span class="line">            </span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">            System.out.println(e);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> person;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 空方法</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>使用反序列化器只需要在 consumer 的配置中加上即可。需要注意的是，consumer 端的反序列化器需要和 producer 端的序列化器解耦，不然无法正常消费。同时，如果有拦截器的话，那么需要注意拦截器中的操作，不能使得序列化器无法序列。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">prop.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, PersonDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Apache-Kafka</category>
        <category>深入理解Kafka读书笔记</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Kafka</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>消费者拦截器</title>
    <url>/2020/06/09/Apache-Kafka/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Kafka%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/12.%E6%B6%88%E8%B4%B9%E8%80%85%E6%8B%A6%E6%88%AA%E5%99%A8/</url>
    <content><![CDATA[<h3 id="消费者拦截器"><a href="#消费者拦截器" class="headerlink" title="消费者拦截器"></a>消费者拦截器</h3><p>原理和生产者的原理一样，主要用于在消费到消息时或者提交 offset 时进行定制化操作。先看看接口的代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ConsumerInterceptor</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Configurable</span> </span>&#123;</span><br><span class="line">	<span class="comment">// poll 返回数据之前会回调此方法。</span></span><br><span class="line">	<span class="comment">// 这里对消息进行定制化，过滤等操作</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ConsumerRecords&lt;K, V&gt; <span class="title">onConsume</span><span class="params">(ConsumerRecords&lt;K, V&gt; records)</span></span>;</span><br><span class="line">	<span class="comment">// 提交完 offset 之后会回调这个方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCommit</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets)</span></span>;</span><br><span class="line">	<span class="comment">// 释放资源</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>简单的实现，根据 ID 进行消息过滤，并去掉消息开头的 first。另外，打印每次提交的<code>offset</code>信息</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> kafka.interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerInterceptor;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.OffsetAndMetadata;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.TopicPartition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyConsumerInterceptor</span> <span class="keyword">implements</span> <span class="title">ConsumerInterceptor</span>&lt;<span class="title">String</span>,<span class="title">String</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 不做任何配置</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ConsumerRecords&lt;String,String&gt; <span class="title">onConsume</span><span class="params">(ConsumerRecords&lt;String,String&gt; records)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 初始化一个 List，来收集需要返回的 ConsumerRecord</span></span><br><span class="line">        HashMap&lt;TopicPartition, List&lt;ConsumerRecord&lt;String,String&gt;&gt;&gt; recordsHashMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        <span class="comment">// 遍历每一个分区</span></span><br><span class="line">        <span class="keyword">for</span> (TopicPartition tp:records.partitions())&#123;</span><br><span class="line">            <span class="comment">// List 收集每个分区内需要消费的数据</span></span><br><span class="line">            ArrayList&lt;ConsumerRecord&lt;String, String&gt;&gt; tpRecords = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">            <span class="comment">// 遍历分区内的消息集合</span></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String,String&gt; record:records.records(tp))&#123;</span><br><span class="line">                <span class="comment">// ID 能整除 2 的才消费</span></span><br><span class="line">                <span class="keyword">if</span> (Integer.valueOf(record.value().split(<span class="string">":"</span>)[<span class="number">1</span>])%<span class="number">2</span> == <span class="number">0</span>)&#123;</span><br><span class="line">                    <span class="comment">// value 部分，去掉开头的 first：</span></span><br><span class="line">                    tpRecords.add(<span class="keyword">new</span> ConsumerRecord&lt;String,String&gt;(</span><br><span class="line">                            record.topic(),</span><br><span class="line">                            record.partition(),</span><br><span class="line">                            record.offset(),</span><br><span class="line">                            record.timestamp(),</span><br><span class="line">                            record.timestampType(),</span><br><span class="line">                            record.checksum(),</span><br><span class="line">                            record.serializedKeySize(),</span><br><span class="line">                            record.serializedValueSize(),</span><br><span class="line">                            record.key(),</span><br><span class="line">                            record.value().substring(<span class="number">6</span>)));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 将每个分区的消息集合添加到 Map</span></span><br><span class="line">            recordsHashMap.put(tp, tpRecords);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 将 Map 转成 ConsumerRecords 并返回</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ConsumerRecords&lt;&gt;(recordsHashMap);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCommit</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        offsets.forEach((tp,offset) -&gt; System.out.println(tp+<span class="string">":"</span>+offset.offset()));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 空方法</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>使用该拦截器也很简单，只需要在 <code>KafkaConsumer</code> 中的配置中添加即可，如果是拦截链，方法和生产者的拦截链一样。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">prop.put(ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, MyConsumerInterceptor<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Apache-Kafka</category>
        <category>深入理解Kafka读书笔记</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Kafka</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>指定位移消费</title>
    <url>/2020/06/10/Apache-Kafka/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Kafka%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/13.%E6%8C%87%E5%AE%9A%E4%BD%8D%E7%A7%BB%E6%B6%88%E8%B4%B9/</url>
    <content><![CDATA[<h3 id="指定位移消费"><a href="#指定位移消费" class="headerlink" title="指定位移消费"></a>指定位移消费</h3><p> 在看指定 Offset 消费之前，先看一个参数 <code>auto.offset.reset</code> ，这个参数有三个值：<code>earliest</code>，<code>latest</code>，<code>none</code>，默认值是 latest。通常情况下，我们在配置 consumer 时，我们都会指定 earliest 或 latest。因为当consumer消费时找不到正常的offset时，前者让 consumer 从头 (最早 offset) 开始消费，而后者让 consumer 从尾 (最新 offset) 开始消费。但是 none 在这种情况下就会直接抛异常了，即找不到 offset。</p>
<p>上面所述都是在消费整个 topic 的情况下，如果需要指定消费某个分区的话，那么就需要具体指定到 TopicPartition 的 offset了。指定的方式为 KafkaConsumer 中的 seek() 方法。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// seek 只能重置消费者分配到的TopicPartition的offset，所以使用seek之前需要先使用一次poll或者使用一次assign方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">seek</span><span class="params">(TopicPartition partition, <span class="keyword">long</span> offset)</span></span></span><br><span class="line"><span class="function"><span class="comment">// </span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">seekToBeginning</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span></span></span><br><span class="line"><span class="function"><span class="comment">// </span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">seekToEnd</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span></span></span><br></pre></td></tr></table></figure>

<p>如代码所述，因为使用 <code>seek()</code> 方法的时候需要知道当前消费者所消费的分区，所以必须再 <code>subscribe</code> 订阅经过一次poll 或者 直接使用 <code>assign</code> 指定分区之后才能使用 seek。</p>
<p>seek() 的使用场景：在消费者重启时，如果能找到对应的 offset，此时如果 offset 无越界情况，那么 <code>auto.offset.reset</code> 参数就不会生效。但是又想让 消费者 从头、从尾或者指定位移消费时，就只能通过上面三个方法进行消费了。</p>
<p>另外，通过 seek 方案，我们还可以将 <code>offset</code> 自由保存在 <code>ZK</code>，<code>HBase</code>等之上，实现 <code>exactly-once</code> 的语义保证。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 从 Zookeeper 对应路径 /$zookeeperNameSpace/$baseZkPath/topic 下获取各分区offset</span></span><br><span class="line">        HashMap&lt;TopicPartition, Long&gt; topicPartitionLongHashMap = MyUtils.getOffsetFromZk(client, groupID, topicConsumer, baseZkPath);</span><br><span class="line">        <span class="comment">// consumer 指定partition offset消费</span></span><br><span class="line">        <span class="keyword">if</span>(!topicPartitionLongHashMap.isEmpty())&#123;</span><br><span class="line">            <span class="comment">// assign 指定 consumer 消费的分区</span></span><br><span class="line">            kafkaConsumer.assign(topicPartitionLongHashMap.keySet());</span><br><span class="line">            <span class="keyword">for</span> (TopicPartition tp:topicPartitionLongHashMap.keySet())&#123;</span><br><span class="line">                <span class="comment">// 指定分区的具体 offset+1</span></span><br><span class="line">                kafkaConsumer.seek(tp,topicPartitionLongHashMap.get(tp)+<span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            kafkaConsumer.subscribe(Arrays.asList(topicConsumer));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// Map 存储 最新的offset</span></span><br><span class="line">        HashMap&lt;String, Long&gt; topicOffsetHashMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            ConsumerRecords&lt;String,String&gt; records = kafkaConsumer.poll(<span class="number">100</span>);</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                <span class="comment">// 获取消息数据，并转成json</span></span><br><span class="line">                String data =record.value();</span><br><span class="line">                JSONObject json = JSONObject.parseObject(data);</span><br><span class="line">                <span class="comment">// 当前的topic和partition 装成 TopicPartition</span></span><br><span class="line">                String topicPartition = record.topic()+<span class="string">"_"</span>+record.partition();</span><br><span class="line">                <span class="comment">// 覆盖 topicOffsetHashMap 中对应的 offset 信息</span></span><br><span class="line">                topicOffsetHashMap.put(topicPartition, record.offset());</span><br><span class="line">                    </span><br><span class="line">                System.out.println(json);</span><br><span class="line">            &#125;</span><br><span class="line">                </span><br><span class="line">            <span class="comment">// 保存 offSet 到 Zookeeper</span></span><br><span class="line">            <span class="keyword">for</span> (String s:topicOffsetHashMap.keySet())&#123;</span><br><span class="line">                Long offset = topicOffsetHashMap.get(s);</span><br><span class="line">                MyUtils.storeOffsetToZk(client, baseZkPath,groupID, s,offset);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>







]]></content>
      <categories>
        <category>Apache-Kafka</category>
        <category>深入理解Kafka读书笔记</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Kafka</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>多线程消费</title>
    <url>/2020/06/11/Apache-Kafka/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Kafka%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/14.%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%B6%88%E8%B4%B9/</url>
    <content><![CDATA[<h2 id="多线程消费"><a href="#多线程消费" class="headerlink" title="多线程消费"></a>多线程消费</h2><p>总所周知，KafkaProducer 是线程安全的，而 KafkaConsumer 是非线程安全的。</p>
]]></content>
      <categories>
        <category>Apache-Kafka</category>
        <category>深入理解Kafka读书笔记</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Kafka</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>多线程消费</title>
    <url>/2020/06/12/Apache-Kafka/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Kafka%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/15.%E9%87%8D%E8%A6%81%E7%9A%84%E6%B6%88%E8%B4%B9%E8%80%85%E5%8F%82%E6%95%B0/</url>
    <content><![CDATA[<h3 id="重要的消费者参数"><a href="#重要的消费者参数" class="headerlink" title="重要的消费者参数"></a>重要的消费者参数</h3><h4 id="1-fetch-min-bytes"><a href="#1-fetch-min-bytes" class="headerlink" title="1.fetch.min.bytes"></a>1.fetch.min.bytes</h4><p>默认值为 1 B。consumer 在一次 poll 时，拉取的最小数据量，小于该值时，consumer 需要进行等待，直到满足该参数的大小。当吞吐要求高时可以适当调大此参数用延迟换取吞吐。</p>
<h4 id="2-fetch-max-bytes"><a href="#2-fetch-max-bytes" class="headerlink" title="2.fetch.max.bytes"></a>2.fetch.max.bytes</h4><p>默认值为 52428800 B，即 50 MB。与最小对应，此参数表示每次拉取的最大数据量。这个最大指的是多条消息的情况下，如果是单挑消息超过 该参数值，那么还是能正常消费的。而限制消息大小的参数是通过 broker 端的参数 message.max.bytes 来设置的</p>
<h4 id="3-fetch-max-wait-ms"><a href="#3-fetch-max-wait-ms" class="headerlink" title="3.fetch.max.wait.ms"></a>3.fetch.max.wait.ms</h4><p>如果一直没有达到 fetch.min.bytes 参数要求的数据量时，不能一直阻塞等待导致 consumer收不到响应信息，因此使用 此参数 来限制当没达到指定数据量时的等待时间，默认为 500 ms 。对于延迟要求较高的场景可以选择调低此参数值。</p>
<h4 id="4-max-partition-fetch-bytes"><a href="#4-max-partition-fetch-bytes" class="headerlink" title="4.max.partition.fetch.bytes"></a>4.max.partition.fetch.bytes</h4><p>默认值为 1048576 B，即 1 MB。此参数配置的是一次poll中 ，每个分区的最大数据量。与 fetch.max.bytes 类似。一个是分区范围，一个是整体范围。如果单条超过参数最大值，也不会影响消费。</p>
<h4 id="5-max-poll-records"><a href="#5-max-poll-records" class="headerlink" title="5.max.poll.records"></a>5.max.poll.records</h4><p>默认值为 500。配置 consumer 一次拉取的最大消息数量。增加此参数值可以提高消费速度，即吞吐量。</p>
<h4 id="6-connections-max-idle-ms"><a href="#6-connections-max-idle-ms" class="headerlink" title="6.connections.max.idle.ms"></a>6.connections.max.idle.ms</h4><p>默认值 540000 ms，即 9 分钟。指定多久之后关闭限制的连接。</p>
<h4 id="7-exclude-internal-topics"><a href="#7-exclude-internal-topics" class="headerlink" title="7.exclude.internal.topics"></a>7.exclude.internal.topics</h4><p>默认值为 true。设置 Kafka 中默认主题是否可以向 consumer 公开，如果公开，那么只能使用 subscribe(collection)  来订阅内部主题，而不能使用正则方式 ( subscribe(Pattern) ) 。</p>
<h4 id="8-receive-buffer-bytes"><a href="#8-receive-buffer-bytes" class="headerlink" title="8.receive.buffer.bytes"></a>8.receive.buffer.bytes</h4><p>此参数用来设置 <code>Socket</code> 接受消息缓冲区（<code>SO_RECBUF</code>）的大小，默认值 32768 B，即 32 KB。如果为 -1 ，则为操作系统的默认值。如果<code>Producer</code>和<code>Kafka</code>在不同的机房，可以适当调大此参数值。</p>
<h4 id="9-send-buffer-bytes"><a href="#9-send-buffer-bytes" class="headerlink" title="9.send.buffer.bytes"></a>9.send.buffer.bytes</h4><p>此参数用来设置 Socket 发送消息缓冲区（<code>SO_SNDBUF</code>）的大小，默认值 131072 B，即 128KB。如果为 -1，则为操作系统默认值。</p>
<h4 id="10-request-timeout-ms"><a href="#10-request-timeout-ms" class="headerlink" title="10.request.timeout.ms"></a>10.request.timeout.ms</h4><p>此参数用来配置 <code>Producer</code> 等待请求响应的最长时间，默认值为 30000 ms，即 5 分钟。请求超时之后可以选择重试。此参数最好比 broker 端参数 <code>replica.lag.time.max.ms</code> 的值要大，减小 <code>Producer</code> 重试引起的消息重复的概率。</p>
<h4 id="11-metadata-max-age-ms"><a href="#11-metadata-max-age-ms" class="headerlink" title="11.metadata.max.age.ms"></a>11.metadata.max.age.ms</h4><p>默认值为 300000 ms，即 5 分钟。元数据的过期时间，如果此时间内没有进行更新，那么将强制更新，即使没有任何分区变化或者新的broker加入等情况。</p>
<h4 id="12-reconnect-backoff-ms"><a href="#12-reconnect-backoff-ms" class="headerlink" title="12.reconnect.backoff.ms"></a>12.reconnect.backoff.ms</h4><p>退避时间。</p>
]]></content>
      <categories>
        <category>Apache-Kafka</category>
        <category>深入理解Kafka读书笔记</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Kafka</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Topic的管理</title>
    <url>/2020/06/13/Apache-Kafka/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Kafka%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/20.Topic%E7%9A%84%E7%AE%A1%E7%90%86-KafkaAdminClient%20/</url>
    <content><![CDATA[<h4 id="Topic的管理"><a href="#Topic的管理" class="headerlink" title="Topic的管理"></a>Topic的管理</h4><p>Topic的管理包括创建、查看、修改和删除 topic 等操作。四个操作均可以用 Kafka 提供的脚本              <code>kafka-topic.sh</code> 来执行。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# cat kafka-topics.sh </span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Licensed to the Apache Software Foundation (ASF) under one or more</span></span><br><span class="line"></span><br><span class="line">exec $(dirname $0)/kafka-run-class.sh kafka.admin.TopicCommand "$@"</span><br></pre></td></tr></table></figure>

<p>可以看到实际上 脚本是通过调用这个类来实现的，我们也可以直接通过 KafkaAdminClient 的方式来实现这些操作。脚本的操作方式可以看 本笔记 Kafka主目录下的<br>[Kafka客户端命令操作]: F:\myGit\DT-Learner\Kafka\Kafka客户端命令操作.md    “文档1”</p>
<p> 然后看 KafkaAdminClient 的实现</p>
<h4 id="初始化-KafkaAdminClient-配置"><a href="#初始化-KafkaAdminClient-配置" class="headerlink" title="初始化 KafkaAdminClient 配置"></a>初始化 KafkaAdminClient 配置</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Properties <span class="title">initKafkaClient</span><span class="params">()</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    String brokerList = <span class="string">"tnode3:9092"</span>;</span><br><span class="line">    Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">    properties.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList);</span><br><span class="line">    properties.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG, <span class="number">30000</span>);</span><br><span class="line">    <span class="keyword">return</span> properties;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h4 id="创建-Topic"><a href="#创建-Topic" class="headerlink" title="创建 Topic"></a>创建 Topic</h4><p>单个topic，最简配置。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// init client</span></span><br><span class="line">AdminClient adminClient = AdminClient.create(initKafkaClient());</span><br><span class="line"></span><br><span class="line">String topic0011 = <span class="string">"topic001"</span>;</span><br><span class="line"><span class="comment">// new topic</span></span><br><span class="line">NewTopic topic001 = <span class="keyword">new</span> NewTopic(topic0011, <span class="number">3</span>, (<span class="keyword">short</span>) <span class="number">2</span>);</span><br><span class="line"><span class="comment">// create topic</span></span><br><span class="line">CreateTopicsResult topicsResult = adminClient.createTopics(Collections.singleton(topic001));</span><br><span class="line"><span class="comment">// 获取创建结果是否有异常</span></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    topicsResult.all().get();</span><br><span class="line">    System.out.println(<span class="string">"Topic:"</span> + topic0011 + <span class="string">"创建成功"</span>);</span><br><span class="line">&#125; <span class="keyword">catch</span> (InterruptedException | ExecutionException e)&#123;</span><br><span class="line">    e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line">adminClient.close();</span><br></pre></td></tr></table></figure>

<p>多个 topic ，复杂配置</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">AdminClient adminClient = AdminClient.create(initKafkaClient());</span><br><span class="line"></span><br><span class="line">String topic001 = <span class="string">"topic001"</span>;</span><br><span class="line">String topic002 = <span class="string">"topic002"</span>;</span><br><span class="line">        </span><br><span class="line"><span class="comment">// 删除 Topic</span></span><br><span class="line">adminClient.deleteTopics(Arrays.asList(topic001,topic002));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第一个 Topic 配置</span></span><br><span class="line">NewTopic newTopic001 = <span class="keyword">new</span> NewTopic(topic001, <span class="number">3</span>, (<span class="keyword">short</span>) <span class="number">2</span>);</span><br><span class="line"><span class="comment">// 第二个 Topic 配置，指定明细的副本和分区策略</span></span><br><span class="line">HashMap&lt;Integer, List&lt;Integer&gt;&gt; replicasAssignments002 = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">replicasAssignments002.put(<span class="number">0</span>, Arrays.asList(<span class="number">0</span>,<span class="number">1</span>));</span><br><span class="line">replicasAssignments002.put(<span class="number">1</span>, Arrays.asList(<span class="number">1</span>,<span class="number">0</span>));</span><br><span class="line">NewTopic newTopic002 = <span class="keyword">new</span> NewTopic(topic002, replicasAssignments002);</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    CreateTopicsResult topicsResult = adminClient.createTopics(Arrays.asList(newTopic001, newTopic002));</span><br><span class="line">    topicsResult.all().get();</span><br><span class="line">    System.out.println(<span class="string">"Topic:"</span> + topic001 + <span class="string">"创建成功"</span>);</span><br><span class="line">&#125; <span class="keyword">catch</span> (InterruptedException | ExecutionException e)&#123;</span><br><span class="line">    System.out.println(e.getMessage());</span><br><span class="line">&#125;</span><br><span class="line">adminClient.close();</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Apache-Kafka</category>
        <category>深入理解Kafka读书笔记</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Kafka</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>分区的管理</title>
    <url>/2020/06/15/Apache-Kafka/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Kafka%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/21.%E5%88%86%E5%8C%BA%E7%9A%84%E7%AE%A1%E7%90%86/</url>
    <content><![CDATA[<h3 id="优先副本的选举"><a href="#优先副本的选举" class="headerlink" title="优先副本的选举"></a>优先副本的选举</h3><p>   Kafka 通过副本机制来提升服务的可靠性，但是只有 leader 副本提供对外的读写服务，这样就意味着，当 leader 丢失时，需要从 follower 副本中选举一个 leader 来提供服务。Kafka 在我们创建 Topic 的时候，会尽量的把分区和副本均匀的分布在所有的 broker 上，同时 leader 副本也会比较均匀，以此来保证整个集群的负载均衡和稳定性。例如我们创建一个 Topic：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode2 bin]# ./kafka-topics.sh --create --zookeeper tnode3:2181 --replication-factor 3 --partitions 3 --topic topic003</span><br><span class="line">Created topic "topic003".</span><br><span class="line">[root@tnode2 bin]# ./kafka-topics.sh --describe --zookeeper tnode3:2181 --topic topic003</span><br><span class="line">Topic:topic003	PartitionCount:3	ReplicationFactor:3	Configs:</span><br><span class="line">	Topic: topic003	Partition: 0	Leader: 0	Replicas: 0,2,1	Isr: 0,2,1</span><br><span class="line">	Topic: topic003	Partition: 1	Leader: 1	Replicas: 1,0,2	Isr: 1,0,2</span><br><span class="line">	Topic: topic003	Partition: 2	Leader: 2	Replicas: 2,1,0	Isr: 2,1,0</span><br></pre></td></tr></table></figure>

<p>可以看出，三个分区分别在不同的 broker 上，同时每个分区的每个副本都在不同的 broker 上 ( 副本数是不能超过 broker 数的，否则创建时会报错 ) ，另外，三个分区的 leader 也分布在不同的 broker 上。但是时间久了，总会有 leader 副本丢失，导致 副本的选举，重新出现新的 leader。假设我们将 broker2 重启一下，再看 Topic 情况。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode2 bin]# ./kafka-topics.sh --describe --zookeeper tnode3:2181 --topic topic003</span><br><span class="line">Topic:topic003	PartitionCount:3	ReplicationFactor:3	Configs:</span><br><span class="line">	Topic: topic003	Partition: 0	Leader: 0	Replicas: 0,2,1	Isr: 0,1,2</span><br><span class="line">	Topic: topic003	Partition: 1	Leader: 1	Replicas: 1,0,2	Isr: 1,0,2</span><br><span class="line">	Topic: topic003	Partition: 2	Leader: 1	Replicas: 2,1,0	Isr: 1,0,2</span><br></pre></td></tr></table></figure>

<p>很明显，在重启之后，Topic 的 leader副本发生了变化，即发生了副本的重新选举，如果这种选举经常发生的话，就很有可能导致负载很不均衡。在 Kafka 中，为了保证集群的稳定性和可靠性，引入了一个优先副本的机制，即在创建 Topic 的时候，AR 列表的第一个副本为 优先副本，尽可能会将它选为 leader 副本。</p>
<p>同时，为了保证所有分区的 leader 副本均衡分布，Kafka 有几种方式来进行分区平衡。</p>
<ul>
<li><p>broker端参数</p>
<p>auto.leader.rebalance.enable 默认为 true，即开启自动分区平衡。当开启自动时，Kafka 的控制器会启动定时任务，定时（周期由 <code>leader.imbalance.check.interval.seconds</code> 控制，默认为300秒，即 5分钟）轮询所有 broker 节点，计算每个 broker 节点的分区不平衡率（非优先副本 leader 数 / 分区总数），如果超过一定值（<code>leader.imbalance.per.broker.percentage</code>，默认为10%）。</p>
<p>不建议在生产环境中开启自动分区平衡，因为自动平衡的时间无法控制，可能会造成不必要的麻烦。尽量对不平衡率进行监控，然后手动进行分区平衡</p>
</li>
<li><p>分区平衡脚本 <code>kafka-preferred-replica-election.sh</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode2 bin]# ./kafka-preferred-replica-election.sh --zookeeper tnode3:2181</span><br><span class="line">Created preferred replica election path with &#123;"version":1,"partitions":[&#123;"topic":"LxmTest","partition":2&#125;,&#123;"topic":"FLINK-TOPIC-1","partition":0&#125;,&#123;"topic":"__consumer_offsets","partition":34&#125;,&#123;"topic":"LxmTest","partition":5&#125;,&#123;"topic":"SINK-TEST-TOPIC","partition":0&#125;,&#123;"topic":"__consumer_offsets","partition":36&#125;,&#123;"to....</span><br><span class="line"></span><br><span class="line">[root@tnode2 bin]# ./kafka-topics.sh --describe --zookeeper tnode3:2181 --topic topic003</span><br><span class="line">Topic:topic003	PartitionCount:3	ReplicationFactor:3	Configs:</span><br><span class="line">	Topic: topic003	Partition: 0	Leader: 0	Replicas: 0,2,1	Isr: 0,1,2</span><br><span class="line">	Topic: topic003	Partition: 1	Leader: 1	Replicas: 1,0,2	Isr: 1,0,2</span><br><span class="line">	Topic: topic003	Partition: 2	Leader: 2	Replicas: 2,1,0	Isr: 1,0,2</span><br></pre></td></tr></table></figure>

<p>在使用 此脚本时，会将集群中所有的 Topic 的分区进行分区平衡的操作，如果集群规模较大，直接这样使用可能也会影响集群的可用性。所幸此脚本还有参数 <code>--path-to-json-file</code> 来批量进行分区平衡，用法如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode2 bin]# ./kafka-topics.sh --describe --zookeeper tnode3:2181 --topic topic003</span><br><span class="line">Topic:topic003	PartitionCount:3	ReplicationFactor:3	Configs:</span><br><span class="line">	Topic: topic003	Partition: 0	Leader: 0	Replicas: 0,2,1	Isr: 0,1</span><br><span class="line">	Topic: topic003	Partition: 1	Leader: 1	Replicas: 1,0,2	Isr: 1,0</span><br><span class="line">	Topic: topic003	Partition: 2	Leader: 1	Replicas: 2,1,0	Isr: 1,0</span><br><span class="line">[root@tnode2 bin]# ./kafka-preferred-replica-election.sh --zookeeper tnode3:2181 --path-to-json-file election.json </span><br><span class="line">Created preferred replica election path with &#123;"version":1,"partitions":[&#123;"topic":"topic003","partition":0&#125;,&#123;"topic":"topic003","partition":1&#125;,&#123;"topic":"topic003","partition":2&#125;]&#125;</span><br><span class="line">Successfully started preferred replica election for partitions Set([topic003,0], [topic003,1], [topic003,2])</span><br><span class="line">[root@tnode2 bin]# ./kafka-topics.sh --describe --zookeeper tnode3:2181 --topic topic003</span><br><span class="line">Topic:topic003	PartitionCount:3	ReplicationFactor:3	Configs:</span><br><span class="line">	Topic: topic003	Partition: 0	Leader: 0	Replicas: 0,2,1	Isr: 0,1,2</span><br><span class="line">	Topic: topic003	Partition: 1	Leader: 1	Replicas: 1,0,2	Isr: 1,0,2</span><br><span class="line">	Topic: topic003	Partition: 2	Leader: 2	Replicas: 2,1,0	Isr: 1,0,2</span><br></pre></td></tr></table></figure>


</li>
</ul>
<p>在生产中，尽量使用批量分区平衡的方式来进行平衡分区 leader，同时也要避开业务高峰期。</p>
<h3 id="分区重分配"><a href="#分区重分配" class="headerlink" title="分区重分配"></a>分区重分配</h3><p>在 Kafka 中，如果发生 broker 节点宕机、broker 下线、新增 broker 等情况时，会发生部分副本不可用或者新 broker 无副本，这样会导致集群的负载不均衡。为了解决这类问题，就出现了分区重分配的策略，见文档</p>
<h3 id="复制限流"><a href="#复制限流" class="headerlink" title="复制限流"></a>复制限流</h3><p>副本间的复制限流有两种方式：</p>
<ul>
<li><p>kafka-config.sh 脚本来实现</p>
<p><strong>broker</strong> 端有两个参数与复制限流相关：<code>follower.replication.throttled.rate</code> 和 <code>leader.replication.throttled.rate</code> ，一个用于设置 follower 的复制速度，一个用于设置 leader 的传输速度，单位均为 B/s。示例：先设置两个参数值，后再删除两个设置。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode2 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type brokers --entity-name 2 --alter --add-config follower.replication.throttled.rate=1024,leader.replication.throttled.rate=1024</span><br><span class="line">Completed Updating config for entity: brokers '2'.</span><br><span class="line">[root@tnode2 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type brokers --entity-name 2 --describe</span><br><span class="line">Configs for brokers '2' are leader.replication.throttled.rate=1024,follower.replication.throttled.rate=1024</span><br><span class="line">[root@tnode2 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type brokers --entity-name 2 --alter --delete-config follower.replication.throttled.rate,leader.replication.throttled.rate</span><br><span class="line">Completed Updating config for entity: brokers '2'.</span><br><span class="line">[root@tnode2 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type brokers --entity-name 2 --describe</span><br><span class="line">Configs for brokers '2' are</span><br></pre></td></tr></table></figure>

<p>Topic 端也有两个参数与复制限流相关：follower.replication.throttled.replicas 和 leader.replication.throttled.replicas ，用法和上面的类似，<code>--entity-type</code> 为 topic ，<code>--entity-name</code> 为 topicName。</p>
</li>
<li><p>kafka-reassign-partition.sh 脚本实现</p>
</li>
</ul>
<h3 id="修改副本数"><a href="#修改副本数" class="headerlink" title="修改副本数"></a>修改副本数</h3><p>增加副本数：和分区重分配的用法一样，只不过需要执行的分区文件不一样，如下：（原来分区只有2个副本，这里添加第三个副本）。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&#123;"version":1,"partitions":[</span><br><span class="line">&#123;"topic":"LxmTest","partition":2,"replicas":[2,1,0],"log_dirs":["any","any","any"]&#125;,   &#123;"topic":"LxmTest","partition":0,"replicas":[0,2,1],"log_dirs":["any","any","any"]&#125;,   &#123;"topic":"LxmTest","partition":1,"replicas":[1,0,2],"log_dirs":["any","any","any"]&#125;</span><br><span class="line">]&#125;</span><br></pre></td></tr></table></figure>

<p>减少副本数：和增加类似，减少 replicas 列表中副本数和对应的 log_dirs 列表 即可</p>
<h3 id="选择合适的分区数"><a href="#选择合适的分区数" class="headerlink" title="选择合适的分区数"></a>选择合适的分区数</h3><p>同等外部条件下，随着分区数的增加，吞吐量会随着增加，直到一个临界值后，分区数再增加，吞吐量下降…</p>
<p>因此，对于新 Kafka 集群，最好先使用性能测试工具，测出合适的分区数范围。如果不测的话，普遍使用broker 的整数倍 来作为分区数。</p>
<p>分区数也有上限，上限在于系统的打开文件上限数。</p>
]]></content>
      <categories>
        <category>Apache-Kafka</category>
        <category>深入理解Kafka读书笔记</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Kafka</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>日志存储</title>
    <url>/2020/06/16/Apache-Kafka/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Kafka%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/30.%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8/</url>
    <content><![CDATA[<h3 id="日志目录布局"><a href="#日志目录布局" class="headerlink" title="日志目录布局"></a>日志目录布局</h3><p>offset：分区中每条消息都会分配一个唯一的序列号</p>
<p>Log：日志，不考虑副本的情况下，一个分区对应一个Log。N副本的情况下，一个分区有N个相同的Log分布在不同broker上</p>
<p>LogSegment：日志分段，防止Log过大，也是便于消息的维护和清理，Log会被切分为多个相对较小的文件，即LogSegment</p>
<p>实际上，Log是以文件夹 ( <topic> - <partition>  ) 的形式存在，而LogSegment则对应该目录下的一个日志文件和两个索引文件，以及其他可能的文件，但是其文件名均是日志文件中的第一条消息的 offset。</p>
<p>Topic、Partition、Replication、Log、LogSegment之间的关系，如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">                     --&gt; Replica </span><br><span class="line">                                         --&gt; LogSegment</span><br><span class="line">                                                        --&gt; .log 日志文件 </span><br><span class="line">      --&gt; Partition1 --&gt; Replica --&gt; Log --&gt; LogSegment --&gt; .index 索引文件</span><br><span class="line">                                                        --&gt; .timeIndex 时间戳索引文件</span><br><span class="line">                                         --&gt; LogSegment </span><br><span class="line">                                 --&gt; Log</span><br><span class="line">                     --&gt; Replica </span><br><span class="line">Topic --&gt; Partition2</span><br><span class="line">      --&gt; Partition3</span><br></pre></td></tr></table></figure>



<p>此外，kafka broker 第一次启动的时候，默认会在kafka-log的每个根目录下创建几个特殊文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 0.10版本下</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 存了每个log的最后清理offset</span><br><span class="line">cleaner-offset-checkpoint</span><br><span class="line">&#x2F;&#x2F; broker.id 信息</span><br><span class="line">meta.properties</span><br><span class="line">&#x2F;&#x2F; 表示已经刷写到磁盘的记录，point以下的数据都是已经刷写的</span><br><span class="line">recovery-point-offset-checkpoint</span><br><span class="line">&#x2F;&#x2F; 每个replica的HighWatermark(HW)，已经commit的数据，HW以下的数据所有replica同步</span><br><span class="line">replication-offset-checkpoint</span><br></pre></td></tr></table></figure>

<p>值得注意的是，对于 __consumer_offsets 这个topic，初始化是不会创建的，只有第一次有consumer消费message时才会自动创建这个Topic。此Topic默认50个Partition，Replica 为配置中的 Replica 数。</p>
<p>最后，关于Topic的Partition的落盘问题，创建Topic时，如果broker配置了多个日志根目录，那么会挑选分区数最少的那个根目录来完成Partition的创建，即落盘。</p>
<h3 id="日志索引"><a href="#日志索引" class="headerlink" title="日志索引"></a>日志索引</h3><p>上面有提到，每个LogSegment日志文件都有两个对应的索引文件，offset索引文件和时间戳索引文件。offset索引文件用来建立 offset 到物理地址之间的映射，快速定位；时间戳索引文件根据指定的时间戳查找对应offset信息。</p>
<p>两类索引均为稀疏索引 ( sparse index )，不能保证每条message在索引文件中都有对应的索引信息，只有当写入一定量 ( broker 端参数 <code>log.index.interval.bytes</code> 指定，默认值为4096，即4KB ) 消息时 ，索引文件会对应增加offset索引项和时间戳索引项，同时增大或减少这个参数的值，对应可以改变索引项的密度。</p>
<p>LogSegment 达到一定大小时会进行切分，切分条件如下：</p>
<ul>
<li>当前 LogSegment 大小超过了 broker 端参数 <code>log.segment.bytes</code>  配置的值时；默认值为 1073741824，即1GB；</li>
<li>当前 LogSegment 中消息的最大时间戳与系统当前时间戳差值超过 <code>log.roll.ms</code> 或者 <code>log.roll.hours</code> 时；<code>log.roll.ms</code> 优先级更高。默认只配置了 <code>log.rool.hours</code> 值为 168 即 7 天；</li>
<li>offset索引文件或者时间戳索引文件大小达到 broker 端参数 <code>log.index.size.max.bytes</code> 的值时，<code>log.index.size.max.bytes</code> 默认值为 10485760，即10MB；</li>
<li>新 append 的 message 的 offset 与当前 LogSegment 的最大 offset 差值大于 Integer.MAX_VALUE 时，即 append 的 message 的 offset 不能转变为相对偏移量（offset - baseOffset &gt; Integer.MAX_VALUE）;</li>
</ul>
<p>对于索引文件大小问题，非当前活跃的Logsegment而言，因为不会再写入新 message 了，所以其对应的索引文件会被设置为只读，其大小也就是实际占用的空间。对于当前活跃的 LogSegment 而言，在 LogSegment 切分时会以可读写的模式创建新的索引文件，同时预分配 <code>log.index.size.max.bytes</code> 大小的空间，直到索引文件进行切分时才会把该索引文件裁剪到实际数据大小。</p>
<h5 id="偏移量-offset-索引"><a href="#偏移量-offset-索引" class="headerlink" title="偏移量 (offset) 索引"></a>偏移量 (offset) 索引</h5><p>offset索引文件中每行索引项格式都是 relativeOffset + position  <code>相对偏移量</code>+<code>物理地址</code> （每个占4B）</p>
<p>根据 offset 查找日志消息（分区内）：</p>
<ol>
<li><p>根据 offset 定位到 Logsegment ：每个 LogSegment 的baseOffset都会作为 key ，保存在对应的跳跃表（<code>ConcurrentSkipListMap</code>）里。根据 offset 就可以在跳跃表里通过二分法定位基础的 baseOffset对应的 LogSegment </p>
</li>
<li><p>计算相对偏移量：相对偏移量 = offset - baseOffset。</p>
</li>
<li><p>在索引文件中查找物理地址：根基相对偏移量，在索引文件中找到不大于相对偏移量的最大索引项，根据其对应的物理地址 position 开始顺序查找目标消息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">offset --&gt; LogSegment、baseOffset --&gt; index --&gt; offset - baseOffset &#x3D; relativeOffset  --&gt; position --&gt; LogSegment顺序查找目标消息</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>另外，kafka要求索引文件大小必须是索引项大小的整数倍。对偏移量索引文件来说，其索引项大小为8，则必须为8的整数倍，如果不是，则为最接近配置的8的整数倍的最大值。例如配置为 75，则会置为 72 。时间戳索引同理，不过时间戳索引项大小是12B。</p>
<h5 id="时间戳索引"><a href="#时间戳索引" class="headerlink" title="时间戳索引"></a>时间戳索引</h5><p>和偏移量索引一样，时间戳索引中每行索引项是 timestamp+relativeOffset 当前LogSegment中的最大时间戳+对应消息的物理地址（时间戳占8B，物理地址占4B）</p>
<p>对应每个新append的索引项来说，要求其 timestamp 必须大于之前append的索引项的 timestamp，否则不会 append。决定索引文件中 timestamp 是否单调递增的是broker端参数 <code>log.message.timestamp.type</code> ，设置为 LogAppendTime 时，能够保证；反之，如果是CreateTime类型则无法保证。</p>
<p><em>当为CreateTIme时，虽然Producer 可以指定 message 的 timestamp ，但是如果多机发往同一个分区，并且每个机器的时间不一样，那么 timestamp 铁定会混乱。</em></p>
<p>根据时间戳targetTimeStamp查找日志消息 (分区内)：</p>
<ol>
<li><p>targetTimeStamp与每个 LogSegment 中的 largestTimeStamp对比，直到找到不小于targetTimeStamp的largestTimeStamp所对应的LogSegment。</p>
<p>LogSegment 的 largestTimeStamp计算：查询对应的时间戳索引文件，找到最后一条索引项，如果其timestamp &gt; 0 ，则取值；否则取 LogSegment 的最近修改时间</p>
</li>
<li><p>在对应的时间戳索引文件中通过二分法找到不大于 targetTimeStamp 的最大索引项，取其相对偏移量</p>
</li>
<li><p>在偏移量索引文件中二分法查找不大于 该相对偏移量 的最大索引项，取到对应的物理地址 position</p>
</li>
<li><p>从LogSegment的position位置开始顺序查找不小于 targetTimeStamp 的 message。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>Apache-Kafka</category>
        <category>深入理解Kafka读书笔记</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Kafka</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>日志清理</title>
    <url>/2020/06/17/Apache-Kafka/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Kafka%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/31.%E6%97%A5%E5%BF%97%E6%B8%85%E7%90%86/</url>
    <content><![CDATA[<h3 id="日志清理"><a href="#日志清理" class="headerlink" title="日志清理"></a>日志清理</h3><p>kafka message 存在磁盘中，为了控制磁盘占用空间的增加就需要对落盘的 message 做一定的清理操作。每个 Partition 对应的 Log 都有多个 LogSegment ，也是方便日志清理。目前有 delete 和 compact 两种策略：</p>
<ul>
<li>Log Retention：按照一定的保留策略直接删除不符合条件的 LogSegment；</li>
<li>Log Compaction：针对每天消息的 Key 进行整合，具有相同 Key的不同 Value 值，只保留最后版本</li>
</ul>
<p>日志清理策略通过 broker 端参数 <code>log.cleanup.policy</code> 来设置，默认值是 <code>delete</code> 。对应 <code>compact</code> 策略，还需要将 <code>log.cleaner.enable</code> 设为 true，默认为 true。同时，<code>log.cleanup.policy</code> 还可以设置为 <code>delete,compact</code> 来同时支持两种策略。</p>
<p>注意，日志清理策略也可以控制到主题级别，对应的参数是 <code>cleanup.policy</code> 。</p>
<h5 id="日志删除"><a href="#日志删除" class="headerlink" title="日志删除"></a>日志删除</h5><p>LogManager 中会有一个专门的日志删除任务来周期性的检测和删除不符合保留条件的 LogSegment 。周期通过 <code>log.retention.check.interval.ms</code> 来进行控制，默认 30000 ，即 5 分钟。当前的日志保留策略也有三种：基于时间的、基于Log大小的、基于日志起始 offset 的。</p>
<h6 id="1、基于时间"><a href="#1、基于时间" class="headerlink" title="1、基于时间"></a>1、基于时间</h6><h6 id="2、基于大小"><a href="#2、基于大小" class="headerlink" title="2、基于大小"></a>2、基于大小</h6><h6 id="3、基于日志起始offset"><a href="#3、基于日志起始offset" class="headerlink" title="3、基于日志起始offset"></a>3、基于日志起始offset</h6>]]></content>
      <categories>
        <category>Apache-Kafka</category>
        <category>深入理解Kafka读书笔记</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Kafka</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>副本攻略</title>
    <url>/2020/06/18/Apache-Kafka/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Kafka%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/80.%E5%89%AF%E6%9C%AC%E6%94%BB%E7%95%A5/</url>
    <content><![CDATA[<h2 id="副本（Replica）攻略"><a href="#副本（Replica）攻略" class="headerlink" title="副本（Replica）攻略"></a>副本（Replica）攻略</h2><p>在分布式系统中，副本是最常见的概念之一，一般指的是对数据或服务的冗余方式。数据副本是在不同的节点上持久化存储同一份数据，防止数据丢失；服务副本指的是多个节点提供同样的服务，单节点挂掉不影响正常服务。</p>
<p>Kafka 中的副本相关：</p>
<ul>
<li>副本是特定分区 Partition 的副本</li>
<li>Kafka 中每个 Partition 都有一个或多个副本，至少一个为 leader 副本，其他为 follower 副本，各个副本都应该在不同的broker中 即副本数不能大于 broker 数。</li>
<li>AR：分区的所有副本； ISR：跟上 leader 的副本，包含 leader 副本。</li>
<li>LEO 即 Log EndOffset，每个分区中最好一条消息的下一个 offset。每个分区都有自己的 LEO，而 ISR 中最小的 LEO 即为 HW，俗称高水位，consumer 只能消费 HW 之前的消息。</li>
</ul>
<h4 id="副本失效"><a href="#副本失效" class="headerlink" title="副本失效"></a>副本失效</h4><p>副本失效，即在AR中的副本，未跟上 leader 同步的副本，被移出了 ISR 的副本，其对应的分区就叫同步失效分区。通过命令查看：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-topics.sh --zookeeper tnode3:2181 --topic LxmTest  --describe</span><br><span class="line">Topic:LxmTest	PartitionCount:6	ReplicationFactor:2	Configs:retention.ms=259200000,flush.messages=6</span><br><span class="line">	Topic: LxmTest	Partition: 0	Leader: 0	Replicas: 1,0	Isr: 0,1</span><br><span class="line">	Topic: LxmTest	Partition: 1	Leader: 1	Replicas: 0,1	Isr: 1,0</span><br><span class="line">	Topic: LxmTest	Partition: 2	Leader: 1	Replicas: 1,0	Isr: 1,0</span><br><span class="line">	Topic: LxmTest	Partition: 3	Leader: 1	Replicas: 1,2	Isr: 1</span><br><span class="line">	Topic: LxmTest	Partition: 4	Leader: 1	Replicas: 2,1	Isr: 1</span><br><span class="line">	Topic: LxmTest	Partition: 5	Leader: 0	Replicas: 0,2	Isr: 0</span><br><span class="line">[root@tnode1 bin]# ./kafka-topics.sh --zookeeper tnode3:2181 --topic LxmTest  --describe --under-replicated-partitions</span><br><span class="line">	Topic: LxmTest	Partition: 3	Leader: 1	Replicas: 1,2	Isr: 1</span><br><span class="line">	Topic: LxmTest	Partition: 4	Leader: 1	Replicas: 2,1	Isr: 1</span><br><span class="line">	Topic: LxmTest	Partition: 5	Leader: 0	Replicas: 0,2	Isr: 0</span><br></pre></td></tr></table></figure>

<p>如果没有失效分区，第二个命令返回应该是空。</p>
<p>Kafka 通过参数 <code>replica.lag.time.max.ms</code> 来判断分区副本是否失效。当 ISR 中 follower 副本滞后 leader 副本的时间超过此参数的值时则判断为 同步失败，需要将此副本移出 ISR 列表（<code>replica.lag.time.max.ms</code> 默认值为 10000 即 100秒 ）。具体判断滞后的原理也很简单：</p>
<ol>
<li>follower 副本将 leader 副本 LEO 之前的日志全部同步时，表示已经追上 leader 副本</li>
<li>更新 lastCaughtUpTime 标识</li>
<li>定时任务，定时检测 lastCaughtUpTime 与当前时间的差值是否大于 <code>replica.lag.time.max.ms</code> ，如果大于则判断失效。</li>
</ol>
<p>这种滞后的情况会有两种情况：</p>
<ul>
<li>follower 副本一段时间内未发起同步请求</li>
<li>follower 副本同步速度过慢，一段时间内无法追上 leader 副本</li>
</ul>
<p><strong>！## ！在早期版本，还有一个根据 follower 副本和 leader 副本同步消息条数的差值来判断同步是否失效，已被放弃</strong></p>
<h4 id="ISR-列表变动"><a href="#ISR-列表变动" class="headerlink" title="ISR 列表变动"></a>ISR 列表变动</h4><p>ISR 列表变动与两个定时任务相关 isr-expiraion 和 isr-change-propagation 。isr-expiration 任务会周期性的检查每个分区的 ISR 列表是否需要缩减，周期为上面 <code>replica.lag.time.max.ms</code> 参数的一半，默认是 5000, 即 5 秒。当检测到 ISR 中有失效副本时，就会缩减 ISR 列表，并将变更后的数据记录到 Zookeeper 对应节点中，同时将记录缓存到 isrChangeSet 中。而 isr-change-propagation 任务会周期性 (固定2500ms) 检查 is人ChangeSet，如果有变更记录，那么将在 Zookeeper 对应目录下创建持久的顺序节点，并添加 Watcher。如果 isr-change-propagation 对应目录下有节点发生变化，将触发 Watcher 通知 Kafka 控制器，更新元数据。</p>
<p>但是如果频繁的触发 Watcher，将影响 ZK 和 Kafka 性能，因此，在检测到分区 ISR 列表发生变化时，还需要：</p>
<ul>
<li>上一次 ISR 列表 发生变化时间距离现在超过 5 秒</li>
<li>上一次写入 ZK 的时间距离现在超过 60 秒</li>
</ul>
<p>以上满足其一，才会将 ISR 列表的变化写入目标节点。</p>
<p>ISR 扩充的情况和 ISR 缩减的原理类似，只不过是，定时检测 是否有 follower 副本有资格进入 ISR 列表。这个资格为：follower 副本的 LEO 不小于 leader 副本的 HW（注意是 HW，而不是 LEO）。</p>
<p><strong>！##  ！无论 ISR 增加，还是任一副本的LEO发生变化时，都会影响整个分区的 HW ！##  !</strong></p>
<h4 id="LEO-和-HW"><a href="#LEO-和-HW" class="headerlink" title="LEO 和 HW"></a>LEO 和 HW</h4><p>在 Kafka 中，同一个分区的信息会存在多个 broker 节点上，并被其上的副本管理器所管理。先看一下消息写入的过程：</p>
<ol>
<li><p>生产者向 leader 副本发送消息</p>
</li>
<li><p>leader 副本存储消息，并更新 LEO，HW  </p>
<p>（假设 msg=10，LEO=10，HW=10 ）</p>
</li>
<li><p>follower 副本们 向 leader 副本请求同步数据        </p>
<p>（同时会附带自己的 LEO信息  LEO.1 = LEO.2 =0）   </p>
</li>
<li><p>leader 副本读取本地日志，并更新 leader 副本上 关于 follower 副本的信息</p>
<p>（LEO=10， HW = min(10, 0, 0) = 0 ）</p>
</li>
<li><p>leader 副本将读取的结果返回给 follower 副本们，并将 HW 传给 follower 们</p>
</li>
<li><p>follower 副本们接收 leader的返回结果，追加消息到日志，更新 LEO，HW</p>
<p>（LEO.1 =  4，HW.1 = min(LEO.1, HW) = 0，LEO.2 = 5，HW.2 = min(LEO.2, HW) = 0）</p>
</li>
<li><p>leader同一时间也会接收新消息</p>
<p>（假设 msg=5，LEO.0=15，HW = 0）</p>
</li>
<li><p>follower 们再次请求拉取同步数据</p>
<p>（同时带上自己的 LEO信息，LEO.1 =4 ，LEO.2=5）</p>
</li>
<li><p>leader 拉取本地数据，并获取 follower 的请求信息 ( LEO ) 等，将请求结果返回 follower们，带上 HW</p>
<p>（LEO.0=15，HW = min(15, 4, 5) = 4）</p>
</li>
<li><p>follower 们获取返回结果，写日志，更新 LEO 和 HW</p>
<p>（LEO.1 = 8，HW.1 = min(8, 4) =4，LEO.2 = 10，HW.2 = min(10, 4) = 4）</p>
</li>
<li><p>接着 下一批，follower 再次请求数据</p>
<p>（LEO.1 = 8，LEO.2 = 10）</p>
</li>
<li><p>leader 获取本地数据，返回结果给 follower，带上 HW</p>
<p>（LEO.0=20， HW = min(20, 8, 10) = 8）</p>
</li>
<li><p>follower 们接收消息，写日志，更新 LEO 和 HW</p>
<p>（LEO.1 = 18，HW.1 = min(18, 8) = 8，LEO.2 = 10，HW.2 = min(10, 8) = 8）</p>
</li>
<li><p>….</p>
</li>
</ol>
<p><em>这样子算下来，可能会发现，leader 的 HW 并不等于 三个最新 LEO 的最小值。因为 leader 上的 HW 是要用来提供服务的，因此只能计算返回结果到  follower 之前的 LEO 的最小值。如果计算了返回结果之后的 LEO，那么当前 的 HW不能得到可靠保证，因为返回结果到 follower 可能失败，失败的话，如果 leader 重置，而新 leader 的LEO 达不到 旧 leader 的 HW，那么这中间的数据就丢了。</em></p>
]]></content>
      <categories>
        <category>Apache-Kafka</category>
        <category>深入理解Kafka读书笔记</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Kafka</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>源码环境搭建</title>
    <url>/2020/07/11/Apache-Kafka/Kafka%E6%BA%90%E7%A0%81%E5%85%A5%E9%97%A8/%E6%BA%90%E7%A0%81%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</url>
    <content><![CDATA[<h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><ul>
<li>JDK 1.8</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">C:\Users\Administrator&gt;java -version</span><br><span class="line">java version &quot;1.8.0_201&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_201-b09)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.201-b09, mixed mode)</span><br></pre></td></tr></table></figure>

<ul>
<li><p>Gradle 5.6</p>
<p>解压下载的安装到，移动到目标目录。添加对应 bin 目录到环境变量 path。</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">C:\Users\Administrator&gt;gradle -v</span><br><span class="line"></span><br><span class="line">------------------------------------------------------------</span><br><span class="line">Gradle 5.6.2</span><br><span class="line">------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">Build time:   2019-09-05 16:13:54 UTC</span><br><span class="line">Revision:     55a5e53d855db8fc7b0e494412fc624051a8e781</span><br><span class="line"></span><br><span class="line">Kotlin:       1.3.41</span><br><span class="line">Groovy:       2.5.4</span><br><span class="line">Ant:          Apache Ant(TM) version 1.9.14 compiled on March 12 2019</span><br><span class="line">JVM:          1.8.0_201 (Oracle Corporation 25.201-b09)</span><br><span class="line">OS:           Windows 10 10.0 amd64</span><br></pre></td></tr></table></figure>

<ul>
<li>Scala</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">C:\Users\Administrator&gt;scala -version</span><br><span class="line">Scala code runner version 2.11.12 -- Copyright 2002-2017, LAMP/EPFL</span><br></pre></td></tr></table></figure>



<h3 id="源码下载"><a href="#源码下载" class="headerlink" title="源码下载"></a>源码下载</h3><p>git clone <a href="https://gitee.com/romandata/Kafka.git" target="_blank" rel="noopener">https://gitee.com/romandata/Kafka.git</a></p>
<p>切换到对应分支</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git checkout origin/2.3</span></span><br><span class="line">Checking out files: 100% (3374/3374), done.</span><br><span class="line">Note: checking out 'origin/2.3'.</span><br><span class="line"></span><br><span class="line">You are in 'detached HEAD' state. You can look around, make experimental</span><br><span class="line">changes and commit them, and you can discard any commits you make in this</span><br><span class="line">state without impacting any branches by performing another checkout.</span><br><span class="line"></span><br><span class="line">If you want to create a new branch to retain commits you create, you may</span><br><span class="line">do so (now or later) by using -b with the checkout command again. Example:</span><br><span class="line"></span><br><span class="line">  git checkout -b &lt;new-branch-name&gt;</span><br><span class="line"></span><br><span class="line">HEAD is now at 3e3419a... Add recent versions of Kafka to the matrix of ConnectD         istributedTest (#7024)</span><br></pre></td></tr></table></figure>

<p>修改源码目录下的 gradle.properties 文件，修改scala版本为本机环境的scala版本。</p>
<p>源码目录下执行  gradle idea 或者直接用IDEA打开项目，指定 gradle home。</p>
<p>编译慢的原因是 gradle 镜像在国外，修改为使用阿里云镜像：</p>
<ol>
<li><p>gradle home下的init.d目录下加入一个名叫 init.gradle 的文件</p>
</li>
<li><p>添加以下配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">allprojects&#123;</span><br><span class="line">    repositories &#123;</span><br><span class="line">        def ALIYUN_REPOSITORY_URL = 'http://maven.aliyun.com/nexus/content/groups/public'</span><br><span class="line">        def ALIYUN_JCENTER_URL = 'http://maven.aliyun.com/nexus/content/repositories/jcenter'</span><br><span class="line">        all &#123; ArtifactRepository repo -&gt;</span><br><span class="line">            if(repo instanceof MavenArtifactRepository)&#123;</span><br><span class="line">                def url = repo.url.toString()</span><br><span class="line">                if (url.startsWith('https://repo1.maven.org/maven2')) &#123;</span><br><span class="line">                    project.logger.lifecycle "Repository $&#123;repo.url&#125; replaced by $ALIYUN_REPOSITORY_URL."</span><br><span class="line">                    remove repo</span><br><span class="line">                &#125;</span><br><span class="line">                if (url.startsWith('https://jcenter.bintray.com/')) &#123;</span><br><span class="line">                    project.logger.lifecycle "Repository $&#123;repo.url&#125; replaced by $ALIYUN_JCENTER_URL."</span><br><span class="line">                    remove repo</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        maven &#123;</span><br><span class="line">                url ALIYUN_REPOSITORY_URL</span><br><span class="line">            url ALIYUN_JCENTER_URL</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>




</li>
</ol>
<p>如果出现以下错误</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kafka-1.1.1-src gradle idea</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> Configure project :</span></span><br><span class="line">Building project 'core' with Scala version 2.11.12</span><br><span class="line"></span><br><span class="line">FAILURE: Build failed with an exception.</span><br><span class="line"></span><br><span class="line">* Where:</span><br><span class="line">Build file '/Users/bibo/.Trash/kafka-1.1.1-src/build.gradle' line: 552</span><br><span class="line"></span><br><span class="line">* What went wrong:</span><br><span class="line">A problem occurred evaluating root project 'kafka-1.1.1-src'.</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> Failed to apply plugin [id <span class="string">'org.scoverage'</span>]</span></span><br><span class="line"><span class="meta">   &gt;</span><span class="bash"> Could not create an instance of <span class="built_in">type</span> org.scoverage.ScoverageExtension.</span></span><br><span class="line">      &gt; You can't map a property that does not exist: propertyName=testClassesDir</span><br><span class="line"></span><br><span class="line">* Try:</span><br><span class="line">Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.</span><br><span class="line"></span><br><span class="line">* Get more help at https://help.gradle.org</span><br><span class="line"></span><br><span class="line">Deprecated Gradle features were used in this build, making it incompatible with Gradle 6.0.</span><br><span class="line">Use '--warning-mode all' to show the individual deprecation warnings.</span><br><span class="line">See https://docs.gradle.org/5.4.1/userguide/command_line_interface.html#sec:command_line_warnings</span><br><span class="line"></span><br><span class="line">BUILD FAILED in 2s</span><br></pre></td></tr></table></figure>

<p>解决方法参考：KAFKA-7706</p>
<p>修改 build.gradle 文件,将org.scoverage:gradle-scoverage 版本修改,2.1.0修改为2.5.0,重新执行</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">buildscript &#123;</span><br><span class="line">  repositories &#123;</span><br><span class="line">    mavenCentral()</span><br><span class="line">    jcenter()</span><br><span class="line">  &#125;</span><br><span class="line">  apply from: file(<span class="string">'gradle/buildscript.gradle'</span>), to: buildscript</span><br><span class="line"></span><br><span class="line">  dependencies &#123;</span><br><span class="line">    <span class="comment">// For Apache Rat plugin to ignore non-Git files</span></span><br><span class="line">    classpath <span class="string">"org.ajoberstar:grgit:1.7.0"</span></span><br><span class="line">    classpath <span class="string">'com.github.ben-manes:gradle-versions-plugin:0.13.0'</span></span><br><span class="line">    classpath <span class="string">'org.scoverage:gradle-scoverage:2.5.0'</span>   <span class="comment">// 之前是2.1.0</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>IDEA 打开项目目录</p>
]]></content>
      <categories>
        <category>Apache-Kafka</category>
        <category>源码环境搭建</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Kafka</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title>kafka-consumer-offset-monitor</title>
    <url>/2020/08/31/Apache-Kafka/Kafka%E7%9B%91%E6%8E%A7/kafka-consumer-offset-monitor/</url>
    <content><![CDATA[<h5 id="此为监控kafka-consumer-offset的代码，仅供记录。"><a href="#此为监控kafka-consumer-offset的代码，仅供记录。" class="headerlink" title="此为监控kafka consumer offset的代码，仅供记录。"></a>此为监控kafka consumer offset的代码，仅供记录。</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> kafka.common.OffsetAndMetadata;</span><br><span class="line"><span class="keyword">import</span> kafka.coordinator.BaseKey;</span><br><span class="line"><span class="keyword">import</span> kafka.coordinator.GroupMetadataManager;</span><br><span class="line"><span class="keyword">import</span> kafka.coordinator.GroupTopicPartition;</span><br><span class="line"><span class="keyword">import</span> kafka.coordinator.OffsetKey;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.TopicPartition;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.ByteArrayDeserializer;</span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.Logger;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.nio.ByteBuffer;</span><br><span class="line"><span class="keyword">import</span> java.sql.Connection;</span><br><span class="line"><span class="keyword">import</span> java.sql.PreparedStatement;</span><br><span class="line"><span class="keyword">import</span> java.sql.SQLException;</span><br><span class="line"><span class="keyword">import</span> java.time.LocalDateTime;</span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 保消费数据线程</span></span><br><span class="line"><span class="comment"> * 通过 eebbk-monitor-group 消费 __consumer_offsets 数据，获取kafka中的消费数据。确定所有消费组、Topic、Partition当前的 consumer offset</span></span><br><span class="line"><span class="comment"> * 通过 consumer 获取当前TopicPartition的最大最小offset</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * created by lxm at 2020-08-31</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaOffset2Mysql</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOG = Logger.getLogger(KafkaOffset2Mysql<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String     bootstrapServers;</span><br><span class="line">    <span class="keyword">private</span> Connection connection;</span><br><span class="line">    <span class="keyword">private</span> String     clusterId;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">KafkaOffset2Mysql</span><span class="params">(String bootstrapServers, Connection connection)</span></span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.bootstrapServers = bootstrapServers;</span><br><span class="line">        <span class="keyword">this</span>.connection       = connection;</span><br><span class="line">        String s = bootstrapServers.split(<span class="string">":"</span>)[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">int</span> length = s.length();</span><br><span class="line">        <span class="keyword">this</span>.clusterId        = s.substring(<span class="number">0</span>,length-<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>&#123;</span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);</span><br><span class="line">        props.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">"eebbk-monitor-group"</span>);</span><br><span class="line">        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="keyword">false</span>);</span><br><span class="line">        KafkaConsumer&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt; kafkaConsumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        kafkaConsumer.subscribe(Collections.singleton(<span class="string">"__consumer_offsets"</span>));</span><br><span class="line"></span><br><span class="line">        Set&lt;TopicPartition&gt; topicPartitionSet = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line">        <span class="keyword">long</span> beginTimeMillis = System.currentTimeMillis();</span><br><span class="line">        HashMap&lt;String, MonitorOffsetResult&gt; resultHashMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        Map&lt;TopicPartition, Long&gt; topicPartitionLEOMap = <span class="keyword">null</span>;</span><br><span class="line">        Map&lt;TopicPartition, Long&gt; topicPartitionLSOMap = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>)&#123;</span><br><span class="line">            ConsumerRecords&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt; consumerRecords = kafkaConsumer.poll(<span class="number">100</span>);</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt; record:consumerRecords)&#123;</span><br><span class="line">                BaseKey key = GroupMetadataManager.readMessageKey(ByteBuffer.wrap(record.key()));</span><br><span class="line">                <span class="keyword">if</span> (record.value()==<span class="keyword">null</span>)&#123;</span><br><span class="line">                    System.out.println(key);</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span> (key <span class="keyword">instanceof</span> OffsetKey)&#123;</span><br><span class="line">                    GroupTopicPartition groupTopicPartition = (GroupTopicPartition) key.key();</span><br><span class="line">                    OffsetAndMetadata offsetAndMetadata = GroupMetadataManager.readOffsetMessageValue(ByteBuffer.wrap(record.value()));</span><br><span class="line">                    String groupTopicPartitionStr = groupTopicPartition.group() + <span class="string">"__"</span> + groupTopicPartition.topicPartition().topic() + <span class="string">"__"</span> + groupTopicPartition.topicPartition().partition();</span><br><span class="line">                    <span class="keyword">if</span> (!resultHashMap.containsKey(groupTopicPartitionStr))&#123;</span><br><span class="line">                        MonitorOffsetResult monitorOffsetResult = <span class="keyword">new</span> MonitorOffsetResult();</span><br><span class="line">                        monitorOffsetResult.setGroup(groupTopicPartition.group());</span><br><span class="line">                        monitorOffsetResult.setTopicPartition(groupTopicPartition.topicPartition());</span><br><span class="line">                        monitorOffsetResult.setPreviousCurrentOffset(offsetAndMetadata.offset());</span><br><span class="line">                        monitorOffsetResult.setPreviousLogEndOffset(<span class="number">0L</span>);</span><br><span class="line">                        topicPartitionSet.add(groupTopicPartition.topicPartition());</span><br><span class="line">                        resultHashMap.put(groupTopicPartitionStr, monitorOffsetResult);</span><br><span class="line">                    &#125;</span><br><span class="line">                    resultHashMap.get(groupTopicPartitionStr).setCurrentOffset(offsetAndMetadata.offset());</span><br><span class="line">                &#125;</span><br><span class="line"><span class="comment">//                else if (key instanceof GroupMetadataKey)&#123;</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//                    GroupMetadata groupMetadata = GroupMetadataManager.readGroupMessageValue(((GroupMetadataKey)key).key(), ByteBuffer.wrap(record.value()));</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//                    System.out.println("key:"+key+" value:"+groupMetadata.toString());</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//                &#125;</span></span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (System.currentTimeMillis()-beginTimeMillis&gt;=<span class="number">1000</span>*<span class="number">60</span>)&#123;</span><br><span class="line"></span><br><span class="line">                kafkaConsumer.commitAsync();</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 取整分</span></span><br><span class="line">                String dateTime = LocalDateTime.now().toString().replace(<span class="string">"T"</span>, <span class="string">" "</span>).substring(<span class="number">0</span>,<span class="number">16</span>)+<span class="string">":00"</span>;</span><br><span class="line">                LOG.info(<span class="string">"提交新的offset信息到MySQL："</span>+dateTime);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    topicPartitionLEOMap = kafkaConsumer.endOffsets(topicPartitionSet);</span><br><span class="line">                    topicPartitionLSOMap = kafkaConsumer.beginningOffsets(topicPartitionSet);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (topicPartitionLEOMap==<span class="keyword">null</span> || topicPartitionLSOMap==<span class="keyword">null</span>)&#123;</span><br><span class="line">                        LOG.error(<span class="string">"获取 LEO 或 LSO 异常！"</span>+e);</span><br><span class="line">                        <span class="keyword">continue</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                    LOG.warn(<span class="string">"获取 LEO 或 LSO 异常！使用前次记录！"</span>);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> (MonitorOffsetResult monitorOffsetResult: resultHashMap.values())&#123;</span><br><span class="line"></span><br><span class="line">                    TopicPartition topicPartition = monitorOffsetResult.getTopicPartition();</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> (!monitorOffsetResult.isCanInsert() &amp;&amp; monitorOffsetResult.getCurrentOffset() == <span class="number">0L</span>)&#123;</span><br><span class="line">                        monitorOffsetResult.setCanInsert(<span class="keyword">true</span>);</span><br><span class="line">                        <span class="keyword">continue</span>;</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//                    if (monitorOffsetResult.getCurrentOffset() == 0L)&#123;</span></span><br><span class="line"><span class="comment">//                        monitorOffsetResult.setPreviousCurrentOffset(monitorOffsetResult.getCurrentOffset());</span></span><br><span class="line"><span class="comment">//                    &#125;</span></span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> (monitorOffsetResult.getPreviousCurrentOffset() == <span class="number">0L</span>)&#123;</span><br><span class="line">                        monitorOffsetResult.setPreviousCurrentOffset(monitorOffsetResult.getCurrentOffset());</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">if</span> (monitorOffsetResult.getPreviousLogEndOffset()==<span class="number">0L</span>)&#123;</span><br><span class="line">                        monitorOffsetResult.setPreviousLogEndOffset(topicPartitionLEOMap.get(topicPartition));</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">if</span> (topicPartitionLEOMap.get(topicPartition) != <span class="keyword">null</span> )&#123;</span><br><span class="line">                        monitorOffsetResult.setLogEndOffset(topicPartitionLEOMap.get(topicPartition));</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">if</span> (topicPartitionLSOMap.get(topicPartition) != <span class="keyword">null</span> )&#123;</span><br><span class="line">                        monitorOffsetResult.setLogStartOffset(topicPartitionLSOMap.get(topicPartition));</span><br><span class="line">                    &#125;</span><br><span class="line">                    String sql = <span class="string">"insert into kafka_monitor_offsets values (?,?,?,?,?,?,?,?,?,?,?,?)"</span>;</span><br><span class="line">                    <span class="keyword">try</span>(PreparedStatement ps = connection.prepareStatement(sql)) &#123;</span><br><span class="line">                        ps.setString(<span class="number">1</span>, clusterId);</span><br><span class="line">                        ps.setString(<span class="number">2</span>, dateTime);</span><br><span class="line">                        ps.setString(<span class="number">3</span>, monitorOffsetResult.getGroup());</span><br><span class="line">                        ps.setString(<span class="number">4</span>, monitorOffsetResult.getTopicPartition().topic());</span><br><span class="line">                        ps.setInt(   <span class="number">5</span>, monitorOffsetResult.getTopicPartition().partition());</span><br><span class="line">                        ps.setLong(  <span class="number">6</span>, monitorOffsetResult.getLogStartOffset());</span><br><span class="line">                        ps.setLong(  <span class="number">7</span>, monitorOffsetResult.getLogEndOffset());</span><br><span class="line">                        ps.setLong(  <span class="number">8</span>, monitorOffsetResult.getCurrentOffset());</span><br><span class="line">                        ps.setLong(  <span class="number">9</span>, monitorOffsetResult.getLogEndOffset() - monitorOffsetResult.getCurrentOffset());</span><br><span class="line">                        ps.setLong(  <span class="number">10</span>, monitorOffsetResult.getCurrentOffset() - monitorOffsetResult.getLogStartOffset());</span><br><span class="line">                        ps.setLong(  <span class="number">11</span>,monitorOffsetResult.getPreviousCurrentOffset());</span><br><span class="line">                        ps.setLong(  <span class="number">12</span>,monitorOffsetResult.getPreviousLogEndOffset());</span><br><span class="line">                        ps.execute();</span><br><span class="line"></span><br><span class="line">                    &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">                        LOG.error(<span class="string">"执行SQL出错：\n"</span>+e);</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// reset Previous value from Current</span></span><br><span class="line">                    monitorOffsetResult.setPreviousCurrentOffset();</span><br><span class="line">                    monitorOffsetResult.setPreviousLogEndOffset(topicPartitionLEOMap.get(topicPartition));</span><br><span class="line">                &#125;</span><br><span class="line">                beginTimeMillis = System.currentTimeMillis();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Apache-Kafka</category>
        <category>Kafka监控</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Kafka</tag>
        <tag>monitor</tag>
      </tags>
  </entry>
  <entry>
    <title>kafka-topic.sh</title>
    <url>/2020/07/15/Apache-Kafka/Kafka%E6%BA%90%E7%A0%81%E5%85%A5%E9%97%A8/01.%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%91%BD%E4%BB%A4/00.kafka-topic.sh/</url>
    <content><![CDATA[<h3 id="kafka-topic-sh"><a href="#kafka-topic-sh" class="headerlink" title="kafka-topic.sh"></a>kafka-topic.sh</h3><p>先看具体脚本信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 中间注释已删除</span></span><br><span class="line">exec $(dirname $0)/kafka-run-class.sh kafka.admin.TopicCommand "$@"</span><br></pre></td></tr></table></figure>

<p>脚本命令很简单，通过 <code>kafka-run-class.sh</code> 启动 <code>kafka.admin.TopicCommand</code> 类，参数全部传递过去。对于 <code>kafka-run-class.sh</code> ，因为对shell命令不熟，这里不作过多介绍。不过其功能基本上都是初始化启动参数以及环境，然后启动参数中相应的类。另外，-daemon 表示后台运行，日志为日志目录下相应日志文件。</p>
<p>接下来只要看 <code>kafka.admin.TopicCommand</code> 类的运行过程。</p>
<h5 id="main函数"><a href="#main函数" class="headerlink" title="main函数"></a>main函数</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 解析输入参数</span></span><br><span class="line">    <span class="keyword">val</span> opts = <span class="keyword">new</span> <span class="type">TopicCommandOptions</span>(args)</span><br><span class="line">	</span><br><span class="line">    <span class="comment">// 无参数直接退出，为啥这个不在初始化之前...</span></span><br><span class="line">    <span class="keyword">if</span>(args.length == <span class="number">0</span>)</span><br><span class="line">      <span class="type">CommandLineUtils</span>.printUsageAndDie(opts.parser, <span class="string">"Create, delete, describe, or change a topic."</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//  should have exactly one action</span></span><br><span class="line">    <span class="keyword">val</span> actions = <span class="type">Seq</span>(opts.createOpt, opts.listOpt, opts.alterOpt, opts.describeOpt, opts.deleteOpt).count(opts.options.has _)</span><br><span class="line">    <span class="comment">// 每次只能有list、describe、create、alter、delete 中的一个操作 </span></span><br><span class="line">    <span class="keyword">if</span>(actions != <span class="number">1</span>)</span><br><span class="line">      <span class="type">CommandLineUtils</span>.printUsageAndDie(opts.parser, <span class="string">"Command must include exactly one action: --list, --describe, --create, --alter or --delete"</span>)</span><br><span class="line">	</span><br><span class="line">    <span class="comment">// 校验参数</span></span><br><span class="line">    opts.checkArgs()</span><br><span class="line">	</span><br><span class="line">    <span class="comment">// 初始化ZK信息</span></span><br><span class="line">    <span class="keyword">val</span> zkUtils = <span class="type">ZkUtils</span>(opts.options.valueOf(opts.zkConnectOpt),</span><br><span class="line">                          <span class="number">30000</span>,</span><br><span class="line">                          <span class="number">30000</span>,</span><br><span class="line">                          <span class="type">JaasUtils</span>.isZkSecurityEnabled())</span><br><span class="line">    <span class="keyword">var</span> exitCode = <span class="number">0</span></span><br><span class="line">    <span class="comment">// 执行具体的 Topic 操作</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">if</span>(opts.options.has(opts.createOpt))</span><br><span class="line">        createTopic(zkUtils, opts)</span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span>(opts.options.has(opts.alterOpt))</span><br><span class="line">        alterTopic(zkUtils, opts)</span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span>(opts.options.has(opts.listOpt))</span><br><span class="line">        listTopics(zkUtils, opts)</span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span>(opts.options.has(opts.describeOpt))</span><br><span class="line">        describeTopic(zkUtils, opts)</span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span>(opts.options.has(opts.deleteOpt))</span><br><span class="line">        deleteTopic(zkUtils, opts)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</span><br><span class="line">        println(<span class="string">"Error while executing topic command : "</span> + e.getMessage)</span><br><span class="line">        error(<span class="type">Utils</span>.stackTrace(e))</span><br><span class="line">        exitCode = <span class="number">1</span></span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      zkUtils.close()</span><br><span class="line">      <span class="type">System</span>.exit(exitCode)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>首先会通过伴生类 <code>TopicCommandOptions</code> 初始化一个参数列表<code>OptionParse</code> ，然后进行输入参数的接收；</p>
<p>之后判断接收的参数是否为空、是否只含一类操作（create、list、alter、delete、describe）、对应操作是否有必要的配置参数 ( <code>checkArgs()</code> )；</p>
<p>紧接着就是ZkUtils的初始化；</p>
<p>然后就是根据操作类型选择不同的实现方法：</p>
<h5 id="createTopic"><a href="#createTopic" class="headerlink" title="createTopic"></a>createTopic</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTopic</span></span>(zkUtils: <span class="type">ZkUtils</span>, opts: <span class="type">TopicCommandOptions</span>) &#123;</span><br><span class="line">    <span class="comment">// topic名称</span></span><br><span class="line">  <span class="keyword">val</span> topic = opts.options.valueOf(opts.topicOpt)</span><br><span class="line">    <span class="comment">// 创建topic的配置</span></span><br><span class="line">  <span class="keyword">val</span> configs = parseTopicConfigsToBeAdded(opts)</span><br><span class="line">    <span class="comment">// if-not-exists </span></span><br><span class="line">  <span class="keyword">val</span> ifNotExists = opts.options.has(opts.ifNotExistsOpt)</span><br><span class="line">    <span class="comment">// 检测topic</span></span><br><span class="line">  <span class="keyword">if</span> (<span class="type">Topic</span>.hasCollisionChars(topic))</span><br><span class="line">    println(<span class="string">"WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both."</span>)</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 是否指定分区副本 例如三分区两副本  --replica-assignment 0:1,1:2,2:0</span></span><br><span class="line">      <span class="comment">// parseReplicaAssignment 方法会进行格式的判断和格式化，','分隔分区，':'分隔分区中副本</span></span><br><span class="line">    <span class="keyword">if</span> (opts.options.has(opts.replicaAssignmentOpt)) &#123;</span><br><span class="line">      <span class="keyword">val</span> assignment = parseReplicaAssignment(opts.options.valueOf(opts.replicaAssignmentOpt))</span><br><span class="line">      <span class="type">AdminUtils</span>.createOrUpdateTopicPartitionAssignmentPathInZK(zkUtils, topic, assignment, configs, update = <span class="literal">false</span>)</span><br><span class="line">    &#125; <span class="comment">// 另外就是按照 分区数 副本数等自动分配</span></span><br><span class="line">      <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="type">CommandLineUtils</span>.checkRequiredArgs(opts.parser, opts.options, opts.partitionsOpt, opts.replicationFactorOpt)</span><br><span class="line">      <span class="keyword">val</span> partitions = opts.options.valueOf(opts.partitionsOpt).intValue</span><br><span class="line">      <span class="keyword">val</span> replicas = opts.options.valueOf(opts.replicationFactorOpt).intValue</span><br><span class="line">      <span class="keyword">val</span> rackAwareMode = <span class="keyword">if</span> (opts.options.has(opts.disableRackAware)) <span class="type">RackAwareMode</span>.<span class="type">Disabled</span></span><br><span class="line">                          <span class="keyword">else</span> <span class="type">RackAwareMode</span>.<span class="type">Enforced</span></span><br><span class="line">      <span class="type">AdminUtils</span>.createTopic(zkUtils, topic, partitions, replicas, configs, rackAwareMode)</span><br><span class="line">    &#125;</span><br><span class="line">    println(<span class="string">"Created topic \"%s\"."</span>.format(topic))</span><br><span class="line">  &#125; <span class="keyword">catch</span>  &#123;</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">TopicExistsException</span> =&gt; <span class="keyword">if</span> (!ifNotExists) <span class="keyword">throw</span> e</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>看 AdminUtils.createTopic 代码可以发现</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTopic</span></span>(zkUtils: <span class="type">ZkUtils</span>,</span><br><span class="line">                  topic: <span class="type">String</span>,</span><br><span class="line">                  partitions: <span class="type">Int</span>,</span><br><span class="line">                  replicationFactor: <span class="type">Int</span>,</span><br><span class="line">                  topicConfig: <span class="type">Properties</span> = <span class="keyword">new</span> <span class="type">Properties</span>,</span><br><span class="line">                  rackAwareMode: <span class="type">RackAwareMode</span> = <span class="type">RackAwareMode</span>.<span class="type">Enforced</span>) &#123;</span><br><span class="line">    <span class="comment">// 获取broker以及机架信息</span></span><br><span class="line">    <span class="keyword">val</span> brokerMetadatas = getBrokerMetadatas(zkUtils, rackAwareMode)</span><br><span class="line">    <span class="comment">// 根据broker信息自动分配分区副本</span></span><br><span class="line">    <span class="keyword">val</span> replicaAssignment = <span class="type">AdminUtils</span>.assignReplicasToBrokers(brokerMetadatas, partitions, replicationFactor)</span><br><span class="line">    <span class="comment">// 和手动分配分区副本调用一样的方法</span></span><br><span class="line">    <span class="type">AdminUtils</span>.createOrUpdateTopicPartitionAssignmentPathInZK(zkUtils, topic, replicaAssignment, topicConfig)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>kafka自动分配分区副本的方式：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">assignReplicasToBrokers</span></span>(brokerMetadatas: <span class="type">Seq</span>[<span class="type">BrokerMetadata</span>],</span><br><span class="line">                            nPartitions: <span class="type">Int</span>,</span><br><span class="line">                            replicationFactor: <span class="type">Int</span>,</span><br><span class="line">                            fixedStartIndex: <span class="type">Int</span> = <span class="number">-1</span>,</span><br><span class="line">                            startPartitionId: <span class="type">Int</span> = <span class="number">-1</span>): <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">Int</span>]] = &#123;</span><br><span class="line">    <span class="comment">// 分区数必须大于 0</span></span><br><span class="line">  <span class="keyword">if</span> (nPartitions &lt;= <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">InvalidPartitionsException</span>(<span class="string">"number of partitions must be larger than 0"</span>)</span><br><span class="line">    <span class="comment">// 副本数必须大于 0</span></span><br><span class="line">  <span class="keyword">if</span> (replicationFactor &lt;= <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">InvalidReplicationFactorException</span>(<span class="string">"replication factor must be larger than 0"</span>)</span><br><span class="line">    <span class="comment">// 副本数不大于 broker 节点数</span></span><br><span class="line">  <span class="keyword">if</span> (replicationFactor &gt; brokerMetadatas.size)</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">InvalidReplicationFactorException</span>(<span class="string">s"replication factor: <span class="subst">$replicationFactor</span> larger than available brokers: <span class="subst">$&#123;brokerMetadatas.size&#125;</span>"</span>)</span><br><span class="line">    <span class="comment">// 无机架信息时</span></span><br><span class="line">  <span class="keyword">if</span> (brokerMetadatas.forall(_.rack.isEmpty))</span><br><span class="line">    assignReplicasToBrokersRackUnaware(nPartitions, replicationFactor, brokerMetadatas.map(_.id), fixedStartIndex,</span><br><span class="line">      startPartitionId)</span><br><span class="line">    <span class="comment">// 有机架信息时</span></span><br><span class="line">  <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (brokerMetadatas.exists(_.rack.isEmpty))</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">AdminOperationException</span>(<span class="string">"Not all brokers have rack information for replica rack aware assignment"</span>)</span><br><span class="line">    assignReplicasToBrokersRackAware(nPartitions, replicationFactor, brokerMetadatas, fixedStartIndex,</span><br><span class="line">      startPartitionId)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong><em>无机架信息自动分配方式</em></strong>：（assignReplicasToBrokersRackUnaware）</p>
<p>（举例设定broker数为6 [0-5]，broker集合即为(0,1,2,3,4,5)，举例为集合下标）</p>
<ol>
<li><p>0号分区的leader为随机选的一台broker，第一个副本也为随机的不同于leader的broker的 broker，后续副本顺序循环增加（leader所在</p>
</li>
<li><p>除外），即 0号分区的 SR 为 [5, 1,2] or [4, 2,3,5 ] or [2 ,5,0,1]</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 此方法将确定后续副本只会除leader所在broker之外的其他broker</span></span><br><span class="line"><span class="comment">// 这也是副本数不大于broker数的原因  </span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">replicaIndex</span></span>(firstReplicaIndex: <span class="type">Int</span>, secondReplicaShift: <span class="type">Int</span>, replicaIndex: <span class="type">Int</span>, nBrokers: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> shift = <span class="number">1</span> + (secondReplicaShift + replicaIndex) % (nBrokers - <span class="number">1</span>)</span><br><span class="line">    (firstReplicaIndex + shift) % nBrokers</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>下一个分区的leader为上一个分区的broker+1 (broker数内循环)，而其第一个副本为上一个分区的第一个副本的broker+1(broker数内循环)，即 下一个分区的 SR 为 [0, 2,3] or [5, 3,4,0] or [3, 0,1,2] ，后续副本顺序循环增加</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 6个broker,9个分区的partition。分区id超过broker数时，其第一个副本的</span><br><span class="line">(5,1,2)</span><br><span class="line">(0,2,3)</span><br><span class="line">(1,3,4)</span><br><span class="line">(2,4,5)</span><br><span class="line">(3,5,0)</span><br><span class="line">(4,0,1)</span><br><span class="line">(5,2,3)</span><br><span class="line">(0,3,4)</span><br><span class="line">(1,4,5)</span><br></pre></td></tr></table></figure>



</li>
</ol>
<p><strong><em>有机架信息自动分配方式</em></strong>：（assignReplicasToBrokersRackAware）</p>
<ol>
<li><p>broker集合的顺序不一样，无机架是大小顺序排，有机架时，循环遍历机架，每次取一台broker放入集合，如下：</p>
<p>rack1：1,2,3     rack2：4,5,6   rack3：7,8,9    则broker集合为 ( 1,4,7,2,5,8,3,6,9 )</p>
</li>
<li><p>leader计算和副本选择计算和无机架方式一样，差别在于计算出来的副本broker和机架是否有效，无效则使用下一个broker。无效的判断：</p>
<ol>
<li>此机架已经有副本分配，但是还有一个或多个机架没有分配到副本</li>
<li>此broker已经有副本分配，但是还有一个或多个broker没有分配到副本</li>
</ol>
<p>总的来说就是，<strong>副本尽量在不同机架的不同broker上，均衡分布。</strong></p>
<p>​        </p>
</li>
</ol>
<p><strong>两种创建方式主要体现在分区副本的分配上，其他步骤都是一样的：</strong></p>
<p><code>validateCreateOrUpdateTopic</code> :</p>
<ul>
<li>topic名称校验：不能为空，不能是 ‘.’ 和 ‘..’，长度不能超过249，正则匹配(只能是”[a-zA-Z0-9\._\-]”)</li>
<li>topic是否已存在：</li>
<li>每个分区上的副本数是否一致：</li>
</ul>
<p><code>writeEntityConfig</code> ： Topic相关配置信息写入ZK ( zk_kafka_root/config/topic)</p>
<p><code>writeTopicPartitionAssignment</code> ：分区副本信息写入ZK对应broker上（ zk_kafka_root/brokers/topics/topic ）</p>
<p><em>分区目录的创建会在创建完topic后自动创建</em></p>
<h5 id="alterTopic"><a href="#alterTopic" class="headerlink" title="alterTopic"></a>alterTopic</h5><p>修改topic的属性，包括增加更新删除属性，同时也可以扩分区</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">alterTopic</span></span>(zkUtils: <span class="type">ZkUtils</span>, opts: <span class="type">TopicCommandOptions</span>) &#123;</span><br><span class="line">    <span class="comment">// --topic 可以正则匹配</span></span><br><span class="line">  <span class="keyword">val</span> topics = getTopics(zkUtils, opts)</span><br><span class="line">  <span class="keyword">val</span> ifExists = opts.options.has(opts.ifExistsOpt)</span><br><span class="line">    <span class="comment">// 未找到topic同时无 if-not-exist 则打印错误</span></span><br><span class="line">  <span class="keyword">if</span> (topics.isEmpty &amp;&amp; !ifExists) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">"Topic %s does not exist on ZK path %s"</span>.format(opts.options.valueOf(opts.topicOpt),</span><br><span class="line">        opts.options.valueOf(opts.zkConnectOpt)))</span><br><span class="line">  &#125;</span><br><span class="line">  topics.foreach &#123; topic =&gt;</span><br><span class="line">    <span class="keyword">val</span> configs = <span class="type">AdminUtils</span>.fetchEntityConfig(zkUtils, <span class="type">ConfigType</span>.<span class="type">Topic</span>, topic)</span><br><span class="line">    <span class="keyword">if</span>(opts.options.has(opts.configOpt) || opts.options.has(opts.deleteConfigOpt)) &#123;</span><br><span class="line">      println(<span class="string">"WARNING: Altering topic configuration from this script has been deprecated and may be removed in future releases."</span>)</span><br><span class="line">      println(<span class="string">"         Going forward, please use kafka-configs.sh for this functionality"</span>)</span><br><span class="line"><span class="comment">// --config 需要增加或者更新的配置项</span></span><br><span class="line">      <span class="keyword">val</span> configsToBeAdded = parseTopicConfigsToBeAdded(opts)</span><br><span class="line">      <span class="comment">// --delete-config 需要删除的配置</span></span><br><span class="line">      <span class="keyword">val</span> configsToBeDeleted = parseTopicConfigsToBeDeleted(opts)</span><br><span class="line">      <span class="comment">// compile the final set of configs</span></span><br><span class="line">      configs.putAll(configsToBeAdded)</span><br><span class="line">      configsToBeDeleted.foreach(config =&gt; configs.remove(config))</span><br><span class="line">      <span class="type">AdminUtils</span>.changeTopicConfig(zkUtils, topic, configs)</span><br><span class="line">      println(<span class="string">"Updated config for topic \"%s\"."</span>.format(topic))</span><br><span class="line">    &#125;</span><br><span class="line"> <span class="comment">// 如果选型有 --partitions 说明还需要对分区进行变更</span></span><br><span class="line">    <span class="keyword">if</span>(opts.options.has(opts.partitionsOpt)) &#123;</span><br><span class="line">      <span class="comment">// 内部分区 __comsumer_offsets 无法更改</span></span><br><span class="line">      <span class="keyword">if</span> (topic == <span class="type">Topic</span>.<span class="type">GroupMetadataTopicName</span>) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">"The number of partitions for the offsets topic cannot be changed."</span>)</span><br><span class="line">      &#125;</span><br><span class="line">      println(<span class="string">"WARNING: If partitions are increased for a topic that has a key, the partition "</span> +</span><br><span class="line">        <span class="string">"logic or ordering of the messages will be affected"</span>)</span><br><span class="line">        <span class="comment">// 新分区数</span></span><br><span class="line">      <span class="keyword">val</span> nPartitions = opts.options.valueOf(opts.partitionsOpt).intValue</span><br><span class="line">        <span class="comment">// 分区只能增加，同时分区副本是根据之前的分区副本分配规则延续的</span></span><br><span class="line">      <span class="keyword">val</span> replicaAssignmentStr = opts.options.valueOf(opts.replicaAssignmentOpt)</span><br><span class="line">      <span class="type">AdminUtils</span>.addPartitions(zkUtils, topic, nPartitions, replicaAssignmentStr)</span><br><span class="line">      println(<span class="string">"Adding partitions succeeded!"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h5 id="listTopic"><a href="#listTopic" class="headerlink" title="listTopic"></a>listTopic</h5><p>列出topic名称，可根据正则匹配，代码很简单，去ZK获取相关信息，然后打印</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">listTopics</span></span>(zkUtils: <span class="type">ZkUtils</span>, opts: <span class="type">TopicCommandOptions</span>) &#123;</span><br><span class="line">   <span class="keyword">val</span> topics = getTopics(zkUtils, opts)</span><br><span class="line">   <span class="keyword">for</span>(topic &lt;- topics) &#123;</span><br><span class="line">     <span class="keyword">if</span> (zkUtils.pathExists(getDeleteTopicPath(topic))) &#123;</span><br><span class="line">       println(<span class="string">"%s - marked for deletion"</span>.format(topic))</span><br><span class="line">     &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">       println(topic)</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>



<h5 id="deleteTopic"><a href="#deleteTopic" class="headerlink" title="deleteTopic"></a>deleteTopic</h5><p>删除操作只是单纯做一个标记，在ZK对应删除的节点下增加需要删除的topic。如果 broker 端参数 <code>delete.topic.enable</code> 不是true，那么删除操作不会有其他任何影响，除了标记之外。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deleteTopic</span></span>(zkUtils: <span class="type">ZkUtils</span>, opts: <span class="type">TopicCommandOptions</span>) &#123;</span><br><span class="line">  <span class="keyword">val</span> topics = getTopics(zkUtils, opts)</span><br><span class="line">  <span class="keyword">val</span> ifExists = opts.options.has(opts.ifExistsOpt)</span><br><span class="line">  <span class="keyword">if</span> (topics.isEmpty &amp;&amp; !ifExists) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">"Topic %s does not exist on ZK path %s"</span>.format(opts.options.valueOf(opts.topicOpt),</span><br><span class="line">        opts.options.valueOf(opts.zkConnectOpt)))</span><br><span class="line">  &#125;</span><br><span class="line">  topics.foreach &#123; topic =&gt;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (<span class="type">Topic</span>.isInternal(topic)) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">AdminOperationException</span>(<span class="string">"Topic %s is a kafka internal topic and is not allowed to be marked for deletion."</span>.format(topic))</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// 标记删除， admin/delete_topics/topic</span></span><br><span class="line">        zkUtils.createPersistentPath(getDeleteTopicPath(topic))</span><br><span class="line">        println(<span class="string">"Topic %s is marked for deletion."</span>.format(topic))</span><br><span class="line">        println(<span class="string">"Note: This will have no impact if delete.topic.enable is not set to true."</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> _: <span class="type">ZkNodeExistsException</span> =&gt;</span><br><span class="line">        println(<span class="string">"Topic %s is already marked for deletion."</span>.format(topic))</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">AdminOperationException</span> =&gt;</span><br><span class="line">        <span class="keyword">throw</span> e</span><br><span class="line">      <span class="keyword">case</span> _: <span class="type">Throwable</span> =&gt;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">AdminOperationException</span>(<span class="string">"Error while deleting topic %s"</span>.format(topic))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h5 id="describeTopic"><a href="#describeTopic" class="headerlink" title="describeTopic"></a>describeTopic</h5><p>topic可正则，额外有三个特殊配置需要注意。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">describeTopic</span></span>(zkUtils: <span class="type">ZkUtils</span>, opts: <span class="type">TopicCommandOptions</span>) &#123;</span><br><span class="line">  <span class="keyword">val</span> topics = getTopics(zkUtils, opts)</span><br><span class="line">  <span class="comment">// 只列出副本未跟上分区</span></span><br><span class="line">  <span class="keyword">val</span> reportUnderReplicatedPartitions = opts.options.has(opts.reportUnderReplicatedPartitionsOpt)</span><br><span class="line">  <span class="comment">// 只列出leader不可用分区</span></span><br><span class="line">  <span class="keyword">val</span> reportUnavailablePartitions = opts.options.has(opts.reportUnavailablePartitionsOpt)</span><br><span class="line">  <span class="comment">// 只列出配置被覆盖过的topic</span></span><br><span class="line">  <span class="keyword">val</span> reportOverriddenConfigs = opts.options.has(opts.topicsWithOverridesOpt)</span><br><span class="line">  <span class="keyword">val</span> liveBrokers = zkUtils.getAllBrokersInCluster().map(_.id).toSet</span><br><span class="line">  <span class="keyword">for</span> (topic &lt;- topics) &#123;</span><br><span class="line">    zkUtils.getPartitionAssignmentForTopics(<span class="type">List</span>(topic)).get(topic) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(topicPartitionAssignment) =&gt;</span><br><span class="line">        <span class="keyword">val</span> describeConfigs: <span class="type">Boolean</span> = !reportUnavailablePartitions &amp;&amp; !reportUnderReplicatedPartitions</span><br><span class="line">        <span class="keyword">val</span> describePartitions: <span class="type">Boolean</span> = !reportOverriddenConfigs</span><br><span class="line">        <span class="keyword">val</span> sortedPartitions = topicPartitionAssignment.toList.sortWith((m1, m2) =&gt; m1._1 &lt; m2._1)</span><br><span class="line">        <span class="keyword">if</span> (describeConfigs) &#123;</span><br><span class="line">          <span class="keyword">val</span> configs = <span class="type">AdminUtils</span>.fetchEntityConfig(zkUtils, <span class="type">ConfigType</span>.<span class="type">Topic</span>, topic).asScala</span><br><span class="line">          <span class="keyword">if</span> (!reportOverriddenConfigs || configs.nonEmpty) &#123;</span><br><span class="line">            <span class="keyword">val</span> numPartitions = topicPartitionAssignment.size</span><br><span class="line">            <span class="keyword">val</span> replicationFactor = topicPartitionAssignment.head._2.size</span><br><span class="line">            println(<span class="string">"Topic:%s\tPartitionCount:%d\tReplicationFactor:%d\tConfigs:%s"</span></span><br><span class="line">              .format(topic, numPartitions, replicationFactor, configs.map(kv =&gt; kv._1 + <span class="string">"="</span> + kv._2).mkString(<span class="string">","</span>)))</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (describePartitions) &#123;</span><br><span class="line">          <span class="keyword">for</span> ((partitionId, assignedReplicas) &lt;- sortedPartitions) &#123;</span><br><span class="line">            <span class="keyword">val</span> inSyncReplicas = zkUtils.getInSyncReplicasForPartition(topic, partitionId)</span><br><span class="line">            <span class="keyword">val</span> leader = zkUtils.getLeaderForPartition(topic, partitionId)</span><br><span class="line">            <span class="keyword">if</span> ((!reportUnderReplicatedPartitions &amp;&amp; !reportUnavailablePartitions) ||</span><br><span class="line">                (reportUnderReplicatedPartitions &amp;&amp; inSyncReplicas.size &lt; assignedReplicas.size) ||</span><br><span class="line">                (reportUnavailablePartitions &amp;&amp; (leader.isEmpty || !liveBrokers.contains(leader.get)))) &#123;</span><br><span class="line">              print(<span class="string">"\tTopic: "</span> + topic)</span><br><span class="line">              print(<span class="string">"\tPartition: "</span> + partitionId)</span><br><span class="line">              print(<span class="string">"\tLeader: "</span> + (<span class="keyword">if</span>(leader.isDefined) leader.get <span class="keyword">else</span> <span class="string">"none"</span>))</span><br><span class="line">              print(<span class="string">"\tReplicas: "</span> + assignedReplicas.mkString(<span class="string">","</span>))</span><br><span class="line">              println(<span class="string">"\tIsr: "</span> + inSyncReplicas.mkString(<span class="string">","</span>))</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">        println(<span class="string">"Topic "</span> + topic + <span class="string">" doesn't exist!"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>




]]></content>
      <categories>
        <category>Apache-Kafka</category>
        <category>Kafka源码入门</category>
        <category>客户端命令</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Kafka</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title>kafka-config.sh</title>
    <url>/2020/08/16/Apache-Kafka/Kafka%E6%BA%90%E7%A0%81%E5%85%A5%E9%97%A8/01.%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%91%BD%E4%BB%A4/01.kafka-config.sh/</url>
    <content><![CDATA[<h3 id="啥都没"><a href="#啥都没" class="headerlink" title="啥都没"></a>啥都没</h3>]]></content>
      <categories>
        <category>Apache-Kafka</category>
        <category>Kafka源码入门</category>
        <category>客户端命令</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Kafka</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title>kafka启动流程</title>
    <url>/2020/07/20/Apache-Kafka/Kafka%E6%BA%90%E7%A0%81%E5%85%A5%E9%97%A8/02.Kafka%E6%9C%8D%E5%8A%A1%E7%AB%AF/00.kafka%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/</url>
    <content><![CDATA[<p> 通过 <code>kafka-server-start.sh</code> 发现，启动类为 <code>kafka.Kafka</code>，直接看类代码：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 服务启动配置</span></span><br><span class="line">    <span class="keyword">val</span> serverProps = getPropsFromArgs(args)</span><br><span class="line">      <span class="comment">// 启动类</span></span><br><span class="line">    <span class="keyword">val</span> kafkaServerStartable = <span class="type">KafkaServerStartable</span>.fromProps(serverProps)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// attach shutdown handler to catch control-c</span></span><br><span class="line">    <span class="type">Runtime</span>.getRuntime().addShutdownHook(<span class="keyword">new</span> <span class="type">Thread</span>() &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() = &#123;</span><br><span class="line">        kafkaServerStartable.shutdown</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    kafkaServerStartable.startup</span><br><span class="line">    kafkaServerStartable.awaitShutdown</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</span><br><span class="line">      fatal(e)</span><br><span class="line">      <span class="type">System</span>.exit(<span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="type">System</span>.exit(<span class="number">0</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>启动为 KafkaServerStartable 对象进行启动，而 KafkaServerStartable 中具体的实现是 KafkaServer对象的启动，具体启动方法是：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Start up API for bringing up a single instance of the Kafka server.</span></span><br><span class="line"><span class="comment">   * Instantiates the LogManager, the SocketServer and the request handlers - KafkaRequestHandlers</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      info(<span class="string">"starting"</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span>(isShuttingDown.get)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"Kafka server is still shutting down, cannot re-start!"</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span>(startupComplete.get)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> canStartup = isStartingUp.compareAndSet(<span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">      <span class="keyword">if</span> (canStartup) &#123;</span><br><span class="line">        <span class="comment">// 状态调整为 Starting</span></span><br><span class="line">        brokerState.newState(<span class="type">Starting</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* start scheduler 启动调度器 */</span></span><br><span class="line">        kafkaScheduler.startup()</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* setup zookeeper 初始化 ZK连接，同时创建 根节点.*/</span></span><br><span class="line">        zkUtils = initZk()</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* Get or create cluster_id 创建或获取 cluster id*/</span></span><br><span class="line">        _clusterId = getOrGenerateClusterId(zkUtils)</span><br><span class="line">        info(<span class="string">s"Cluster ID = <span class="subst">$clusterId</span>"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* generate brokerId 获取 broker id 以及broker上对应的log_dirs*/</span></span><br><span class="line">        config.brokerId =  getBrokerId</span><br><span class="line">        <span class="keyword">this</span>.logIdent = <span class="string">"[Kafka Server "</span> + config.brokerId + <span class="string">"], "</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">/* create and configure metrics 创建一个metrics，这个metrics提供给kafka内部使用*/</span></span><br><span class="line">        <span class="keyword">val</span> reporters = config.getConfiguredInstances(<span class="type">KafkaConfig</span>.<span class="type">MetricReporterClassesProp</span>, classOf[<span class="type">MetricsReporter</span>],</span><br><span class="line">            <span class="type">Map</span>[<span class="type">String</span>, <span class="type">AnyRef</span>](<span class="type">KafkaConfig</span>.<span class="type">BrokerIdProp</span> -&gt; (config.brokerId.toString)).asJava)</span><br><span class="line">        reporters.add(<span class="keyword">new</span> <span class="type">JmxReporter</span>(jmxPrefix))</span><br><span class="line">        <span class="keyword">val</span> metricConfig = <span class="type">KafkaServer</span>.metricConfig(config)</span><br><span class="line">        metrics = <span class="keyword">new</span> <span class="type">Metrics</span>(metricConfig, reporters, time, <span class="literal">true</span>)</span><br><span class="line">		</span><br><span class="line">        <span class="comment">// 容量管理的一个东西，比如限制kafka producer生产传输速度</span></span><br><span class="line">        quotaManagers = <span class="type">QuotaFactory</span>.instantiate(config, metrics, time)</span><br><span class="line">        notifyClusterListeners(kafkaMetricsReporters ++ reporters.asScala)</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* start log manager 构建LogManager，并运行。broker端的重要线程 */</span></span><br><span class="line">        logManager = createLogManager(zkUtils.zkClient, brokerState)</span><br><span class="line">        logManager.startup()</span><br><span class="line">          </span><br><span class="line">		<span class="comment">//  每个partition的状态缓存，每个broker都会异步维护同一个缓存，更新请求来自controller</span></span><br><span class="line">        metadataCache = <span class="keyword">new</span> <span class="type">MetadataCache</span>(config.brokerId)</span><br><span class="line">        credentialProvider = <span class="keyword">new</span> <span class="type">CredentialProvider</span>(config.saslEnabledMechanisms)</span><br><span class="line">		<span class="comment">// NIO socket server 创建和启动</span></span><br><span class="line">        socketServer = <span class="keyword">new</span> <span class="type">SocketServer</span>(config, metrics, time, credentialProvider)</span><br><span class="line">        socketServer.startup()</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* start replica manager 副本管理器初始化和启动 */</span></span><br><span class="line">        replicaManager = <span class="keyword">new</span> <span class="type">ReplicaManager</span>(config, metrics, time, zkUtils, kafkaScheduler, logManager,</span><br><span class="line">          isShuttingDown, quotaManagers.follower)</span><br><span class="line">        replicaManager.startup()</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* start kafka controller 控制器初始化和启动 */</span></span><br><span class="line">        kafkaController = <span class="keyword">new</span> <span class="type">KafkaController</span>(config, zkUtils, brokerState, time, metrics, threadNamePrefix)</span><br><span class="line">        kafkaController.startup()</span><br><span class="line"></span><br><span class="line">        adminManager = <span class="keyword">new</span> <span class="type">AdminManager</span>(config, metrics, metadataCache, zkUtils)</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* start group coordinator 管理消费组成员和offset */</span></span><br><span class="line">        <span class="comment">// Hardcode Time.SYSTEM for now as some Streams tests fail otherwise, it would be good to fix the underlying issue</span></span><br><span class="line">        groupCoordinator = <span class="type">GroupCoordinator</span>(config, zkUtils, replicaManager, <span class="type">Time</span>.<span class="type">SYSTEM</span>)</span><br><span class="line">        groupCoordinator.startup()</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* Get the authorizer and initialize it if one is specified.*/</span></span><br><span class="line">        authorizer = <span class="type">Option</span>(config.authorizerClassName).filter(_.nonEmpty).map &#123; authorizerClassName =&gt;</span><br><span class="line">          <span class="keyword">val</span> authZ = <span class="type">CoreUtils</span>.createObject[<span class="type">Authorizer</span>](authorizerClassName)</span><br><span class="line">          authZ.configure(config.originals())</span><br><span class="line">          authZ</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* start processing requests 处理各种kafka的请求*/</span></span><br><span class="line">        apis = <span class="keyword">new</span> <span class="type">KafkaApis</span>(socketServer.requestChannel, replicaManager, adminManager, groupCoordinator,</span><br><span class="line">          kafkaController, zkUtils, config.brokerId, config, metadataCache, metrics, authorizer, quotaManagers,</span><br><span class="line">          clusterId, time)</span><br><span class="line">		<span class="comment">// 请求池</span></span><br><span class="line">        requestHandlerPool = <span class="keyword">new</span> <span class="type">KafkaRequestHandlerPool</span>(config.brokerId, socketServer.requestChannel, apis, time,</span><br><span class="line">          config.numIoThreads)</span><br><span class="line"></span><br><span class="line">        <span class="type">Mx4jLoader</span>.maybeLoad()</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* start dynamic config manager Topic 配置管理请求，client配置管理请求，broker配置管理请求，用户配置管理请求 */</span></span><br><span class="line">        dynamicConfigHandlers = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">ConfigHandler</span>](<span class="type">ConfigType</span>.<span class="type">Topic</span> -&gt; <span class="keyword">new</span> <span class="type">TopicConfigHandler</span>(logManager, config, quotaManagers),</span><br><span class="line">                                                           <span class="type">ConfigType</span>.<span class="type">Client</span> -&gt; <span class="keyword">new</span> <span class="type">ClientIdConfigHandler</span>(quotaManagers),</span><br><span class="line">                                                           <span class="type">ConfigType</span>.<span class="type">User</span> -&gt; <span class="keyword">new</span> <span class="type">UserConfigHandler</span>(quotaManagers, credentialProvider),</span><br><span class="line">                                                           <span class="type">ConfigType</span>.<span class="type">Broker</span> -&gt; <span class="keyword">new</span> <span class="type">BrokerConfigHandler</span>(config, quotaManagers))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Create the config manager. start listening to notifications</span></span><br><span class="line">        <span class="comment">// 动态监听所有请求，开始启动</span></span><br><span class="line">        dynamicConfigManager = <span class="keyword">new</span> <span class="type">DynamicConfigManager</span>(zkUtils, dynamicConfigHandlers)</span><br><span class="line">        dynamicConfigManager.startup()</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* tell everyone we are alive 监听kafka进程状态，健康管理*/</span></span><br><span class="line">        <span class="keyword">val</span> listeners = config.advertisedListeners.map &#123; endpoint =&gt;</span><br><span class="line">          <span class="keyword">if</span> (endpoint.port == <span class="number">0</span>)</span><br><span class="line">            endpoint.copy(port = socketServer.boundPort(endpoint.listenerName))</span><br><span class="line">          <span class="keyword">else</span></span><br><span class="line">            endpoint</span><br><span class="line">        &#125;</span><br><span class="line">        kafkaHealthcheck = <span class="keyword">new</span> <span class="type">KafkaHealthcheck</span>(config.brokerId, listeners, zkUtils, config.rack,</span><br><span class="line">          config.interBrokerProtocolVersion)</span><br><span class="line">        kafkaHealthcheck.startup()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Now that the broker id is successfully registered via KafkaHealthcheck, checkpoint it</span></span><br><span class="line">        checkpointBrokerId(config.brokerId)</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* register broker metrics */</span></span><br><span class="line">        registerStats()</span><br><span class="line">		<span class="comment">// 更新broker状态为 Runing</span></span><br><span class="line">        brokerState.newState(<span class="type">RunningAsBroker</span>)</span><br><span class="line">        shutdownLatch = <span class="keyword">new</span> <span class="type">CountDownLatch</span>(<span class="number">1</span>)</span><br><span class="line">        <span class="comment">// 启动完成，修改相关状态</span></span><br><span class="line">        startupComplete.set(<span class="literal">true</span>)</span><br><span class="line">        isStartingUp.set(<span class="literal">false</span>)</span><br><span class="line">        <span class="type">AppInfoParser</span>.registerAppInfo(jmxPrefix, config.brokerId.toString)</span><br><span class="line">        info(<span class="string">"started"</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</span><br><span class="line">        fatal(<span class="string">"Fatal error during KafkaServer startup. Prepare to shutdown"</span>, e)</span><br><span class="line">        isStartingUp.set(<span class="literal">false</span>)</span><br><span class="line">        shutdown()</span><br><span class="line">        <span class="keyword">throw</span> e</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>总的来说，在KafkaServer启动流程中，先后启动了以下几个模块：</p>
<ol>
<li>kafkaScheduler 调度模块，负责kafka内部的周期性调度和非周期性调度</li>
<li>初始化 Zookeeper 工具类</li>
<li>实例化 metrics</li>
<li>quotaManagers 限额管理模块</li>
<li>logManager 日志管理模块</li>
<li>socketServer 网络服务模块</li>
<li>replicaManager 副本管理模块</li>
<li>kafkaController kafka控制器</li>
<li>adminManager 暂时不知道具体干嘛的，应该做客户端管理用的</li>
<li>groupCoordinator  消费组协调器</li>
<li>apis 和 requestHandlerPool 请求API和请求池</li>
<li>dynamicConfigManager 动态配置管理</li>
</ol>
]]></content>
      <categories>
        <category>Apache-Kafka</category>
        <category>Kafka源码入门</category>
        <category>客户端命令</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Kafka</tag>
        <tag>源码</tag>
      </tags>
  </entry>
</search>
