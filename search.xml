<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Kafka客户端命令操作</title>
    <url>/2020/10/08/Kafka%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<h4 id="Topic-管理"><a href="#Topic-管理" class="headerlink" title="Topic 管理"></a>Topic 管理</h4><p><code>kafka-topics.sh</code>  的参数</p>
<table>
<thead>
<tr>
<th align="left">参数名称</th>
<th>解释及其作用</th>
</tr>
</thead>
<tbody><tr>
<td align="left">–alter</td>
<td>用于修改主题，包括分区数及Topic的配置</td>
</tr>
<tr>
<td align="left">–config</td>
<td>创建或修改Topic时，用于设置 Topic 级别的配置</td>
</tr>
<tr>
<td align="left">–create</td>
<td>创建 Topic</td>
</tr>
<tr>
<td align="left">–delete</td>
<td>删除 Topic</td>
</tr>
<tr>
<td align="left">–delete-config</td>
<td>删除 Topic 级别被覆盖的配置</td>
</tr>
<tr>
<td align="left">–describe</td>
<td>查看 Topic 的详细信息</td>
</tr>
<tr>
<td align="left">–disable-rack-aware</td>
<td>创建 Topic 时不考虑机架信息</td>
</tr>
<tr>
<td align="left">–help</td>
<td>打印帮助信息文档</td>
</tr>
<tr>
<td align="left">–if-exists</td>
<td>修改或删除 Topic时，只有 Topic 存在时才会执行操作</td>
</tr>
<tr>
<td align="left">–if-not-exists</td>
<td>创建 Topic 时，只有 Topic 不存在才会执行操作</td>
</tr>
<tr>
<td align="left">–list</td>
<td>列出所有可用的 Topic</td>
</tr>
<tr>
<td align="left">–partitions</td>
<td>创建 Topic 或增加分区时 指定分区数</td>
</tr>
<tr>
<td align="left">–replica-assignment</td>
<td>手工制定分区副本的分配方案</td>
</tr>
<tr>
<td align="left">–replication-factor</td>
<td>创建 Topic 时指定副本数</td>
</tr>
<tr>
<td align="left">–topic</td>
<td>指定 Topic 名称</td>
</tr>
<tr>
<td align="left">–topics-with-overrides</td>
<td>使用describe查看 Topic 时，只展示包含覆盖配置的 Topic</td>
</tr>
<tr>
<td align="left">–unavailable-partition</td>
<td>使用describe查看 Topic 时，只展示包含无leader的分区</td>
</tr>
<tr>
<td align="left">–under-replicated-partitions</td>
<td>使用describe查看 Topic 时，只展示包含失效副本的分区</td>
</tr>
<tr>
<td align="left">–zookeeper</td>
<td>指定连接的zookeeper信息（必填）（zk1:2181/kafka）</td>
</tr>
</tbody></table>
<h6 id="创建-Topic"><a href="#创建-Topic" class="headerlink" title="创建 Topic"></a>创建 Topic</h6><p>首先确认一个参数 <code>auto.create.topics.enable=true</code> ，此参数用来自动创建 Topic，也就说当发送消费等操作用到了未创建的 <code>Topic</code> 的时候，此参数会帮忙自动创建一个默认配置的 Topic。<strong>强烈建议在 Kafka 配置关闭这个参数。</strong></p>
<p> 接下来看看怎么在客户端手动创建一个 <code>Topic</code>，使用  </p>
<p>-topics.sh<code>，指定</code>Zookeeper、partition、replica`  完成创建即可。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# pwd</span><br><span class="line">/data/kafka/kafka_2.11-0.10.2.1/bin</span><br><span class="line">[root@tnode1 bin]# ./kafka-topics.sh --create --zookeeper tnode3:2181 --replication-factor 2 --partitions 3 --topic LxmTest</span><br><span class="line">Created topic "LxmTest".</span><br></pre></td></tr></table></figure>

<h6 id="查看-Topic"><a href="#查看-Topic" class="headerlink" title="查看 Topic"></a>查看 Topic</h6><p>查看所有 <code>Topic</code></p>
<p>查看所有 <code>Topic</code> 信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-topics.sh --zookeeper tnode3:2181  --describe</span><br><span class="line">Topic:LxmTest	PartitionCount:3	ReplicationFactor:2	Configs:</span><br><span class="line">	Topic: LxmTest	Partition: 0	Leader: 1	Replicas: 1,0	Isr: 1,0</span><br><span class="line">	Topic: LxmTest	Partition: 1	Leader: 2	Replicas: 2,1	Isr: 2,1</span><br><span class="line">	Topic: LxmTest	Partition: 2	Leader: 0	Replicas: 0,2	Isr: 0,2</span><br><span class="line">... </span><br><span class="line">(其他 topic 信息省略)</span><br></pre></td></tr></table></figure>

<p>查看单个 <code>Topic</code> 信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-topics.sh --zookeeper tnode3:2181  --describe --topic LxmTest</span><br><span class="line">Topic:LxmTest	PartitionCount:3	ReplicationFactor:2	Configs:</span><br><span class="line">	Topic: LxmTest	Partition: 0	Leader: 1	Replicas: 1,0	Isr: 1,0</span><br><span class="line">	Topic: LxmTest	Partition: 1	Leader: 2	Replicas: 2,1	Isr: 2,1</span><br><span class="line">	Topic: LxmTest	Partition: 2	Leader: 0	Replicas: 0,2	Isr: 0,2</span><br></pre></td></tr></table></figure>

<h6 id="删除-Topic"><a href="#删除-Topic" class="headerlink" title="删除 Topic"></a>删除 Topic</h6><p>删除 Topic 分为手动和自动两种方式</p>
<ul>
<li>手动删除 Topic ：需要先删除 Zookeeper 上的相关元数据，然后删除各个 Broker 节点上日志目录的下的此 Topic 的分区日志。</li>
<li>自动删除 Topic ：首先 <code>delete.topic.enable=true</code> 参数必须配置，然后通过 客户端命令删除 Topic，将会自动将元数据和各分区的数据日志删除。如果未配置，将只会删除元数据，不会自动删除日志。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-topics.sh --zookeeper tnode3:2181  --delete --topic LxmTest</span><br><span class="line">Topic LxmTest is marked for deletion.</span><br><span class="line">Note: This will have no impact if delete.topic.enable is not set to true.</span><br></pre></td></tr></table></figure>

<h6 id="增加分区"><a href="#增加分区" class="headerlink" title="增加分区"></a>增加分区</h6><p>一般来说，我们在需要提升较大吞吐的情况下，可以选择增加 partition 。扩展分区命令如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-topics.sh --zookeeper tnode3:2181 --topic LxmTest  --alter --partitions 3</span><br><span class="line">WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affected</span><br><span class="line">Error while executing topic command : The number of partitions for a topic can only be increased</span><br><span class="line">[2019-08-23 08:42:20,176] ERROR kafka.admin.AdminOperationException: The number of partitions for a topic can only be increased</span><br><span class="line">	at kafka.admin.AdminUtils$.addPartitions(AdminUtils.scala:271)</span><br><span class="line">	at kafka.admin.TopicCommand$$anonfun$alterTopic$1.apply(TopicCommand.scala:145)</span><br><span class="line">	at kafka.admin.TopicCommand$$anonfun$alterTopic$1.apply(TopicCommand.scala:122)</span><br><span class="line">	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)</span><br><span class="line">	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)</span><br><span class="line">	at kafka.admin.TopicCommand$.alterTopic(TopicCommand.scala:122)</span><br><span class="line">	at kafka.admin.TopicCommand$.main(TopicCommand.scala:62)</span><br><span class="line">	at kafka.admin.TopicCommand.main(TopicCommand.scala)</span><br><span class="line"> (kafka.admin.TopicCommand$)</span><br><span class="line">[root@tnode1 bin]# ./kafka-topics.sh --zookeeper tnode3:2181 --topic LxmTest  --alter --partitions 4</span><br><span class="line">WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affected</span><br><span class="line">Adding partitions succeeded!</span><br></pre></td></tr></table></figure>

<p>目前分区只能增加，不能减少或者不变的进行修改。</p>
<h4 id="配置管理"><a href="#配置管理" class="headerlink" title="配置管理"></a>配置管理</h4><p>配置管理分为 Topic 级别和 Client 级别两种</p>
<h6 id="Topics-级别"><a href="#Topics-级别" class="headerlink" title="Topics  级别"></a>Topics  级别</h6><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --describe</span><br><span class="line">Configs for topic 'LxmTest' are</span><br></pre></td></tr></table></figure>

<p>这里的结果表示没有做单独配置，均使用的集群默认配置。</p>
<p>增加一个配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --alter --add-config flush.messages=2</span><br><span class="line">Completed Updating config for entity: topic 'LxmTest'.</span><br><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --describe</span><br><span class="line">Configs for topic 'LxmTest' are flush.messages=2</span><br></pre></td></tr></table></figure>

<p>再增加一个配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --alter --add-config retention.ms=259200000</span><br><span class="line">Completed Updating config for entity: topic 'LxmTest'.</span><br><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --describe</span><br><span class="line">Configs for topic 'LxmTest' are retention.ms=259200000,flush.messages=2</span><br></pre></td></tr></table></figure>

<p>一次性添加多个配置，配置用逗号分隔即可</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --alter --add-config retention.ms=259200000,flush.messages=6</span><br><span class="line">Completed Updating config for entity: topic 'LxmTest'.</span><br><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --describe</span><br><span class="line">Configs for topic 'LxmTest' are retention.ms=259200000,flush.messages=6</span><br></pre></td></tr></table></figure>

<p>修改其中一个配置，其实增加没有区别</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --alter --add-config flush.messages=5</span><br><span class="line">Completed Updating config for entity: topic 'LxmTest'.</span><br><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --describe</span><br><span class="line">Configs for topic 'LxmTest' are retention.ms=259200000,flush.messages=5</span><br></pre></td></tr></table></figure>

<p>删除一个配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --alter --delete-config retention.ms</span><br><span class="line">Completed Updating config for entity: topic 'LxmTest'.</span><br><span class="line">[root@tnode1 bin]# ./kafka-configs.sh --zookeeper tnode3:2181 --entity-type topics --entity-name LxmTest --describe</span><br><span class="line">Configs for topic 'LxmTest' are flush.messages=5</span><br></pre></td></tr></table></figure>

<h6 id="Clients-级别"><a href="#Clients-级别" class="headerlink" title="Clients 级别"></a>Clients 级别</h6><h4 id="分区管理"><a href="#分区管理" class="headerlink" title="分区管理"></a>分区管理</h4><h6 id="分区平衡"><a href="#分区平衡" class="headerlink" title="分区平衡"></a>分区平衡</h6><p>分区平衡分为自动和手动两种：</p>
<ul>
<li>自动：在 server.properties 中配置参数 <code>auto.leader.rebalance.enable = true</code> ，那么集群将会定时进行分区平衡，将 prefer-replica 副本即AR列表第一个副本，置为 leader 副本。 (<code>leader.imbalance.per.broker.percentage = 10</code> 此参数来决定自动平衡的阈值即失衡率超过10%会开始平衡)</li>
<li>手动：执行平衡命令：<code>./kafka-preferred-replica-election.sh --zookeeper tnode3:2181</code></li>
</ul>
<h6 id="分区迁移"><a href="#分区迁移" class="headerlink" title="分区迁移"></a>分区迁移</h6><p>如果我们需要下线一个 broker 节点，那么就需要将该节点上的相关 Topic 的 partition 迁移到其他 broker 节点上去，因为 Kafka 不会自动进行分区迁移的操作，如果不进行手动迁移，就会发生副本实效和数据丢失的情况。同样，如果是我们新上线一个节点，只有新增 Topic 的分区才会分配到新节点，此时也需要进行分区迁移操作。</p>
<p>这个 Topic 目前三个分区分别在 0、1、2 三个 broker 上。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-topics.sh  --zookeeper tnode3:2181 --describe --topic LxmTest</span><br><span class="line">Topic:LxmTest	PartitionCount:3	ReplicationFactor:2	Configs:retention.ms=259200000,flush.messages=6</span><br><span class="line">	Topic: LxmTest	Partition: 0	Leader: 0	Replicas: 0,2	Isr: 0,2</span><br><span class="line">	Topic: LxmTest	Partition: 1	Leader: 1	Replicas: 1,0	Isr: 1,0</span><br><span class="line">	Topic: LxmTest	Partition: 2	Leader: 2	Replicas: 2,1	Isr: 2,1</span><br></pre></td></tr></table></figure>

<p>现在，我们需要下线 broker2，也就是我们需要将 partition 2 迁移到 0,1 broker 上，步骤如下：</p>
<ol>
<li><p>先创建一个分区迁移的配置文件，这里叫  <code>topic-move.json</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    "topics":</span><br><span class="line">    	[	</span><br><span class="line">    		&#123;</span><br><span class="line">            	"topic":"LxmTest"</span><br><span class="line">        	&#125;</span><br><span class="line">    	],</span><br><span class="line">    "version":1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>然后执行命令生成迁移计划</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-reassign-partitions.sh --zookeeper tnode3:2181 --topics-to-move-json-file topic-move.json --broker-list "0,1" --generate</span><br><span class="line">Current partition replica assignment</span><br><span class="line">&#123;"version":1,"partitions":[&#123;"topic":"LxmTest","partition":2,"replicas":[2,1]&#125;,                            &#123;"topic":"LxmTest","partition":0,"replicas":[0,2]&#125;,                            &#123;"topic":"LxmTest","partition":1,"replicas":[1,0]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Proposed partition reassignment configuration</span><br><span class="line">&#123;"version":1,"partitions":[&#123;"topic":"LxmTest","partition":2,"replicas":[1,0]&#125;,                            &#123;"topic":"LxmTest","partition":0,"replicas":[1,0]&#125;,                            &#123;"topic":"LxmTest","partition":1,"replicas":[0,1]&#125;]&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>然后将上面的 <code>Proposed partition reassignment configuration</code> 下面的内容复制到 <code>topic-repartition.json</code> 文件中</p>
</li>
<li><p>执行迁移计划</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# vi topic-repartition.json</span><br><span class="line">[root@tnode1 bin]# ./kafka-reassign-partitions.sh --zookeeper tnode3:2181 --reassignment-json-file topic-repartition.json --execute</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;"version":1,"partitions":[&#123;"topic":"LxmTest","partition":2,"replicas":[2,1]&#125;,                            &#123;"topic":"LxmTest","partition":0,"replicas":[0,2]&#125;,                            &#123;"topic":"LxmTest","partition":1,"replicas":[1,0]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started reassignment of partitions.</span><br></pre></td></tr></table></figure>
</li>
<li><p>验证迁移是否成功</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-reassign-partitions.sh --zookeeper tnode3:2181 --reassignment-json-file topic-repartition.json --verify</span><br><span class="line">Status of partition reassignment: </span><br><span class="line">Reassignment of partition [LxmTest,2] completed successfully</span><br><span class="line">Reassignment of partition [LxmTest,0] completed successfully</span><br><span class="line">Reassignment of partition [LxmTest,1] completed successfully</span><br></pre></td></tr></table></figure>
</li>
<li><p>再次查看 topic 信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-topics.sh  --zookeeper tnode3:2181 --describe --topic LxmTest</span><br><span class="line">Topic:LxmTest	PartitionCount:3	ReplicationFactor:2	Configs:retention.ms=259200000,flush.messages=6</span><br><span class="line">	Topic: LxmTest	Partition: 0	Leader: 0	Replicas: 1,0	Isr: 0,1</span><br><span class="line">	Topic: LxmTest	Partition: 1	Leader: 1	Replicas: 0,1	Isr: 1,0</span><br><span class="line">	Topic: LxmTest	Partition: 2	Leader: 1	Replicas: 1,0	Isr: 1,0</span><br></pre></td></tr></table></figure>

<p>发现已经没有 broker 2 节点的分区了 </p>
</li>
</ol>
<h6 id="集群扩容"><a href="#集群扩容" class="headerlink" title="集群扩容"></a>集群扩容</h6><p>集群扩容很好理解，就是Kafka磁盘容量不足了，需要增加 broker 节点，然后迁移分区。和上面的 broker下线操作类似，只不是生成迁移计划的时候，增加 broker 而已。具体代码就不贴了。</p>
<p>总得来说，其实集群迁移，无论是 broker 下线 还是 集群扩容 或者是 增加副本，我们都只要手动梳理迁移计划，尽量将各个分区平衡分配，同时将 AR 列表第一个副本 (即默认的 leader 副本) 平均分配，然后进行迁移即可。</p>
<p><strong>！## ！大量 topic 进行迁移时，最好分批进行，避免影响正常业务。另外，可以限制迁移的流量。</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@tnode1 bin]# ./kafka-reassign-partitions.sh --zookeeper tnode3:2181 --reassignment-json-file topic-repartition.json --execute --throttle 1024</span><br></pre></td></tr></table></figure>

<p><strong>！## ！如果做了流量限制，需要使用 –verify 来检查是否完成迁移，在完成时，–verify 参数会解除限流</strong></p>
<h2 id="kafka设置某个topic的数据过期时间"><a href="#kafka设置某个topic的数据过期时间" class="headerlink" title="kafka设置某个topic的数据过期时间"></a>kafka设置某个topic的数据过期时间</h2><h6 id="全局设置"><a href="#全局设置" class="headerlink" title="全局设置"></a>全局设置</h6><p>修改 server.properties</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">log.retention.hours&#x3D;72</span><br><span class="line">log.cleanup.policy&#x3D;delete</span><br></pre></td></tr></table></figure>

<h6 id="单独对某一个topic设置过期时间"><a href="#单独对某一个topic设置过期时间" class="headerlink" title="单独对某一个topic设置过期时间"></a>单独对某一个topic设置过期时间</h6><p>但如果只有某一个topic数据量过大。想单独对这个topic的过期时间设置短点：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;kafka-configs.sh --zookeeper localhost:2181 --alter --entity-name mytopic --entity-type topics --add-config retention.ms&#x3D;86400000</span><br></pre></td></tr></table></figure>

<p>retention.ms = 86400000 为一天，单位是毫秒。</p>
<p>查看设置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;kafka-configs.sh --zookeeper localhost:2181 --describe --entity-name mytopic --entity-type topics</span><br><span class="line"></span><br><span class="line">Configs for mytopic are retention.ms&#x3D;86400000</span><br></pre></td></tr></table></figure>

<h6 id="立即删除某个topic下的数据"><a href="#立即删除某个topic下的数据" class="headerlink" title="立即删除某个topic下的数据"></a>立即删除某个topic下的数据</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;kafka-topics.sh --zookeeper localhost:2181 --alter --topic mytopic --config cleanup.policy&#x3D;delete</span><br></pre></td></tr></table></figure>









]]></content>
      <categories>
        <category>kafka</category>
        <category>基础操作</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka问题记录</title>
    <url>/2020/10/09/Kafka%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<p>日志：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Exception in thread &quot;Thread-0&quot; org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.</span><br><span class="line">	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.sendOffsetCommitRequest(ConsumerCoordinator.java:702)</span><br><span class="line">	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsSync(ConsumerCoordinator.java:581)</span><br><span class="line">	at org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:1090)</span><br><span class="line">	at com.eebbk.da.kafka2HDFS.handle.KafkaToParquetAutoOffset.run(KafkaToParquetAutoOffset.java:287)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure>

<p>查看日志，因为处理poll到数据的时间超过了 <code>max.poll.interval.ms</code>  导致 <code>consumer</code> 发生了 <code>rebalance</code> ，从而无法手动提交 <code>offset</code>。</p>
<p>溯源：影响处理poll数据的参数是 <code>max.poll.records</code>，因为此参数设置过大，导致超  <code>max.poll.interval.ms</code>  时间。（<code>max.poll.records</code>默认500条，<code>max.poll.interval.ms</code> 默认300000）</p>
<p>解决办法：降低吞吐量，即将 <code>max.poll.records</code> 调小，目前环境我设置的为5000，调整为2000</p>
<p>​                 调整超时时间，将 <code>max.poll.interval.ms</code>  调整为 600000</p>
<p>​                 在手动提交失败是，跳过本次提交，继续处理，期待下一次提交。</p>
<h3 id="KAFKA-broker一直起不来，有时候有报错信息，有时候直接崩溃。"><a href="#KAFKA-broker一直起不来，有时候有报错信息，有时候直接崩溃。" class="headerlink" title="KAFKA broker一直起不来，有时候有报错信息，有时候直接崩溃。"></a>KAFKA broker一直起不来，有时候有报错信息，有时候直接崩溃。</h3><p>启动日志：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">java.io.IOException: Map failed</span><br><span class="line">        at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:888)</span><br><span class="line">        at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:111)</span><br><span class="line">        at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:101)</span><br><span class="line">        at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:213)</span><br><span class="line">        at kafka.log.AbstractIndex.resize(AbstractIndex.scala:101)</span><br><span class="line">        at kafka.log.LogSegment.truncateTo(LogSegment.scala:292)</span><br><span class="line">        at kafka.log.Log.truncateTo(Log.scala:893)</span><br><span class="line">        at kafka.log.LogManager$$anonfun$truncateTo$2.apply(LogManager.scala:301)</span><br><span class="line">        at kafka.log.LogManager$$anonfun$truncateTo$2.apply(LogManager.scala:293)</span><br><span class="line">        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)</span><br><span class="line">        at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)</span><br><span class="line">        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)</span><br><span class="line">        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)</span><br><span class="line">        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)</span><br><span class="line">        at kafka.log.LogManager.truncateTo(LogManager.scala:293)</span><br><span class="line">        at kafka.server.ReplicaManager.makeFollowers(ReplicaManager.scala:854)</span><br><span class="line">        at kafka.server.ReplicaManager.becomeLeaderOrFollower(ReplicaManager.scala:700)</span><br><span class="line">        at kafka.server.KafkaApis.handleLeaderAndIsrRequest(KafkaApis.scala:148)</span><br><span class="line">        at kafka.server.KafkaApis.handle(KafkaApis.scala:84)</span><br><span class="line">        at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:62)</span><br><span class="line">        at java.lang.Thread.run(Thread.java:745)</span><br><span class="line">Caused by: java.lang.OutOfMemoryError: Map failed</span><br><span class="line">        at sun.nio.ch.FileChannelImpl.map0(Native Method)</span><br><span class="line">        at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:885)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">其他直接JVM崩溃</span><br></pre></td></tr></table></figure>

<p>JVM崩溃日志：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#</span><br><span class="line"># There is insufficient memory for the Java Runtime Environment to continue.</span><br><span class="line"># Native memory allocation (malloc) failed to allocate 312 bytes for AllocateHeap</span><br><span class="line"># Possible reasons:</span><br><span class="line">#   The system is out of physical RAM or swap space</span><br><span class="line">#   In 32 bit mode, the process size limit was hit</span><br><span class="line"># Possible solutions:</span><br><span class="line">#   Reduce memory load on the system</span><br><span class="line">#   Increase physical memory or swap space</span><br><span class="line">#   Check if swap backing store is full</span><br><span class="line">#   Use 64 bit Java on a 64 bit OS</span><br><span class="line">#   Decrease Java heap size (-Xmx&#x2F;-Xms)</span><br><span class="line">#   Decrease number of Java threads</span><br><span class="line">#   Decrease Java thread stack sizes (-Xss)</span><br><span class="line">#   Set larger code cache with -XX:ReservedCodeCacheSize&#x3D;</span><br><span class="line"># This output file may be truncated or incomplete.</span><br><span class="line">#</span><br><span class="line">#  Out of Memory Error (allocation.inline.hpp:61), pid&#x3D;56865, tid&#x3D;0x00007fb2aa587700</span><br><span class="line">#</span><br><span class="line"># JRE version: Java(TM) SE Runtime Environment (8.0_131-b11) (build 1.8.0_131-b11)</span><br><span class="line"># Java VM: Java HotSpot(TM) 64-Bit Server VM (25.131-b11 mixed mode linux-amd64 compressed oops)</span><br><span class="line"># Core dump written. Default location: &#x2F;data1&#x2F;kafka&#x2F;kafka_2.11-0.10.2.1&#x2F;core or core.56865</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line">---------------  T H R E A D  ---------------</span><br><span class="line"></span><br><span class="line">Current thread (0x00007fcaa1e3f000):  JavaThread &quot;ExpirationReaper-5&quot; [_thread_in_vm, id&#x3D;57248, stack(0x00007fb2aa487000,0x00007fb2aa588000)]</span><br><span class="line"></span><br><span class="line">Stack: [0x00007fb2aa487000,0x00007fb2aa588000],  sp&#x3D;0x00007fb2aa586900,  free space&#x3D;1022k</span><br><span class="line">Native frames: (J&#x3D;compiled Java code, j&#x3D;interpreted, Vv&#x3D;VM code, C&#x3D;native code)</span><br><span class="line">V  [libjvm.so+0xac826a]</span><br><span class="line">V  [libjvm.so+0x4fd4cb]</span><br><span class="line">V  [libjvm.so+0x2e23c8]</span><br><span class="line">V  [libjvm.so+0x2e2444]</span><br><span class="line">V  [libjvm.so+0x70f40a]</span><br><span class="line">V  [libjvm.so+0xa76910]</span><br><span class="line">V  [libjvm.so+0x927568]</span><br><span class="line">C  [libpthread.so.0+0x7aa1]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">---------------  P R O C E S S  ---------------</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">power management:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Memory: 4k page, physical 65860516k(289912k free), swap 20479996k(19336528k free)</span><br><span class="line"></span><br><span class="line">vm_info: Java HotSpot(TM) 64-Bit Server VM (25.131-b11) for linux-amd64 JRE (1.8.0_131-b11), built on Mar 15 2017 01:23:40 by &quot;java_re&quot; with gcc 4.3.0 20080428 (Red Hat 4.3.0-8)</span><br><span class="line"></span><br><span class="line">time: Wed Jun 10 08:48:32 2020</span><br><span class="line">elapsed time: 322 seconds (0d 0h 5m 22s)</span><br></pre></td></tr></table></figure>

<p>搜索  <a href="https://blog.csdn.net/b644ROfP20z37485O35M/article/details/81571710" target="_blank" rel="noopener">https://blog.csdn.net/b644ROfP20z37485O35M/article/details/81571710</a></p>
<h5 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h5><p>从上面分析解决问题的方法有两个</p>
<ul>
<li>增大系统限制<code>/proc/sys/vm/max_map_count</code>  <code>vm.max_map_count=200000直接写到/etc/sysctl.conf中,然后执行sysctl -p</code></li>
<li>kafka的索引文件是否不需要一直有？是否可以限制一下</li>
</ul>
<h5 id="问题总结"><a href="#问题总结" class="headerlink" title="问题总结"></a>问题总结</h5><p>上面的过程是我思考的一个过程，可能过程有些绕，不过我这里可以来个简单的概述，描述下整个问题发生的过程：</p>
<p>kafka做了很多索引文件的内存映射，每个索引文件占用的内存还很大，这些索引文件并且一直占着没有释放，于是随着索引文件数的增多，而慢慢达到了物理内存的一个上限，比如映射到某个索引文件的时候，物理内存还剩1G，但是我们要映射一个超过1G的文件，因此会映射失败，映射失败接着就做了一次System GC，而在System GC过程中因为PermSize和MaxPermSize不一样，从而导致了在Full GC完之后Perm进行扩容，在扩容的时候因为又调用了一次mmap，而在mmap的时候check是否达到了vma的最大上限，也就是<code>/proc/sys/vm/max_map_count</code>里的65530，如果超过了，就直接crash了。</p>
<p>这只是我从此次crash文件里能想像到的一个现场，当然其实可能会有更多的场景，只要是能触发mmap动作的地方都有可能是导致crash的案发现场，比如后面又给了我一个crash文件，是在创建线程栈的时候因为mmap而导致的crash，完全和OOM没有关系，所以根本原因还是因为kafka做了太多的索引文件映射，导致mmap的vma非常多，超过了系统的限制，从而导致了crash。</p>
]]></content>
      <categories>
        <category>kafka</category>
        <category>基础操作</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka集群扩容操作记录</title>
    <url>/2020/10/09/Kafka%E9%9B%86%E7%BE%A4%E6%89%A9%E5%AE%B9%E6%93%8D%E4%BD%9C%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<h1 id="kafka集群扩容记录"><a href="#kafka集群扩容记录" class="headerlink" title="kafka集群扩容记录"></a>kafka集群扩容记录</h1><p>因为我们需要扩容 3 个broker，即broker从 0,1,2,3,4,5 扩容到 0,1,2,3,4,5,6,7,8 ，所以先进行第一部分工作</p>
<h2 id="集群扩容"><a href="#集群扩容" class="headerlink" title="集群扩容"></a>集群扩容</h2><ul>
<li><p>新机器的 6,7,8 的准备：JAVA、文件句柄数、防火墙、主机名以及hosts映射</p>
</li>
<li><p>从 0,1,2,3,4,5 选一台机器，拷贝其上的 kafka 安装目录到 6,7,8 三台机器，删除其中的logs目录，修改 server.properties 文件中的 broker.id 项。</p>
</li>
<li><p>新机器启动kafka：</p>
<p><code>sh kafka-server-start.sh -daemon ../config/server.properties</code></p>
</li>
<li><p>检查新broker是否加入成功，检查zookeeper上的/broker/ids下的node是否包含新增broker.id</p>
</li>
</ul>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><h5 id="查看现有Topic的partition、副本以及leader和follower分布。"><a href="#查看现有Topic的partition、副本以及leader和follower分布。" class="headerlink" title="查看现有Topic的partition、副本以及leader和follower分布。"></a>查看现有Topic的partition、副本以及leader和follower分布。</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@gs-kafka1 kafka_2.11-0.10.2.1]# bin&#x2F;kafka-topics.sh --describe --zookeeper gs-kafka1:2181 --topic WATCH-LOCATION</span><br><span class="line">Topic:WATCH-LOCATION	PartitionCount:18	ReplicationFactor:3	Configs:</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 0	Leader: 4	Replicas: 4,2,3	Isr: 3,4,2</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 1	Leader: 5	Replicas: 5,3,4	Isr: 5,3,4</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 2	Leader: 0	Replicas: 0,4,5	Isr: 5,0,4</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 3	Leader: 1	Replicas: 1,5,0	Isr: 5,0,1</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 4	Leader: 2	Replicas: 2,0,1	Isr: 2,0,1</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 5	Leader: 3	Replicas: 3,1,2	Isr: 3,2,1</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 6	Leader: 4	Replicas: 4,3,5	Isr: 5,3,4</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 7	Leader: 5	Replicas: 5,4,0	Isr: 5,0,4</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 8	Leader: 0	Replicas: 0,5,1	Isr: 5,0,1</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 9	Leader: 1	Replicas: 1,0,2	Isr: 0,1,2</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 10	Leader: 2	Replicas: 2,1,3	Isr: 2,3,1</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 11	Leader: 3	Replicas: 3,2,4	Isr: 3,2,4</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 12	Leader: 4	Replicas: 4,5,0	Isr: 5,0,4</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 13	Leader: 5	Replicas: 5,0,1	Isr: 5,0,1</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 14	Leader: 0	Replicas: 0,1,2	Isr: 0,2,1</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 15	Leader: 1	Replicas: 1,2,3	Isr: 1,3,2</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 16	Leader: 2	Replicas: 2,3,4	Isr: 2,4,3</span><br><span class="line">	Topic: WATCH-LOCATION	Partition: 17	Leader: 3	Replicas: 3,4,5	Isr: 5,3,4</span><br></pre></td></tr></table></figure>

<p>因为现有topic的副本及其leader都在 0,1,2,3,4,5 broker，所以我们需要迁移三分之一的副本到新broker，并且将三分之一的leader转到新broker，才能做到网络和磁盘IO的扩容。</p>
<h2 id="迁移"><a href="#迁移" class="headerlink" title="迁移"></a>迁移</h2><h6 id="参考扩容方案中的方法，我们直接编辑-topic-result-json，格式如下"><a href="#参考扩容方案中的方法，我们直接编辑-topic-result-json，格式如下" class="headerlink" title="参考扩容方案中的方法，我们直接编辑 topic-result.json，格式如下"></a>参考扩容方案中的方法，我们直接编辑 topic-result.json，格式如下</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:0, &quot;replicas&quot;:[4,2,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:1, &quot;replicas&quot;:[5,3,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:2, &quot;replicas&quot;:[0,4,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:3, &quot;replicas&quot;:[1,5,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:4, &quot;replicas&quot;:[2,0,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:5, &quot;replicas&quot;:[3,1,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:6, &quot;replicas&quot;:[4,3,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:7, &quot;replicas&quot;:[5,4,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:8, &quot;replicas&quot;:[0,5,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:9, &quot;replicas&quot;:[1,0,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:10,&quot;replicas&quot;:[2,1,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:11,&quot;replicas&quot;:[3,2,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:12,&quot;replicas&quot;:[4,5,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:13,&quot;replicas&quot;:[5,0,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:14,&quot;replicas&quot;:[0,1,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:15,&quot;replicas&quot;:[1,2,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:16,&quot;replicas&quot;:[2,3,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:17,&quot;replicas&quot;:[3,4,8]&#125;</span><br><span class="line">                           ]&#125;</span><br></pre></td></tr></table></figure>

<p>但是上述只考虑到了副本迁移，未考虑到leader迁移，参考扩容方案，我们迁移的时候顺便 修改replicas顺序，修改如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:0, &quot;replicas&quot;:[4,2,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:1, &quot;replicas&quot;:[5,3,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:2, &quot;replicas&quot;:[0,4,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:3, &quot;replicas&quot;:[1,5,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:4, &quot;replicas&quot;:[2,0,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:5, &quot;replicas&quot;:[3,1,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:6, &quot;replicas&quot;:[4,3,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:7, &quot;replicas&quot;:[5,4,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:8, &quot;replicas&quot;:[0,5,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:9, &quot;replicas&quot;:[6,0,1]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:10,&quot;replicas&quot;:[7,1,2]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:11,&quot;replicas&quot;:[8,2,3]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:12,&quot;replicas&quot;:[6,5,4]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:13,&quot;replicas&quot;:[7,0,5]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:14,&quot;replicas&quot;:[8,1,0]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:15,&quot;replicas&quot;:[1,2,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:16,&quot;replicas&quot;:[2,3,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:17,&quot;replicas&quot;:[3,4,8]&#125;</span><br><span class="line">                           ]&#125;</span><br></pre></td></tr></table></figure>

<p>其他 topic 也可参考此方法一起修改副本位置和副本顺序。然后开始迁移</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;kafka-reassign-partitions.sh --zookeeper gs-kafka1:2181 --reassignment-json-file topic-result.json  --execute</span><br></pre></td></tr></table></figure>


<h2 id="等所有topic迁移完成后，开始leader迁移，两个方法"><a href="#等所有topic迁移完成后，开始leader迁移，两个方法" class="headerlink" title="等所有topic迁移完成后，开始leader迁移，两个方法"></a>等所有topic迁移完成后，开始leader迁移，两个方法</h2><ul>
<li>一次性平衡所有topic<br><code>bin/kafka-preferred-replica-election.sh --zookeeper gs-kafka1:2181</code></li>
<li>每次平衡部分topic，编写 topicPartition.json ，格式如下。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:0&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:1&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:2&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:3&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:4&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:5&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:6&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:7&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:8&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:9&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:10&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:11&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:12&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:13&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:14&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:15&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:16&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:17&#125;</span><br><span class="line">                           ]&#125;</span><br></pre></td></tr></table></figure>

<p>然后平衡 <code>bin/kafka-preferred-replica-election.sh --zookeeper gs-kafka1:2181 --path-to-json-file topicPartition.json</code></p>
]]></content>
      <categories>
        <category>kafka</category>
        <category>基础操作</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka集群扩容</title>
    <url>/2020/10/08/Kafka%E9%9B%86%E7%BE%A4%E6%89%A9%E5%AE%B9/</url>
    <content><![CDATA[<p>根据官方脚本：kafka-reassign-partitions.sh<br>直接看脚本帮助  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost bin]# .&#x2F;kafka-reassign-partitions.sh </span><br><span class="line">This command moves topic partitions between replicas.</span><br><span class="line">Option                                Description                           </span><br><span class="line">------                                -----------                           </span><br><span class="line">--broker-list &lt;String: brokerlist&gt;    The list of brokers to which the      </span><br><span class="line">                                        partitions need to be reassigned in </span><br><span class="line">                                        the form &quot;0,1,2&quot;. This is required  </span><br><span class="line">                                        if --topics-to-move-json-file is    </span><br><span class="line">                                        used to generate reassignment       </span><br><span class="line">                                        configuration                       </span><br><span class="line">--disable-rack-aware                  Disable rack aware replica assignment </span><br><span class="line">--execute                             Kick off the reassignment as specified</span><br><span class="line">                                        by the --reassignment-json-file     </span><br><span class="line">                                        option.                             </span><br><span class="line">--generate                            Generate a candidate partition        </span><br><span class="line">                                        reassignment configuration. Note    </span><br><span class="line">                                        that this only generates a candidate</span><br><span class="line">                                        assignment, it does not execute it. </span><br><span class="line">--reassignment-json-file &lt;String:     The JSON file with the partition      </span><br><span class="line">  manual assignment json file path&gt;     reassignment configurationThe format</span><br><span class="line">                                        to use is -                         </span><br><span class="line">                                      &#123;&quot;partitions&quot;:                        </span><br><span class="line">                                      	[&#123;&quot;topic&quot;: &quot;foo&quot;,                    </span><br><span class="line">                                      	  &quot;partition&quot;: 1,                    </span><br><span class="line">                                      	  &quot;replicas&quot;: [1,2,3] &#125;],            </span><br><span class="line">                                      &quot;version&quot;:1                           </span><br><span class="line">                                      &#125;                                     </span><br><span class="line">--throttle &lt;Long: throttle&gt;           The movement of partitions will be    </span><br><span class="line">                                        throttled to this value (bytes&#x2F;sec).</span><br><span class="line">                                        Rerunning with this option, whilst a</span><br><span class="line">                                        rebalance is in progress, will alter</span><br><span class="line">                                        the throttle value. The throttle    </span><br><span class="line">                                        rate should be at least 1 KB&#x2F;s.     </span><br><span class="line">                                        (default: -1)                       </span><br><span class="line">--topics-to-move-json-file &lt;String:   Generate a reassignment configuration </span><br><span class="line">  topics to reassign json file path&gt;    to move the partitions of the       </span><br><span class="line">                                        specified topics to the list of     </span><br><span class="line">                                        brokers specified by the --broker-  </span><br><span class="line">                                        list option. The format to use is - </span><br><span class="line">                                      &#123;&quot;topics&quot;:                            </span><br><span class="line">                                      	[&#123;&quot;topic&quot;: &quot;foo&quot;&#125;,&#123;&quot;topic&quot;: &quot;foo1&quot;&#125;],</span><br><span class="line">                                      &quot;version&quot;:1                           </span><br><span class="line">                                      &#125;                                     </span><br><span class="line">--verify                              Verify if the reassignment completed  </span><br><span class="line">                                        as specified by the --reassignment- </span><br><span class="line">                                        json-file option. If there is a     </span><br><span class="line">                                        throttle engaged for the replicas   </span><br><span class="line">                                        specified, and the rebalance has    </span><br><span class="line">                                        completed, the throttle will be     </span><br><span class="line">                                        removed                             </span><br><span class="line">--zookeeper &lt;String: urls&gt;            REQUIRED: The connection string for   </span><br><span class="line">                                        the zookeeper connection in the form</span><br><span class="line">                                        host:port. Multiple URLS can be     </span><br><span class="line">                                        given to allow fail-over.</span><br></pre></td></tr></table></figure>



<h2 id="几个重要的参数"><a href="#几个重要的参数" class="headerlink" title="几个重要的参数"></a>几个重要的参数</h2><h4 id="三种模式"><a href="#三种模式" class="headerlink" title="三种模式"></a>三种模式</h4><h6 id="–generate"><a href="#–generate" class="headerlink" title="–generate"></a>–generate</h6><p>给定需要重新分配的Topic，自动生成reassign plan，并不会执行</p>
<h6 id="–execute"><a href="#–execute" class="headerlink" title="–execute"></a>–execute</h6><p>根据指定的reassign plan重新分配Partition</p>
<h6 id="–verify"><a href="#–verify" class="headerlink" title="–verify"></a>–verify</h6><p>验证重新分配Partition是否成功</p>
<h4 id="两个文件参数"><a href="#两个文件参数" class="headerlink" title="两个文件参数"></a>两个文件参数</h4><h6 id="–topics-to-move-json-file"><a href="#–topics-to-move-json-file" class="headerlink" title="–topics-to-move-json-file"></a>–topics-to-move-json-file</h6><p>需要进行重新分配的Topic配置文件，格式如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&quot;partitions&quot;:                        </span><br><span class="line">    [&#123;&quot;topic&quot;: &quot;foo&quot;,                    </span><br><span class="line">      &quot;partition&quot;: 1,                    </span><br><span class="line">      &quot;replicas&quot;: [1,2,3] &#125;],            </span><br><span class="line">      &quot;version&quot;:1                           </span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>文件名不重要，保证内容是json格式就行了。</p>
<h6 id="–reassignment-json-file"><a href="#–reassignment-json-file" class="headerlink" title="–reassignment-json-file"></a>–reassignment-json-file</h6><p>根据上面的配置文件生成的reassign plan。格式如下，可以自己修改。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&quot;topics&quot;:                            </span><br><span class="line">  [&#123;&quot;topic&quot;: &quot;foo&quot;&#125;,&#123;&quot;topic&quot;: &quot;foo1&quot;&#125;],</span><br><span class="line">    &quot;version&quot;:1                           </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="两个集群参数"><a href="#两个集群参数" class="headerlink" title="两个集群参数"></a>两个集群参数</h4><h6 id="–zookeeper"><a href="#–zookeeper" class="headerlink" title="–zookeeper"></a>–zookeeper</h6><p>zk1:2181,zk2:2181</p>
<h6 id="–broker-list"><a href="#–broker-list" class="headerlink" title="–broker-list"></a>–broker-list</h6><p>具体数值为各个broker下面的配置ID，”1,2,3,4,5”。该参数为指定新分配到的broker节点，即，假设原始broker是 “1,2,3,4”，扩容一台，则为”1,2,3,4,5”。</p>
<h2 id="官方方案"><a href="#官方方案" class="headerlink" title="官方方案"></a>官方方案</h2><blockquote>
<ul>
<li>写好 topics-to-move-json-file 配置文件 tp-file.json</li>
<li>使用命令生成 reassign plan即 reassignment-json-file</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;.&#x2F;kafka-reassign-partitions.sh --zookeeper zk1:2181 --topics-to-move-json-file tp-file.json --broker-list &quot;1,2,3,4&quot; --generate</span><br></pre></td></tr></table></figure>

<ul>
<li>(可选)根据需求自己定义或修改上面的 reassignment-json-file 文件</li>
<li>使用命令触发分配计划执行</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;.&#x2F;kafka-reassign-partitions.sh --zookeeper zk1:2181 --reassignment-json-file reassignment-json-file.json --execute</span><br></pre></td></tr></table></figure>

<ul>
<li>校验分配结果</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;.&#x2F;kafka-reassign-partitions.sh --zookeeper zk1:2181 --reassignment-json-file reassignment-json-file.json --verify</span><br></pre></td></tr></table></figure>
</blockquote>
<h2 id="扩容原理"><a href="#扩容原理" class="headerlink" title="扩容原理"></a>扩容原理</h2><p>一般来说，我们Kafka集群上的Topic的Partition数是大于broker数的，所以基本上，一个Topic的partition基本是遍布所有broker的。<br>所以我们扩容的根本原理就在于，将原来的Partition的副本进行迁移。整个迁移过程可能会涉及到副本leader和follower的迁移。  </p>
<h6 id="整体原理如下"><a href="#整体原理如下" class="headerlink" title="整体原理如下:"></a>整体原理如下:</h6><blockquote>
<ol>
<li>将副本数增加一份(无论扩容多少台)</li>
<li>新增副本开始从副本leader开始从头开始复制，即从earliest offset开始</li>
<li>等新增副本跟随上最新offset，将新增的副本添加到 ISR 列表</li>
<li>移除需要移除的broker上的副本</li>
</ol>
</blockquote>
<h6 id="这种方案的缺陷"><a href="#这种方案的缺陷" class="headerlink" title="这种方案的缺陷"></a>这种方案的缺陷</h6><blockquote>
<p>在数据量大的时候进行扩容是，因为要从头拷贝数据，会造成大量读原磁盘，消耗大量的I/O，造成producer操作缓慢，容易产生抖动。</p>
</blockquote>
<h2 id="改良方案"><a href="#改良方案" class="headerlink" title="改良方案"></a>改良方案</h2><p>优化点主要在第二步的从earliest offset。假设我们从latest offset开始复制数据，然后等待新副本保持稳定一段时间后，添加到ISR列表，再移除就副本。<br>具体代码可以看官方的issues，链接：<br><a href="https://issues.apache.org/jira/browse/KAFKA-8328" target="_blank" rel="noopener">官方代码：https://issues.apache.org/jira/browse/KAFKA-8328</a></p>
<h2 id="就官方方案进行测试"><a href="#就官方方案进行测试" class="headerlink" title="就官方方案进行测试"></a>就官方方案进行测试</h2><h6 id="查看-topic-信息"><a href="#查看-topic-信息" class="headerlink" title="查看 topic 信息"></a>查看 topic 信息</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost bin]# .&#x2F;kafka-topics.sh --describe --topic FLINK-TOPIC-1  --zookeeper tnode3:2181</span><br><span class="line">Topic:FLINK-TOPIC-1	PartitionCount:4	ReplicationFactor:2	Configs:</span><br><span class="line">	Topic: FLINK-TOPIC-1	Partition: 0	Leader: 0	Replicas: 0,2	Isr: 2,0</span><br><span class="line">	Topic: FLINK-TOPIC-1	Partition: 1	Leader: 1	Replicas: 1,0	Isr: 1,0</span><br><span class="line">	Topic: FLINK-TOPIC-1	Partition: 2	Leader: 2	Replicas: 2,1	Isr: 2,1</span><br><span class="line">	Topic: FLINK-TOPIC-1	Partition: 3	Leader: 0	Replicas: 0,1	Isr: 1,0</span><br></pre></td></tr></table></figure>

<h6 id="编辑-topics-to-move-json-file-需要的json格式文件"><a href="#编辑-topics-to-move-json-file-需要的json格式文件" class="headerlink" title="编辑 topics-to-move-json-file 需要的json格式文件"></a>编辑 topics-to-move-json-file 需要的json格式文件</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@tnode3 kafka_2.11-0.10.2.1]# cat flink-topic-json </span><br><span class="line">&#123;&quot;topics&quot;: [&#123;&quot;topic&quot;: &quot;FLINK-TOPIC-1&quot;&#125;],</span><br><span class="line"> &quot;version&quot;:1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="生成重分配计划-即-reassignment-json-file-所需要的文件"><a href="#生成重分配计划-即-reassignment-json-file-所需要的文件" class="headerlink" title="生成重分配计划 即 reassignment-json-file 所需要的文件"></a>生成重分配计划 即 reassignment-json-file 所需要的文件</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@tnode3 kafka_2.11-0.10.2.1]# bin&#x2F;kafka-reassign-partitions.sh --zookeeper tnode3:2181 --topics-to-move-json-file flink-topic-json --broker-list &quot;0,1,2,3&quot; --generate</span><br><span class="line">Current partition replica assignment</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[0,1]&#125;,                                &#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[2,1]&#125;,                                &#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[1,0]&#125;,                                &#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[0,2]&#125;</span><br><span class="line">                          ]&#125;</span><br><span class="line"></span><br><span class="line">Proposed partition reassignment configuration</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[2,3]&#125;,                                &#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,2]&#125;,                                &#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[0,1]&#125;,                                &#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[3,0]&#125;</span><br><span class="line">                          ]&#125;</span><br></pre></td></tr></table></figure>

<p>将 Proposed partition reassignment configuration 下面的内容保存到 flink-topic-result.json</p>
<h6 id="执行修改计划"><a href="#执行修改计划" class="headerlink" title="执行修改计划"></a>执行修改计划</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@tnode3 kafka_2.11-0.10.2.1]# bin&#x2F;kafka-reassign-partitions.sh --zookeeper tnode3:2181 --reassignment-json-file flink-topic-result.json --execute</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[0,1]&#125;,                                &#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[2,1]&#125;,                                &#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[1,0]&#125;,                                &#123;&quot;topic&quot;:&quot;FLINK-TOPIC-1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[0,2]&#125;</span><br><span class="line">                          ]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started reassignment of partitions.</span><br></pre></td></tr></table></figure>

<p>上面有提到。可以再讲 当前的分配情况保存成新的计划文件，用来回滚重分配的操作。</p>
<h6 id="校验重分配是否成功"><a href="#校验重分配是否成功" class="headerlink" title="校验重分配是否成功"></a>校验重分配是否成功</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@tnode3 kafka_2.11-0.10.2.1]# bin&#x2F;kafka-reassign-partitions.sh --zookeeper tnode3:2181 --reassignment-json-file flink-topic-result.json --verify</span><br><span class="line">Status of partition reassignment: </span><br><span class="line">Reassignment of partition [FLINK-TOPIC-1,3] completed successfully</span><br><span class="line">Reassignment of partition [FLINK-TOPIC-1,2] completed successfully</span><br><span class="line">Reassignment of partition [FLINK-TOPIC-1,1] completed successfully</span><br><span class="line">Reassignment of partition [FLINK-TOPIC-1,0] completed successfully</span><br></pre></td></tr></table></figure>

<h6 id="修改副本顺序-利用kafka自动生成计划的跳过下面部分，因为自动生成的计划，已经调整了副本顺序"><a href="#修改副本顺序-利用kafka自动生成计划的跳过下面部分，因为自动生成的计划，已经调整了副本顺序" class="headerlink" title="修改副本顺序   (利用kafka自动生成计划的跳过下面部分，因为自动生成的计划，已经调整了副本顺序)"></a>修改副本顺序   (利用kafka自动生成计划的跳过下面部分，因为自动生成的计划，已经调整了副本顺序)</h6><p>此时，topic的副本已经迁移完成，但是所有副本的leader还在原来的broker上，我们需要分配一定的leader到新的broker上，所以还需要进行平衡leader。</p>
<p>手动编辑的计划(也可在编辑的时候就调整顺序，此处为只迁移副本的计划，不含调整副本顺序)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:0, &quot;replicas&quot;:[4,2,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:1, &quot;replicas&quot;:[5,3,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:2, &quot;replicas&quot;:[0,4,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:3, &quot;replicas&quot;:[1,5,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:4, &quot;replicas&quot;:[2,0,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:5, &quot;replicas&quot;:[3,1,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:6, &quot;replicas&quot;:[4,3,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:7, &quot;replicas&quot;:[5,4,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:8, &quot;replicas&quot;:[0,5,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:9, &quot;replicas&quot;:[1,0,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:10,&quot;replicas&quot;:[2,1,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:11,&quot;replicas&quot;:[3,2,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:12,&quot;replicas&quot;:[4,5,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:13,&quot;replicas&quot;:[5,0,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:14,&quot;replicas&quot;:[0,1,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:15,&quot;replicas&quot;:[1,2,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:16,&quot;replicas&quot;:[2,3,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:17,&quot;replicas&quot;:[3,4,8]&#125;</span><br><span class="line">                           ]&#125;</span><br></pre></td></tr></table></figure>

<p>修改副本顺序，因为副本顺序中，kafka默认第一个为首选leader，经过平衡后会首选使用第一个副本作为leader。topicPartition.json 格式，只是调整了副本顺序</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:0, &quot;replicas&quot;:[4,2,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:1, &quot;replicas&quot;:[5,3,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:2, &quot;replicas&quot;:[0,4,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:3, &quot;replicas&quot;:[1,5,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:4, &quot;replicas&quot;:[2,0,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:5, &quot;replicas&quot;:[3,1,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:6, &quot;replicas&quot;:[4,3,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:7, &quot;replicas&quot;:[5,4,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:8, &quot;replicas&quot;:[0,5,8]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:9, &quot;replicas&quot;:[6,0,1]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:10,&quot;replicas&quot;:[7,1,2]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:11,&quot;replicas&quot;:[8,2,3]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:12,&quot;replicas&quot;:[6,5,4]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:13,&quot;replicas&quot;:[7,0,5]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:14,&quot;replicas&quot;:[8,1,0]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:15,&quot;replicas&quot;:[1,2,6]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:16,&quot;replicas&quot;:[2,3,7]&#125;,</span><br><span class="line">                           &#123;&quot;topic&quot;:&quot;WATCH-LOCATION&quot;,&quot;partition&quot;:17,&quot;replicas&quot;:[3,4,8]&#125;</span><br><span class="line">                           ]&#125;</span><br></pre></td></tr></table></figure>

<p>以上两步也可一次性编辑完成。</p>
<h6 id="平衡leader"><a href="#平衡leader" class="headerlink" title="平衡leader"></a>平衡leader</h6><p>一次性平衡所有Topic：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;kafka-preferred-replica-election.sh --zookeeper gs-kafka1:2181</span><br></pre></td></tr></table></figure>

<p>或者 添加配置 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">auto.leader.rebalance.enable&#x3D;true</span><br></pre></td></tr></table></figure>

<p>或者平衡单个Topic</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;kafka-preferred-replica-election.sh --zookeeper gs-kafka1:2181 --path-to-json-file topicPartition.json</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>kafka</category>
        <category>基础操作</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka源码环境搭建</title>
    <url>/2020/10/08/Kafka%E6%BA%90%E7%A0%81%E5%85%A5%E9%97%A8/00.%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</url>
    <content><![CDATA[<h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><ul>
<li>JDK 1.8</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">C:\Users\Administrator&gt;java -version</span><br><span class="line">java version &quot;1.8.0_201&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_201-b09)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.201-b09, mixed mode)</span><br></pre></td></tr></table></figure>

<ul>
<li><p>Gradle 5.6</p>
<p>解压下载的安装到，移动到目标目录。添加对应 bin 目录到环境变量 path。</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">C:\Users\Administrator&gt;gradle -v</span><br><span class="line"></span><br><span class="line">------------------------------------------------------------</span><br><span class="line">Gradle 5.6.2</span><br><span class="line">------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">Build time:   2019-09-05 16:13:54 UTC</span><br><span class="line">Revision:     55a5e53d855db8fc7b0e494412fc624051a8e781</span><br><span class="line"></span><br><span class="line">Kotlin:       1.3.41</span><br><span class="line">Groovy:       2.5.4</span><br><span class="line">Ant:          Apache Ant(TM) version 1.9.14 compiled on March 12 2019</span><br><span class="line">JVM:          1.8.0_201 (Oracle Corporation 25.201-b09)</span><br><span class="line">OS:           Windows 10 10.0 amd64</span><br></pre></td></tr></table></figure>

<ul>
<li>Scala</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">C:\Users\Administrator&gt;scala -version</span><br><span class="line">Scala code runner version 2.11.12 -- Copyright 2002-2017, LAMP/EPFL</span><br></pre></td></tr></table></figure>



<h3 id="源码下载"><a href="#源码下载" class="headerlink" title="源码下载"></a>源码下载</h3><p>git clone <a href="https://gitee.com/romandata/Kafka.git" target="_blank" rel="noopener">https://gitee.com/romandata/Kafka.git</a></p>
<p>切换到对应分支</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git checkout origin/2.3</span></span><br><span class="line">Checking out files: 100% (3374/3374), done.</span><br><span class="line">Note: checking out 'origin/2.3'.</span><br><span class="line"></span><br><span class="line">You are in 'detached HEAD' state. You can look around, make experimental</span><br><span class="line">changes and commit them, and you can discard any commits you make in this</span><br><span class="line">state without impacting any branches by performing another checkout.</span><br><span class="line"></span><br><span class="line">If you want to create a new branch to retain commits you create, you may</span><br><span class="line">do so (now or later) by using -b with the checkout command again. Example:</span><br><span class="line"></span><br><span class="line">  git checkout -b &lt;new-branch-name&gt;</span><br><span class="line"></span><br><span class="line">HEAD is now at 3e3419a... Add recent versions of Kafka to the matrix of ConnectD         istributedTest (#7024)</span><br></pre></td></tr></table></figure>

<p>修改源码目录下的 gradle.properties 文件，修改scala版本为本机环境的scala版本。</p>
<p>源码目录下执行  gradle idea 或者直接用IDEA打开项目，指定 gradle home。</p>
<p>编译慢的原因是 gradle 镜像在国外，修改为使用阿里云镜像：</p>
<ol>
<li><p>gradle home下的init.d目录下加入一个名叫 init.gradle 的文件</p>
</li>
<li><p>添加以下配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">allprojects&#123;</span><br><span class="line">    repositories &#123;</span><br><span class="line">        def ALIYUN_REPOSITORY_URL = 'http://maven.aliyun.com/nexus/content/groups/public'</span><br><span class="line">        def ALIYUN_JCENTER_URL = 'http://maven.aliyun.com/nexus/content/repositories/jcenter'</span><br><span class="line">        all &#123; ArtifactRepository repo -&gt;</span><br><span class="line">            if(repo instanceof MavenArtifactRepository)&#123;</span><br><span class="line">                def url = repo.url.toString()</span><br><span class="line">                if (url.startsWith('https://repo1.maven.org/maven2')) &#123;</span><br><span class="line">                    project.logger.lifecycle "Repository $&#123;repo.url&#125; replaced by $ALIYUN_REPOSITORY_URL."</span><br><span class="line">                    remove repo</span><br><span class="line">                &#125;</span><br><span class="line">                if (url.startsWith('https://jcenter.bintray.com/')) &#123;</span><br><span class="line">                    project.logger.lifecycle "Repository $&#123;repo.url&#125; replaced by $ALIYUN_JCENTER_URL."</span><br><span class="line">                    remove repo</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        maven &#123;</span><br><span class="line">                url ALIYUN_REPOSITORY_URL</span><br><span class="line">            url ALIYUN_JCENTER_URL</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>




</li>
</ol>
<p>如果出现以下错误</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kafka-1.1.1-src gradle idea</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> Configure project :</span></span><br><span class="line">Building project 'core' with Scala version 2.11.12</span><br><span class="line"></span><br><span class="line">FAILURE: Build failed with an exception.</span><br><span class="line"></span><br><span class="line">* Where:</span><br><span class="line">Build file '/Users/bibo/.Trash/kafka-1.1.1-src/build.gradle' line: 552</span><br><span class="line"></span><br><span class="line">* What went wrong:</span><br><span class="line">A problem occurred evaluating root project 'kafka-1.1.1-src'.</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> Failed to apply plugin [id <span class="string">'org.scoverage'</span>]</span></span><br><span class="line"><span class="meta">   &gt;</span><span class="bash"> Could not create an instance of <span class="built_in">type</span> org.scoverage.ScoverageExtension.</span></span><br><span class="line">      &gt; You can't map a property that does not exist: propertyName=testClassesDir</span><br><span class="line"></span><br><span class="line">* Try:</span><br><span class="line">Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.</span><br><span class="line"></span><br><span class="line">* Get more help at https://help.gradle.org</span><br><span class="line"></span><br><span class="line">Deprecated Gradle features were used in this build, making it incompatible with Gradle 6.0.</span><br><span class="line">Use '--warning-mode all' to show the individual deprecation warnings.</span><br><span class="line">See https://docs.gradle.org/5.4.1/userguide/command_line_interface.html#sec:command_line_warnings</span><br><span class="line"></span><br><span class="line">BUILD FAILED in 2s</span><br></pre></td></tr></table></figure>

<p>解决方法参考：KAFKA-7706</p>
<p>修改 build.gradle 文件,将org.scoverage:gradle-scoverage 版本修改,2.1.0修改为2.5.0,重新执行</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">buildscript &#123;</span><br><span class="line">  repositories &#123;</span><br><span class="line">    mavenCentral()</span><br><span class="line">    jcenter()</span><br><span class="line">  &#125;</span><br><span class="line">  apply from: file(<span class="string">'gradle/buildscript.gradle'</span>), to: buildscript</span><br><span class="line"></span><br><span class="line">  dependencies &#123;</span><br><span class="line">    <span class="comment">// For Apache Rat plugin to ignore non-Git files</span></span><br><span class="line">    classpath <span class="string">"org.ajoberstar:grgit:1.7.0"</span></span><br><span class="line">    classpath <span class="string">'com.github.ben-manes:gradle-versions-plugin:0.13.0'</span></span><br><span class="line">    classpath <span class="string">'org.scoverage:gradle-scoverage:2.5.0'</span>   <span class="comment">// 之前是2.1.0</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>IDEA 打开项目目录</p>
]]></content>
      <categories>
        <category>kafka</category>
        <category>源码入门</category>
        <category>环境搭建</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka源码入门/01.客户端命令/01.kafka-config.sh</title>
    <url>/2020/10/09/Kafka%E6%BA%90%E7%A0%81%E5%85%A5%E9%97%A8/01.%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%91%BD%E4%BB%A4/01.kafka-config.sh/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>kafka-topic.sh命令</title>
    <url>/2020/10/08/Kafka%E6%BA%90%E7%A0%81%E5%85%A5%E9%97%A8/01.%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%91%BD%E4%BB%A4/00.kafka-topic.sh/</url>
    <content><![CDATA[<h3 id="kafka-topic-sh"><a href="#kafka-topic-sh" class="headerlink" title="kafka-topic.sh"></a>kafka-topic.sh</h3><p>先看具体脚本信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 中间注释已删除</span></span><br><span class="line">exec $(dirname $0)/kafka-run-class.sh kafka.admin.TopicCommand "$@"</span><br></pre></td></tr></table></figure>

<p>脚本命令很简单，通过 <code>kafka-run-class.sh</code> 启动 <code>kafka.admin.TopicCommand</code> 类，参数全部传递过去。对于 <code>kafka-run-class.sh</code> ，因为对shell命令不熟，这里不作过多介绍。不过其功能基本上都是初始化启动参数以及环境，然后启动参数中相应的类。另外，-daemon 表示后台运行，日志为日志目录下相应日志文件。</p>
<p>接下来只要看 <code>kafka.admin.TopicCommand</code> 类的运行过程。</p>
<h5 id="main函数"><a href="#main函数" class="headerlink" title="main函数"></a>main函数</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 解析输入参数</span></span><br><span class="line">    <span class="keyword">val</span> opts = <span class="keyword">new</span> <span class="type">TopicCommandOptions</span>(args)</span><br><span class="line">	</span><br><span class="line">    <span class="comment">// 无参数直接退出，为啥这个不在初始化之前...</span></span><br><span class="line">    <span class="keyword">if</span>(args.length == <span class="number">0</span>)</span><br><span class="line">      <span class="type">CommandLineUtils</span>.printUsageAndDie(opts.parser, <span class="string">"Create, delete, describe, or change a topic."</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//  should have exactly one action</span></span><br><span class="line">    <span class="keyword">val</span> actions = <span class="type">Seq</span>(opts.createOpt, opts.listOpt, opts.alterOpt, opts.describeOpt, opts.deleteOpt).count(opts.options.has _)</span><br><span class="line">    <span class="comment">// 每次只能有list、describe、create、alter、delete 中的一个操作 </span></span><br><span class="line">    <span class="keyword">if</span>(actions != <span class="number">1</span>)</span><br><span class="line">      <span class="type">CommandLineUtils</span>.printUsageAndDie(opts.parser, <span class="string">"Command must include exactly one action: --list, --describe, --create, --alter or --delete"</span>)</span><br><span class="line">	</span><br><span class="line">    <span class="comment">// 校验参数</span></span><br><span class="line">    opts.checkArgs()</span><br><span class="line">	</span><br><span class="line">    <span class="comment">// 初始化ZK信息</span></span><br><span class="line">    <span class="keyword">val</span> zkUtils = <span class="type">ZkUtils</span>(opts.options.valueOf(opts.zkConnectOpt),</span><br><span class="line">                          <span class="number">30000</span>,</span><br><span class="line">                          <span class="number">30000</span>,</span><br><span class="line">                          <span class="type">JaasUtils</span>.isZkSecurityEnabled())</span><br><span class="line">    <span class="keyword">var</span> exitCode = <span class="number">0</span></span><br><span class="line">    <span class="comment">// 执行具体的 Topic 操作</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">if</span>(opts.options.has(opts.createOpt))</span><br><span class="line">        createTopic(zkUtils, opts)</span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span>(opts.options.has(opts.alterOpt))</span><br><span class="line">        alterTopic(zkUtils, opts)</span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span>(opts.options.has(opts.listOpt))</span><br><span class="line">        listTopics(zkUtils, opts)</span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span>(opts.options.has(opts.describeOpt))</span><br><span class="line">        describeTopic(zkUtils, opts)</span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span>(opts.options.has(opts.deleteOpt))</span><br><span class="line">        deleteTopic(zkUtils, opts)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</span><br><span class="line">        println(<span class="string">"Error while executing topic command : "</span> + e.getMessage)</span><br><span class="line">        error(<span class="type">Utils</span>.stackTrace(e))</span><br><span class="line">        exitCode = <span class="number">1</span></span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      zkUtils.close()</span><br><span class="line">      <span class="type">System</span>.exit(exitCode)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>首先会通过伴生类 <code>TopicCommandOptions</code> 初始化一个参数列表<code>OptionParse</code> ，然后进行输入参数的接收；</p>
<p>之后判断接收的参数是否为空、是否只含一类操作（create、list、alter、delete、describe）、对应操作是否有必要的配置参数 ( <code>checkArgs()</code> )；</p>
<p>紧接着就是ZkUtils的初始化；</p>
<p>然后就是根据操作类型选择不同的实现方法：</p>
<h5 id="createTopic"><a href="#createTopic" class="headerlink" title="createTopic"></a>createTopic</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTopic</span></span>(zkUtils: <span class="type">ZkUtils</span>, opts: <span class="type">TopicCommandOptions</span>) &#123;</span><br><span class="line">    <span class="comment">// topic名称</span></span><br><span class="line">  <span class="keyword">val</span> topic = opts.options.valueOf(opts.topicOpt)</span><br><span class="line">    <span class="comment">// 创建topic的配置</span></span><br><span class="line">  <span class="keyword">val</span> configs = parseTopicConfigsToBeAdded(opts)</span><br><span class="line">    <span class="comment">// if-not-exists </span></span><br><span class="line">  <span class="keyword">val</span> ifNotExists = opts.options.has(opts.ifNotExistsOpt)</span><br><span class="line">    <span class="comment">// 检测topic</span></span><br><span class="line">  <span class="keyword">if</span> (<span class="type">Topic</span>.hasCollisionChars(topic))</span><br><span class="line">    println(<span class="string">"WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both."</span>)</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 是否指定分区副本 例如三分区两副本  --replica-assignment 0:1,1:2,2:0</span></span><br><span class="line">      <span class="comment">// parseReplicaAssignment 方法会进行格式的判断和格式化，','分隔分区，':'分隔分区中副本</span></span><br><span class="line">    <span class="keyword">if</span> (opts.options.has(opts.replicaAssignmentOpt)) &#123;</span><br><span class="line">      <span class="keyword">val</span> assignment = parseReplicaAssignment(opts.options.valueOf(opts.replicaAssignmentOpt))</span><br><span class="line">      <span class="type">AdminUtils</span>.createOrUpdateTopicPartitionAssignmentPathInZK(zkUtils, topic, assignment, configs, update = <span class="literal">false</span>)</span><br><span class="line">    &#125; <span class="comment">// 另外就是按照 分区数 副本数等自动分配</span></span><br><span class="line">      <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="type">CommandLineUtils</span>.checkRequiredArgs(opts.parser, opts.options, opts.partitionsOpt, opts.replicationFactorOpt)</span><br><span class="line">      <span class="keyword">val</span> partitions = opts.options.valueOf(opts.partitionsOpt).intValue</span><br><span class="line">      <span class="keyword">val</span> replicas = opts.options.valueOf(opts.replicationFactorOpt).intValue</span><br><span class="line">      <span class="keyword">val</span> rackAwareMode = <span class="keyword">if</span> (opts.options.has(opts.disableRackAware)) <span class="type">RackAwareMode</span>.<span class="type">Disabled</span></span><br><span class="line">                          <span class="keyword">else</span> <span class="type">RackAwareMode</span>.<span class="type">Enforced</span></span><br><span class="line">      <span class="type">AdminUtils</span>.createTopic(zkUtils, topic, partitions, replicas, configs, rackAwareMode)</span><br><span class="line">    &#125;</span><br><span class="line">    println(<span class="string">"Created topic \"%s\"."</span>.format(topic))</span><br><span class="line">  &#125; <span class="keyword">catch</span>  &#123;</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">TopicExistsException</span> =&gt; <span class="keyword">if</span> (!ifNotExists) <span class="keyword">throw</span> e</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>看 AdminUtils.createTopic 代码可以发现</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTopic</span></span>(zkUtils: <span class="type">ZkUtils</span>,</span><br><span class="line">                  topic: <span class="type">String</span>,</span><br><span class="line">                  partitions: <span class="type">Int</span>,</span><br><span class="line">                  replicationFactor: <span class="type">Int</span>,</span><br><span class="line">                  topicConfig: <span class="type">Properties</span> = <span class="keyword">new</span> <span class="type">Properties</span>,</span><br><span class="line">                  rackAwareMode: <span class="type">RackAwareMode</span> = <span class="type">RackAwareMode</span>.<span class="type">Enforced</span>) &#123;</span><br><span class="line">    <span class="comment">// 获取broker以及机架信息</span></span><br><span class="line">    <span class="keyword">val</span> brokerMetadatas = getBrokerMetadatas(zkUtils, rackAwareMode)</span><br><span class="line">    <span class="comment">// 根据broker信息自动分配分区副本</span></span><br><span class="line">    <span class="keyword">val</span> replicaAssignment = <span class="type">AdminUtils</span>.assignReplicasToBrokers(brokerMetadatas, partitions, replicationFactor)</span><br><span class="line">    <span class="comment">// 和手动分配分区副本调用一样的方法</span></span><br><span class="line">    <span class="type">AdminUtils</span>.createOrUpdateTopicPartitionAssignmentPathInZK(zkUtils, topic, replicaAssignment, topicConfig)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>kafka自动分配分区副本的方式：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">assignReplicasToBrokers</span></span>(brokerMetadatas: <span class="type">Seq</span>[<span class="type">BrokerMetadata</span>],</span><br><span class="line">                            nPartitions: <span class="type">Int</span>,</span><br><span class="line">                            replicationFactor: <span class="type">Int</span>,</span><br><span class="line">                            fixedStartIndex: <span class="type">Int</span> = <span class="number">-1</span>,</span><br><span class="line">                            startPartitionId: <span class="type">Int</span> = <span class="number">-1</span>): <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">Int</span>]] = &#123;</span><br><span class="line">    <span class="comment">// 分区数必须大于 0</span></span><br><span class="line">  <span class="keyword">if</span> (nPartitions &lt;= <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">InvalidPartitionsException</span>(<span class="string">"number of partitions must be larger than 0"</span>)</span><br><span class="line">    <span class="comment">// 副本数必须大于 0</span></span><br><span class="line">  <span class="keyword">if</span> (replicationFactor &lt;= <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">InvalidReplicationFactorException</span>(<span class="string">"replication factor must be larger than 0"</span>)</span><br><span class="line">    <span class="comment">// 副本数不大于 broker 节点数</span></span><br><span class="line">  <span class="keyword">if</span> (replicationFactor &gt; brokerMetadatas.size)</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">InvalidReplicationFactorException</span>(<span class="string">s"replication factor: <span class="subst">$replicationFactor</span> larger than available brokers: <span class="subst">$&#123;brokerMetadatas.size&#125;</span>"</span>)</span><br><span class="line">    <span class="comment">// 无机架信息时</span></span><br><span class="line">  <span class="keyword">if</span> (brokerMetadatas.forall(_.rack.isEmpty))</span><br><span class="line">    assignReplicasToBrokersRackUnaware(nPartitions, replicationFactor, brokerMetadatas.map(_.id), fixedStartIndex,</span><br><span class="line">      startPartitionId)</span><br><span class="line">    <span class="comment">// 有机架信息时</span></span><br><span class="line">  <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (brokerMetadatas.exists(_.rack.isEmpty))</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">AdminOperationException</span>(<span class="string">"Not all brokers have rack information for replica rack aware assignment"</span>)</span><br><span class="line">    assignReplicasToBrokersRackAware(nPartitions, replicationFactor, brokerMetadatas, fixedStartIndex,</span><br><span class="line">      startPartitionId)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong><em>无机架信息自动分配方式</em></strong>：（assignReplicasToBrokersRackUnaware）</p>
<p>（举例设定broker数为6 [0-5]，broker集合即为(0,1,2,3,4,5)，举例为集合下标）</p>
<ol>
<li><p>0号分区的leader为随机选的一台broker，第一个副本也为随机的不同于leader的broker的 broker，后续副本顺序循环增加（leader所在</p>
</li>
<li><p>除外），即 0号分区的 SR 为 [5, 1,2] or [4, 2,3,5 ] or [2 ,5,0,1]</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 此方法将确定后续副本只会除leader所在broker之外的其他broker</span></span><br><span class="line"><span class="comment">// 这也是副本数不大于broker数的原因  </span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">replicaIndex</span></span>(firstReplicaIndex: <span class="type">Int</span>, secondReplicaShift: <span class="type">Int</span>, replicaIndex: <span class="type">Int</span>, nBrokers: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> shift = <span class="number">1</span> + (secondReplicaShift + replicaIndex) % (nBrokers - <span class="number">1</span>)</span><br><span class="line">    (firstReplicaIndex + shift) % nBrokers</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>下一个分区的leader为上一个分区的broker+1 (broker数内循环)，而其第一个副本为上一个分区的第一个副本的broker+1(broker数内循环)，即 下一个分区的 SR 为 [0, 2,3] or [5, 3,4,0] or [3, 0,1,2] ，后续副本顺序循环增加</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 6个broker,9个分区的partition。分区id超过broker数时，其第一个副本的</span><br><span class="line">(5,1,2)</span><br><span class="line">(0,2,3)</span><br><span class="line">(1,3,4)</span><br><span class="line">(2,4,5)</span><br><span class="line">(3,5,0)</span><br><span class="line">(4,0,1)</span><br><span class="line">(5,2,3)</span><br><span class="line">(0,3,4)</span><br><span class="line">(1,4,5)</span><br></pre></td></tr></table></figure>



</li>
</ol>
<p><strong><em>有机架信息自动分配方式</em></strong>：（assignReplicasToBrokersRackAware）</p>
<ol>
<li><p>broker集合的顺序不一样，无机架是大小顺序排，有机架时，循环遍历机架，每次取一台broker放入集合，如下：</p>
<p>rack1：1,2,3     rack2：4,5,6   rack3：7,8,9    则broker集合为 ( 1,4,7,2,5,8,3,6,9 )</p>
</li>
<li><p>leader计算和副本选择计算和无机架方式一样，差别在于计算出来的副本broker和机架是否有效，无效则使用下一个broker。无效的判断：</p>
<ol>
<li>此机架已经有副本分配，但是还有一个或多个机架没有分配到副本</li>
<li>此broker已经有副本分配，但是还有一个或多个broker没有分配到副本</li>
</ol>
<p>总的来说就是，<strong>副本尽量在不同机架的不同broker上，均衡分布。</strong></p>
<p>​        </p>
</li>
</ol>
<p><strong>两种创建方式主要体现在分区副本的分配上，其他步骤都是一样的：</strong></p>
<p><code>validateCreateOrUpdateTopic</code> :</p>
<ul>
<li>topic名称校验：不能为空，不能是 ‘.’ 和 ‘..’，长度不能超过249，正则匹配(只能是”[a-zA-Z0-9\._\-]”)</li>
<li>topic是否已存在：</li>
<li>每个分区上的副本数是否一致：</li>
</ul>
<p><code>writeEntityConfig</code> ： Topic相关配置信息写入ZK ( zk_kafka_root/config/topic)</p>
<p><code>writeTopicPartitionAssignment</code> ：分区副本信息写入ZK对应broker上（ zk_kafka_root/brokers/topics/topic ）</p>
<p><em>分区目录的创建会在创建完topic后自动创建</em></p>
<h5 id="alterTopic"><a href="#alterTopic" class="headerlink" title="alterTopic"></a>alterTopic</h5><p>修改topic的属性，包括增加更新删除属性，同时也可以扩分区</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">alterTopic</span></span>(zkUtils: <span class="type">ZkUtils</span>, opts: <span class="type">TopicCommandOptions</span>) &#123;</span><br><span class="line">    <span class="comment">// --topic 可以正则匹配</span></span><br><span class="line">  <span class="keyword">val</span> topics = getTopics(zkUtils, opts)</span><br><span class="line">  <span class="keyword">val</span> ifExists = opts.options.has(opts.ifExistsOpt)</span><br><span class="line">    <span class="comment">// 未找到topic同时无 if-not-exist 则打印错误</span></span><br><span class="line">  <span class="keyword">if</span> (topics.isEmpty &amp;&amp; !ifExists) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">"Topic %s does not exist on ZK path %s"</span>.format(opts.options.valueOf(opts.topicOpt),</span><br><span class="line">        opts.options.valueOf(opts.zkConnectOpt)))</span><br><span class="line">  &#125;</span><br><span class="line">  topics.foreach &#123; topic =&gt;</span><br><span class="line">    <span class="keyword">val</span> configs = <span class="type">AdminUtils</span>.fetchEntityConfig(zkUtils, <span class="type">ConfigType</span>.<span class="type">Topic</span>, topic)</span><br><span class="line">    <span class="keyword">if</span>(opts.options.has(opts.configOpt) || opts.options.has(opts.deleteConfigOpt)) &#123;</span><br><span class="line">      println(<span class="string">"WARNING: Altering topic configuration from this script has been deprecated and may be removed in future releases."</span>)</span><br><span class="line">      println(<span class="string">"         Going forward, please use kafka-configs.sh for this functionality"</span>)</span><br><span class="line"><span class="comment">// --config 需要增加或者更新的配置项</span></span><br><span class="line">      <span class="keyword">val</span> configsToBeAdded = parseTopicConfigsToBeAdded(opts)</span><br><span class="line">      <span class="comment">// --delete-config 需要删除的配置</span></span><br><span class="line">      <span class="keyword">val</span> configsToBeDeleted = parseTopicConfigsToBeDeleted(opts)</span><br><span class="line">      <span class="comment">// compile the final set of configs</span></span><br><span class="line">      configs.putAll(configsToBeAdded)</span><br><span class="line">      configsToBeDeleted.foreach(config =&gt; configs.remove(config))</span><br><span class="line">      <span class="type">AdminUtils</span>.changeTopicConfig(zkUtils, topic, configs)</span><br><span class="line">      println(<span class="string">"Updated config for topic \"%s\"."</span>.format(topic))</span><br><span class="line">    &#125;</span><br><span class="line"> <span class="comment">// 如果选型有 --partitions 说明还需要对分区进行变更</span></span><br><span class="line">    <span class="keyword">if</span>(opts.options.has(opts.partitionsOpt)) &#123;</span><br><span class="line">      <span class="comment">// 内部分区 __comsumer_offsets 无法更改</span></span><br><span class="line">      <span class="keyword">if</span> (topic == <span class="type">Topic</span>.<span class="type">GroupMetadataTopicName</span>) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">"The number of partitions for the offsets topic cannot be changed."</span>)</span><br><span class="line">      &#125;</span><br><span class="line">      println(<span class="string">"WARNING: If partitions are increased for a topic that has a key, the partition "</span> +</span><br><span class="line">        <span class="string">"logic or ordering of the messages will be affected"</span>)</span><br><span class="line">        <span class="comment">// 新分区数</span></span><br><span class="line">      <span class="keyword">val</span> nPartitions = opts.options.valueOf(opts.partitionsOpt).intValue</span><br><span class="line">        <span class="comment">// 分区只能增加，同时分区副本是根据之前的分区副本分配规则延续的</span></span><br><span class="line">      <span class="keyword">val</span> replicaAssignmentStr = opts.options.valueOf(opts.replicaAssignmentOpt)</span><br><span class="line">      <span class="type">AdminUtils</span>.addPartitions(zkUtils, topic, nPartitions, replicaAssignmentStr)</span><br><span class="line">      println(<span class="string">"Adding partitions succeeded!"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h5 id="listTopic"><a href="#listTopic" class="headerlink" title="listTopic"></a>listTopic</h5><p>列出topic名称，可根据正则匹配，代码很简单，去ZK获取相关信息，然后打印</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">listTopics</span></span>(zkUtils: <span class="type">ZkUtils</span>, opts: <span class="type">TopicCommandOptions</span>) &#123;</span><br><span class="line">   <span class="keyword">val</span> topics = getTopics(zkUtils, opts)</span><br><span class="line">   <span class="keyword">for</span>(topic &lt;- topics) &#123;</span><br><span class="line">     <span class="keyword">if</span> (zkUtils.pathExists(getDeleteTopicPath(topic))) &#123;</span><br><span class="line">       println(<span class="string">"%s - marked for deletion"</span>.format(topic))</span><br><span class="line">     &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">       println(topic)</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>



<h5 id="deleteTopic"><a href="#deleteTopic" class="headerlink" title="deleteTopic"></a>deleteTopic</h5><p>删除操作只是单纯做一个标记，在ZK对应删除的节点下增加需要删除的topic。如果 broker 端参数 <code>delete.topic.enable</code> 不是true，那么删除操作不会有其他任何影响，除了标记之外。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deleteTopic</span></span>(zkUtils: <span class="type">ZkUtils</span>, opts: <span class="type">TopicCommandOptions</span>) &#123;</span><br><span class="line">  <span class="keyword">val</span> topics = getTopics(zkUtils, opts)</span><br><span class="line">  <span class="keyword">val</span> ifExists = opts.options.has(opts.ifExistsOpt)</span><br><span class="line">  <span class="keyword">if</span> (topics.isEmpty &amp;&amp; !ifExists) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">"Topic %s does not exist on ZK path %s"</span>.format(opts.options.valueOf(opts.topicOpt),</span><br><span class="line">        opts.options.valueOf(opts.zkConnectOpt)))</span><br><span class="line">  &#125;</span><br><span class="line">  topics.foreach &#123; topic =&gt;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (<span class="type">Topic</span>.isInternal(topic)) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">AdminOperationException</span>(<span class="string">"Topic %s is a kafka internal topic and is not allowed to be marked for deletion."</span>.format(topic))</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// 标记删除， admin/delete_topics/topic</span></span><br><span class="line">        zkUtils.createPersistentPath(getDeleteTopicPath(topic))</span><br><span class="line">        println(<span class="string">"Topic %s is marked for deletion."</span>.format(topic))</span><br><span class="line">        println(<span class="string">"Note: This will have no impact if delete.topic.enable is not set to true."</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> _: <span class="type">ZkNodeExistsException</span> =&gt;</span><br><span class="line">        println(<span class="string">"Topic %s is already marked for deletion."</span>.format(topic))</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">AdminOperationException</span> =&gt;</span><br><span class="line">        <span class="keyword">throw</span> e</span><br><span class="line">      <span class="keyword">case</span> _: <span class="type">Throwable</span> =&gt;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">AdminOperationException</span>(<span class="string">"Error while deleting topic %s"</span>.format(topic))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h5 id="describeTopic"><a href="#describeTopic" class="headerlink" title="describeTopic"></a>describeTopic</h5><p>topic可正则，额外有三个特殊配置需要注意。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">describeTopic</span></span>(zkUtils: <span class="type">ZkUtils</span>, opts: <span class="type">TopicCommandOptions</span>) &#123;</span><br><span class="line">  <span class="keyword">val</span> topics = getTopics(zkUtils, opts)</span><br><span class="line">  <span class="comment">// 只列出副本未跟上分区</span></span><br><span class="line">  <span class="keyword">val</span> reportUnderReplicatedPartitions = opts.options.has(opts.reportUnderReplicatedPartitionsOpt)</span><br><span class="line">  <span class="comment">// 只列出leader不可用分区</span></span><br><span class="line">  <span class="keyword">val</span> reportUnavailablePartitions = opts.options.has(opts.reportUnavailablePartitionsOpt)</span><br><span class="line">  <span class="comment">// 只列出配置被覆盖过的topic</span></span><br><span class="line">  <span class="keyword">val</span> reportOverriddenConfigs = opts.options.has(opts.topicsWithOverridesOpt)</span><br><span class="line">  <span class="keyword">val</span> liveBrokers = zkUtils.getAllBrokersInCluster().map(_.id).toSet</span><br><span class="line">  <span class="keyword">for</span> (topic &lt;- topics) &#123;</span><br><span class="line">    zkUtils.getPartitionAssignmentForTopics(<span class="type">List</span>(topic)).get(topic) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(topicPartitionAssignment) =&gt;</span><br><span class="line">        <span class="keyword">val</span> describeConfigs: <span class="type">Boolean</span> = !reportUnavailablePartitions &amp;&amp; !reportUnderReplicatedPartitions</span><br><span class="line">        <span class="keyword">val</span> describePartitions: <span class="type">Boolean</span> = !reportOverriddenConfigs</span><br><span class="line">        <span class="keyword">val</span> sortedPartitions = topicPartitionAssignment.toList.sortWith((m1, m2) =&gt; m1._1 &lt; m2._1)</span><br><span class="line">        <span class="keyword">if</span> (describeConfigs) &#123;</span><br><span class="line">          <span class="keyword">val</span> configs = <span class="type">AdminUtils</span>.fetchEntityConfig(zkUtils, <span class="type">ConfigType</span>.<span class="type">Topic</span>, topic).asScala</span><br><span class="line">          <span class="keyword">if</span> (!reportOverriddenConfigs || configs.nonEmpty) &#123;</span><br><span class="line">            <span class="keyword">val</span> numPartitions = topicPartitionAssignment.size</span><br><span class="line">            <span class="keyword">val</span> replicationFactor = topicPartitionAssignment.head._2.size</span><br><span class="line">            println(<span class="string">"Topic:%s\tPartitionCount:%d\tReplicationFactor:%d\tConfigs:%s"</span></span><br><span class="line">              .format(topic, numPartitions, replicationFactor, configs.map(kv =&gt; kv._1 + <span class="string">"="</span> + kv._2).mkString(<span class="string">","</span>)))</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (describePartitions) &#123;</span><br><span class="line">          <span class="keyword">for</span> ((partitionId, assignedReplicas) &lt;- sortedPartitions) &#123;</span><br><span class="line">            <span class="keyword">val</span> inSyncReplicas = zkUtils.getInSyncReplicasForPartition(topic, partitionId)</span><br><span class="line">            <span class="keyword">val</span> leader = zkUtils.getLeaderForPartition(topic, partitionId)</span><br><span class="line">            <span class="keyword">if</span> ((!reportUnderReplicatedPartitions &amp;&amp; !reportUnavailablePartitions) ||</span><br><span class="line">                (reportUnderReplicatedPartitions &amp;&amp; inSyncReplicas.size &lt; assignedReplicas.size) ||</span><br><span class="line">                (reportUnavailablePartitions &amp;&amp; (leader.isEmpty || !liveBrokers.contains(leader.get)))) &#123;</span><br><span class="line">              print(<span class="string">"\tTopic: "</span> + topic)</span><br><span class="line">              print(<span class="string">"\tPartition: "</span> + partitionId)</span><br><span class="line">              print(<span class="string">"\tLeader: "</span> + (<span class="keyword">if</span>(leader.isDefined) leader.get <span class="keyword">else</span> <span class="string">"none"</span>))</span><br><span class="line">              print(<span class="string">"\tReplicas: "</span> + assignedReplicas.mkString(<span class="string">","</span>))</span><br><span class="line">              println(<span class="string">"\tIsr: "</span> + inSyncReplicas.mkString(<span class="string">","</span>))</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">        println(<span class="string">"Topic "</span> + topic + <span class="string">" doesn't exist!"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>




]]></content>
      <categories>
        <category>kafka</category>
        <category>源码入门</category>
        <category>kafka客户端命令</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title>kafka启动流程</title>
    <url>/2020/10/08/Kafka%E6%BA%90%E7%A0%81%E5%85%A5%E9%97%A8/02.Kafka%E6%9C%8D%E5%8A%A1%E7%AB%AF/00.kafka%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/</url>
    <content><![CDATA[<p> 通过 <code>kafka-server-start.sh</code> 发现，启动类为 <code>kafka.Kafka</code>，直接看类代码：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 服务启动配置</span></span><br><span class="line">    <span class="keyword">val</span> serverProps = getPropsFromArgs(args)</span><br><span class="line">      <span class="comment">// 启动类</span></span><br><span class="line">    <span class="keyword">val</span> kafkaServerStartable = <span class="type">KafkaServerStartable</span>.fromProps(serverProps)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// attach shutdown handler to catch control-c</span></span><br><span class="line">    <span class="type">Runtime</span>.getRuntime().addShutdownHook(<span class="keyword">new</span> <span class="type">Thread</span>() &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() = &#123;</span><br><span class="line">        kafkaServerStartable.shutdown</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    kafkaServerStartable.startup</span><br><span class="line">    kafkaServerStartable.awaitShutdown</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</span><br><span class="line">      fatal(e)</span><br><span class="line">      <span class="type">System</span>.exit(<span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="type">System</span>.exit(<span class="number">0</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>启动为 KafkaServerStartable 对象进行启动，而 KafkaServerStartable 中具体的实现是 KafkaServer对象的启动，具体启动方法是：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Start up API for bringing up a single instance of the Kafka server.</span></span><br><span class="line"><span class="comment">   * Instantiates the LogManager, the SocketServer and the request handlers - KafkaRequestHandlers</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      info(<span class="string">"starting"</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span>(isShuttingDown.get)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"Kafka server is still shutting down, cannot re-start!"</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span>(startupComplete.get)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> canStartup = isStartingUp.compareAndSet(<span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">      <span class="keyword">if</span> (canStartup) &#123;</span><br><span class="line">        <span class="comment">// 状态调整为 Starting</span></span><br><span class="line">        brokerState.newState(<span class="type">Starting</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* start scheduler 启动调度器 */</span></span><br><span class="line">        kafkaScheduler.startup()</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* setup zookeeper 初始化 ZK连接，同时创建 根节点.*/</span></span><br><span class="line">        zkUtils = initZk()</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* Get or create cluster_id 创建或获取 cluster id*/</span></span><br><span class="line">        _clusterId = getOrGenerateClusterId(zkUtils)</span><br><span class="line">        info(<span class="string">s"Cluster ID = <span class="subst">$clusterId</span>"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* generate brokerId 获取 broker id 以及broker上对应的log_dirs*/</span></span><br><span class="line">        config.brokerId =  getBrokerId</span><br><span class="line">        <span class="keyword">this</span>.logIdent = <span class="string">"[Kafka Server "</span> + config.brokerId + <span class="string">"], "</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">/* create and configure metrics 创建一个metrics，这个metrics提供给kafka内部使用*/</span></span><br><span class="line">        <span class="keyword">val</span> reporters = config.getConfiguredInstances(<span class="type">KafkaConfig</span>.<span class="type">MetricReporterClassesProp</span>, classOf[<span class="type">MetricsReporter</span>],</span><br><span class="line">            <span class="type">Map</span>[<span class="type">String</span>, <span class="type">AnyRef</span>](<span class="type">KafkaConfig</span>.<span class="type">BrokerIdProp</span> -&gt; (config.brokerId.toString)).asJava)</span><br><span class="line">        reporters.add(<span class="keyword">new</span> <span class="type">JmxReporter</span>(jmxPrefix))</span><br><span class="line">        <span class="keyword">val</span> metricConfig = <span class="type">KafkaServer</span>.metricConfig(config)</span><br><span class="line">        metrics = <span class="keyword">new</span> <span class="type">Metrics</span>(metricConfig, reporters, time, <span class="literal">true</span>)</span><br><span class="line">		</span><br><span class="line">        <span class="comment">// 容量管理的一个东西，比如限制kafka producer生产传输速度</span></span><br><span class="line">        quotaManagers = <span class="type">QuotaFactory</span>.instantiate(config, metrics, time)</span><br><span class="line">        notifyClusterListeners(kafkaMetricsReporters ++ reporters.asScala)</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* start log manager 构建LogManager，并运行。broker端的重要线程 */</span></span><br><span class="line">        logManager = createLogManager(zkUtils.zkClient, brokerState)</span><br><span class="line">        logManager.startup()</span><br><span class="line">          </span><br><span class="line">		<span class="comment">//  每个partition的状态缓存，每个broker都会异步维护同一个缓存，更新请求来自controller</span></span><br><span class="line">        metadataCache = <span class="keyword">new</span> <span class="type">MetadataCache</span>(config.brokerId)</span><br><span class="line">        credentialProvider = <span class="keyword">new</span> <span class="type">CredentialProvider</span>(config.saslEnabledMechanisms)</span><br><span class="line">		<span class="comment">// NIO socket server 创建和启动</span></span><br><span class="line">        socketServer = <span class="keyword">new</span> <span class="type">SocketServer</span>(config, metrics, time, credentialProvider)</span><br><span class="line">        socketServer.startup()</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* start replica manager 副本管理器初始化和启动 */</span></span><br><span class="line">        replicaManager = <span class="keyword">new</span> <span class="type">ReplicaManager</span>(config, metrics, time, zkUtils, kafkaScheduler, logManager,</span><br><span class="line">          isShuttingDown, quotaManagers.follower)</span><br><span class="line">        replicaManager.startup()</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* start kafka controller 控制器初始化和启动 */</span></span><br><span class="line">        kafkaController = <span class="keyword">new</span> <span class="type">KafkaController</span>(config, zkUtils, brokerState, time, metrics, threadNamePrefix)</span><br><span class="line">        kafkaController.startup()</span><br><span class="line"></span><br><span class="line">        adminManager = <span class="keyword">new</span> <span class="type">AdminManager</span>(config, metrics, metadataCache, zkUtils)</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* start group coordinator 管理消费组成员和offset */</span></span><br><span class="line">        <span class="comment">// Hardcode Time.SYSTEM for now as some Streams tests fail otherwise, it would be good to fix the underlying issue</span></span><br><span class="line">        groupCoordinator = <span class="type">GroupCoordinator</span>(config, zkUtils, replicaManager, <span class="type">Time</span>.<span class="type">SYSTEM</span>)</span><br><span class="line">        groupCoordinator.startup()</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* Get the authorizer and initialize it if one is specified.*/</span></span><br><span class="line">        authorizer = <span class="type">Option</span>(config.authorizerClassName).filter(_.nonEmpty).map &#123; authorizerClassName =&gt;</span><br><span class="line">          <span class="keyword">val</span> authZ = <span class="type">CoreUtils</span>.createObject[<span class="type">Authorizer</span>](authorizerClassName)</span><br><span class="line">          authZ.configure(config.originals())</span><br><span class="line">          authZ</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* start processing requests 处理各种kafka的请求*/</span></span><br><span class="line">        apis = <span class="keyword">new</span> <span class="type">KafkaApis</span>(socketServer.requestChannel, replicaManager, adminManager, groupCoordinator,</span><br><span class="line">          kafkaController, zkUtils, config.brokerId, config, metadataCache, metrics, authorizer, quotaManagers,</span><br><span class="line">          clusterId, time)</span><br><span class="line">		<span class="comment">// 请求池</span></span><br><span class="line">        requestHandlerPool = <span class="keyword">new</span> <span class="type">KafkaRequestHandlerPool</span>(config.brokerId, socketServer.requestChannel, apis, time,</span><br><span class="line">          config.numIoThreads)</span><br><span class="line"></span><br><span class="line">        <span class="type">Mx4jLoader</span>.maybeLoad()</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* start dynamic config manager Topic 配置管理请求，client配置管理请求，broker配置管理请求，用户配置管理请求 */</span></span><br><span class="line">        dynamicConfigHandlers = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">ConfigHandler</span>](<span class="type">ConfigType</span>.<span class="type">Topic</span> -&gt; <span class="keyword">new</span> <span class="type">TopicConfigHandler</span>(logManager, config, quotaManagers),</span><br><span class="line">                                                           <span class="type">ConfigType</span>.<span class="type">Client</span> -&gt; <span class="keyword">new</span> <span class="type">ClientIdConfigHandler</span>(quotaManagers),</span><br><span class="line">                                                           <span class="type">ConfigType</span>.<span class="type">User</span> -&gt; <span class="keyword">new</span> <span class="type">UserConfigHandler</span>(quotaManagers, credentialProvider),</span><br><span class="line">                                                           <span class="type">ConfigType</span>.<span class="type">Broker</span> -&gt; <span class="keyword">new</span> <span class="type">BrokerConfigHandler</span>(config, quotaManagers))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Create the config manager. start listening to notifications</span></span><br><span class="line">        <span class="comment">// 动态监听所有请求，开始启动</span></span><br><span class="line">        dynamicConfigManager = <span class="keyword">new</span> <span class="type">DynamicConfigManager</span>(zkUtils, dynamicConfigHandlers)</span><br><span class="line">        dynamicConfigManager.startup()</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* tell everyone we are alive 监听kafka进程状态，健康管理*/</span></span><br><span class="line">        <span class="keyword">val</span> listeners = config.advertisedListeners.map &#123; endpoint =&gt;</span><br><span class="line">          <span class="keyword">if</span> (endpoint.port == <span class="number">0</span>)</span><br><span class="line">            endpoint.copy(port = socketServer.boundPort(endpoint.listenerName))</span><br><span class="line">          <span class="keyword">else</span></span><br><span class="line">            endpoint</span><br><span class="line">        &#125;</span><br><span class="line">        kafkaHealthcheck = <span class="keyword">new</span> <span class="type">KafkaHealthcheck</span>(config.brokerId, listeners, zkUtils, config.rack,</span><br><span class="line">          config.interBrokerProtocolVersion)</span><br><span class="line">        kafkaHealthcheck.startup()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Now that the broker id is successfully registered via KafkaHealthcheck, checkpoint it</span></span><br><span class="line">        checkpointBrokerId(config.brokerId)</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* register broker metrics */</span></span><br><span class="line">        registerStats()</span><br><span class="line">		<span class="comment">// 更新broker状态为 Runing</span></span><br><span class="line">        brokerState.newState(<span class="type">RunningAsBroker</span>)</span><br><span class="line">        shutdownLatch = <span class="keyword">new</span> <span class="type">CountDownLatch</span>(<span class="number">1</span>)</span><br><span class="line">        <span class="comment">// 启动完成，修改相关状态</span></span><br><span class="line">        startupComplete.set(<span class="literal">true</span>)</span><br><span class="line">        isStartingUp.set(<span class="literal">false</span>)</span><br><span class="line">        <span class="type">AppInfoParser</span>.registerAppInfo(jmxPrefix, config.brokerId.toString)</span><br><span class="line">        info(<span class="string">"started"</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</span><br><span class="line">        fatal(<span class="string">"Fatal error during KafkaServer startup. Prepare to shutdown"</span>, e)</span><br><span class="line">        isStartingUp.set(<span class="literal">false</span>)</span><br><span class="line">        shutdown()</span><br><span class="line">        <span class="keyword">throw</span> e</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>总的来说，在KafkaServer启动流程中，先后启动了以下几个模块：</p>
<ol>
<li>kafkaScheduler 调度模块，负责kafka内部的周期性调度和非周期性调度</li>
<li>初始化 Zookeeper 工具类</li>
<li>实例化 metrics</li>
<li>quotaManagers 限额管理模块</li>
<li>logManager 日志管理模块</li>
<li>socketServer 网络服务模块</li>
<li>replicaManager 副本管理模块</li>
<li>kafkaController kafka控制器</li>
<li>adminManager 暂时不知道具体干嘛的，应该做客户端管理用的</li>
<li>groupCoordinator  消费组协调器</li>
<li>apis 和 requestHandlerPool 请求API和请求池</li>
<li>dynamicConfigManager 动态配置管理</li>
</ol>
]]></content>
      <categories>
        <category>kafka</category>
        <category>源码入门</category>
        <category>kafka服务端</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>源码</tag>
      </tags>
  </entry>
</search>
